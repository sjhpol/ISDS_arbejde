{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Introduction to Social Data Science: Text as Data\n",
    "\n",
    "#### Required readings\n",
    "\n",
    "*(PML) Raschka, Sebastian, and Mirjalili, Vahid. Python Machine Learning - Second Edition. Birmingham: Packt Publishing, Limited, 2017. Chapter 8*\n",
    "\n",
    "- PML: following sections from chapter 8:\n",
    "  - Preparing the IMDb movie review data for text processing\n",
    "  - Introducing the bag-of-words model\n",
    "  - Training a logistic regression model for document classification\n",
    " \n",
    "Gentzkow, M., Kelly, B.T. and Taddy, M., 2017. [\"Text as data\"](http://www.nber.org/papers/w23276.pdf) (No. w23276). *National Bureau of Economic Research*.\n",
    "\n",
    "Jurafsky, D., & Martin, J. H. (2019). Vector Semantics and Embeddings. Speech and Language Processing, 3rd ed. draft. https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Inspirational readings\n",
    "\n",
    "Gorrell, Genevieve et al. “Twits, Twats and Twaddle: Trends in Online Abuse towards UK Politicians.” ICWSM (2018). https://gate-socmedia.group.shef.ac.uk/wp-content/uploads/2019/07/Gorrell-Greenwood.pdf\n",
    "\n",
    "Pang, Bo et al. “Thumbs up? Sentiment Classification using Machine Learning Techniques.” EMNLP (2002). https://www.aclweb.org/anthology/W02-1011.pdf \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "xom1o-ebeBv3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Course page: https://isdsucph.github.io/isds2021/"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "MbfuNY1YkeU5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Agenda**\n",
    "- Preprocessing text\n",
    "- Feature extraction\n",
    "- Text classification with ML and using lexicons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('XJFB6-qpyTs', width=740, height=460)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import packages"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "lQX-pwV2IHMw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from the textbook, for printing a process bar.\n",
    "import pyprind\n",
    "\n",
    "# basic packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re # python regular expressions\n",
    "import string # for efficient operations with strings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For creating dictionaries that you can fill in a loop\n",
    "from collections import defaultdict\n",
    "\n",
    "# NLTK: A basic, popular NLP package. Find many examples of applications at https://www.nltk.org/book/\n",
    "# Install guide: https://www.nltk.org/install.html\n",
    "import nltk\n",
    "nltk.download('punkt') # you will probably need to do this\n",
    "nltk.download('wordnet') # and this\n",
    "nltk.download('stopwords') # aand this\n",
    "\n",
    "# for vectorization \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Vader Lexicon for sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# similarity/distance measures\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "\n",
    "# for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Lexicons for sentiment analysis\n",
    "from vaderSentiment import vaderSentiment\n",
    "from afinn import Afinn\n",
    "\n",
    "# to display images in notebook\n",
    "from IPython.display import Image"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ja2q7_ZrIGdl",
    "outputId": "9fca1081-29bf-4a14-889c-3c207c020c35"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing data (following PML chapter 8)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "c-cRg6Cv0N1k"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data source:\n",
    "\n",
    "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n",
    "\n",
    "Download from http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "L_b3SdU51T1-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# download the data\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "dcjbWCcDd8q7",
    "outputId": "25d555af-5442-42f0-d40d-251392fb94c3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# unpack\n",
    "import tarfile\n",
    "with tarfile.open(\"aclImdb_v1.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1eCVNJD2TRK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load data into a pandas DataFrame\n",
    "\n",
    "basepath = \"aclImdb\"\n",
    "\n",
    "labels = {\"pos\":1, \"neg\":0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in (\"test\", \"train\"):\n",
    "    for l in (\"pos\", \"neg\"):\n",
    "        path = os.path.join(basepath,s,l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path,file), \"r\", encoding=\"utf-8\") as infile:\n",
    "                txt = infile.read()\n",
    "            # Here I also append s (\"train\" or \"test\") to later use the predefined \n",
    "            # split of the data. They didn't do this in the textbook.\n",
    "            df = df.append([[txt, labels[l], s]], ignore_index=True) \n",
    "            pbar.update()\n",
    "df.columns = [\"review\", \"sentiment\", \"set\"]"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YAfU_OC0Dl2w",
    "outputId": "bb0d2ce5-9916-4f4e-d5e2-d33d54ae26f7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df # look at the dataframe"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "nk3K4svoD0T5",
    "outputId": "81656afc-605c-4a9f-a312-bec0a43d6c26",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dividing into train and test set again.\n",
    "df_train = df[df.set==\"train\"]\n",
    "df_test = df[df.set==\"test\"]\n",
    "\n",
    "# Permute data such that pos and neg samples are mixed\n",
    "\n",
    "np.random.seed(0)\n",
    "df_train = df.reindex(np.random.permutation(df_train.index))\n",
    "df_test = df.reindex(np.random.permutation(df_test.index))\n",
    "\n",
    "# save dataframes:\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-X7lKhZcHzPD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load saved dataframes:\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "YouTubeVideo('mz9uPDsOER8', width=740, height=460)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We preprocess text data to standardize the format and to define what a unique \"feature\" is.\n",
    " \n",
    " \n",
    " *What is a feature?* \n",
    " Features are the independent variables. A flower may have features such as color and leaf length, and you can use such features to classify flowers into species. \n",
    " With text data, a feature is commonly a word (or a character in some special cases). If we have a small collection of texts where 2500 *unique* words are used, we commonly say that the *vocabulary* is of size 2500 and the number of features is equal to the size of the vocabulary.\n",
    " \n",
    " For most types of analysis, we have to represent each feature with some numerical values. But, this is where the importance of preprocessing comes in: Should the words \"try\" and \"trying\" have different values or do they convey the same basic information? What about \"USA\" and \"usa\"? And are punctuations important? \n",
    "\n",
    " \n",
    "When working with text as data, the number of features is much larger than when working with most other types as data. Preproccessing is therefore both about standardizing the data and about trying to reduce the number of features to relieve the computation cost.\n",
    "\n",
    "The most standard preprocessing steps are:\n",
    "\n",
    "- Segmentation (dividing a string into reasonable sentences and words)\n",
    "- Lowercasing (standardizing)\n",
    "- Stemming or lemmatizing (standardizing)\n",
    "- Removing so-called stopwords and punctuation and other potentially disrupting text patterns (reducing \"noise\" and feature dimensionality)\n",
    "\n",
    "\n",
    "What features are important, and hence which preprocessing steps should be included/excluded, depends on the task / research question.\n",
    " "
   ],
   "metadata": {
    "colab_type": "text",
    "id": "ZHoRouz1XfWn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization (segmentation)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "y6ylK3UlFnUV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sentence = df_train.review.values[0]\n",
    "print(sentence)\n",
    " \n",
    "# word tokenization (segmentation)\n",
    "# by blank space (crude)\n",
    "sent = sentence.split(\" \")\n",
    "print(sent)\n",
    "# by using a package such as NLTK.\n",
    "sent = nltk.tokenize.word_tokenize(sentence)\n",
    "print(sent)\n",
    "# nltk.tokenize.TweetTokenizer() is another option, and good for social media text.\n",
    "\n",
    "# Sentence segmentation (sentence splitting)\n",
    "sents = nltk.tokenize.sent_tokenize(sentence)\n",
    "print(sents)\n",
    "\n",
    "# There are many other more complex versions used for specific cases/tasks/models and sometime you may want to costumize the segmentation.\n",
    "# Learn more: https://web.stanford.edu/~jurafsky/slp3/2.pdf"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "6QLQRZD-FzAw",
    "outputId": "cd63fff3-7c26-4f4c-d7d6-ad577cd538dd",
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming or lemmatising"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "fW2XjFinGrDr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Stemming**: Stripping the word down to it's central meaning (it's stem) by removing affixes. Stemming usually only removes suffixes (i.e. affixes at the end of the words).\n",
    "\n",
    "E.g.: \n",
    "\n",
    "sings --> sing\n",
    "\n",
    "laughing --> laugh\n",
    "\n",
    "wonderful --> wonder\n",
    "\n",
    "\n",
    "**Lemmatizing**: \"the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing.\" (Jurafsky, D., & Martin, J. H., 2019. Chap. 2, p. 2)\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "_2ZmB0lrO7YW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def stem(word):\n",
    "    # find suffixes and return stems\n",
    "    # (.*?) matches any character in front of a word (non greedy version)\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "# Tokenize the text first\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "print([stem(t) for t in tokens]) # hmm not so good"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "eYVlrxLPG_bo",
    "outputId": "10866ba8-d268-4285-fe1a-784418aa71bc",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# using NLTK's Porter stemmer\n",
    "porter = nltk.PorterStemmer()\n",
    "#print(sentence)\n",
    "print([porter.stem(t) for t in tokens])"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "xhdCmmadOP9e",
    "outputId": "4c4fd319-61a3-437a-8c14-64d407642355",
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# using the WordNet lemmatizer through NLTK\n",
    "# WordNet is a large lexical database of English (think \"lexicon\")\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print(tokens)\n",
    "print([wnl.lemmatize(t) for t in tokens])\n",
    "# note the differences.. this lemmatizer knows that the word \"based\" does not have the same meaning as \"base\" here. \n",
    "# Seems more comprehensible. Or lot less has actually changed. (examples: \"goes\" became \"go\" and \"villagers\" became \"villager\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "0AFQaZQxOwtn",
    "outputId": "ae113ba5-d288-4afa-da40-68532d904b1d",
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Casing, stopwords, punctuation etc."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "aQKMQ2f0HBk0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Casing (lower casing is normal procedure such that the word \". That\", after a punctuation, and \"that\" are not treated as two seperate features.\n",
    "sent = sentence.lower()\n",
    "print(sent)\n",
    "# Note: you may want some exceptions, e.g. distinction between US and us can be important for some tasks.\n",
    "\n",
    "\n",
    "# lowercase and then tokenize all texts in the training set – in one list comprehension (takes a minute or two):\n",
    "train_sents = [nltk.word_tokenize(i.lower()) for i in df_train.review.values]\n",
    "print(train_sents[0])\n",
    "# And do the same with the test set\n",
    "test_sents = [nltk.word_tokenize(i.lower()) for i in df_test.review.values]\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "uJ70y0WhJakr",
    "outputId": "8be7fe9b-05f3-4f23-b63c-377a671448ad",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing certain words (stopwords)\n",
    "stop_words_list = nltk.corpus.stopwords.words(\"english\")\n",
    "print(stop_words_list)\n",
    "print()\n",
    "sent_sw_removed = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words_list]\n",
    "print(sent_sw_removed)\n",
    "#train_sents_sw_removed = [[i for i in sent if i not in stop_words_list] for sent in train_sents] # nested list comprehension; lists in list.\n",
    "#test_sents_sw_removed = [[i for i in sent if i not in stop_words_list] for sent in test_sents]\n",
    "#print(train_sents_sw_removed[0])\n",
    "\n",
    "# unfortunately the tokenization does not totally align with the stopwords – the simple split at whitespace may be more appropiate before stopword removal.\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "OebDZy9nqBi0",
    "outputId": "7393e23d-23bb-46f5-9c33-27a2291d7249",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing punctuation, two ways:\n",
    "\n",
    "punct_removed_1 = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "print(punct_removed_1)\n",
    "\n",
    "#def removePunctuation (word):\n",
    "#    return re.sub(\"[^\\w\\s\\-\\']\", \"\", word)\n",
    "\n",
    "punct_removed_2 = re.sub(r'[^\\w\\s]','',sentence) # learn more about regex at https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial or find a cheat sheet.\n",
    "print(punct_removed_2)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "2caJ50E80Bw-",
    "outputId": "3fe72a41-e266-4f11-f0da-dee2d3c61880",
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1: preprocessing\n",
    "Make a preprocessing function that takes a single string and \n",
    " 1) Lowercase the words,\n",
    " 2) split the text into words (tokenize), \n",
    " 3) either stem or lemmatize words.\n",
    "Feel free to add more steps of preprocessing. For example stopword removal or removal of what seems to be HTML elements (such as \"< br/>\") in the text, and removal of punctuation, and handling of emoticons as in the textbook."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Fqo9iqZnogVQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess(text):\n",
    "    stop_words_list = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [i for i in nltk.word_tokenize(text.lower()) if i not in stop_words_list]\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(t) for t in tokens]\n",
    "    return lemmas # return a list of stems/lemmas\n",
    "    \n",
    "sentence = df_train.review.values[0]\n",
    "print(preprocess(sentence))\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "kQ9i04yboff0",
    "outputId": "43959584-3fd1-4155-a56a-e992c3b972d6",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Have a look at the size of the vocabulary of the train data that \n",
    "# has only been lower-cased and tokenized (a few cells above).\n",
    "\n",
    "d = defaultdict(int)\n",
    "for i in train_sents:\n",
    "    for w in i:\n",
    "        d[w]+=1\n",
    "V = d.keys()\n",
    "print(\"Vocabulary size:\",len(V))\n",
    "\n",
    "# inspect the 10 most frequent tokens:\n",
    "{k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)[:10]}\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "PTj5j8xEsDPq",
    "outputId": "78c2fd64-f925-4891-848e-6ac76e0a6aa1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Now do it yourself with your train data after preprocessing:\n",
    "\n",
    "# YOUR CODE HERE\n",
    "d = defaultdict(int)\n",
    "\n",
    "for i in df_train.review.values:\n",
    "    for w in preprocess(i):\n",
    "        d[w]+=1\n",
    "V = d.keys()\n",
    "print(\"Vocabulary size:\",len(V))\n",
    "\n",
    "\n",
    "# inspect the N most frequent tokens (you decide which N to use):\n",
    "{k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)[:10]}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Optional: Look at the content of V in some way, and comment on what you could do to improve your preproccessing.\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature representation/extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "YouTubeVideo('xvvkx49HuuU', width=740, height=460)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x145901ac0>"
      ],
      "image/jpeg": "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",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"740\"\n",
       "            height=\"460\"\n",
       "            src=\"https://www.youtube.com/embed/xvvkx49HuuU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, the goal is to make a sentiment classification model trained on the movie review data. \n",
    "\n",
    "No matter whether we want to make a (supervised) classification model or (unsupervised) clustering of texts and other analyses of text similarities, we will have to transform our features, that are currently strings, into a numerical representation, that will convey the content of a sentence or document in some way. There are many approaches one can take, ranging from very simple representations (such as vectors with binary values for each feature, stating wether the feature/word appears in the given document or not) to very complex representations learned from a seperate prediction model.\n",
    "\n",
    "One standard, simple approach is called Bag of Words:"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "8gLIFQzihZZ-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BoW (Bag of Words)\n",
    "\n",
    "- Represent the texts with a **Term-Document frequency matrix** where each text is represented by a vector of word counts. \n",
    "- The creats a matrix with size n_samples x n_features. (n_features = size of vocabulary)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "CignHk3ALBPb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Image(\"term_document.png\") # (Jurafsky, D., & Martin, J. H., 2019. Chap. 6, p. 7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# CountVectorizer (from sklearn.feature_extraction.text) has a build-in tokenizer and lowercases by default. It also has an option to remove stopwords (look at the documentation).\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# You can override the default tokenization with your own defined function, like so:\n",
    "#vectorizer = CountVectorizer(tokenizer=preprocess)\n",
    "\n",
    "# you can also restrict the number of features to the top N most frequent features:\n",
    "#vectorizer = CountVectorizer(max_features=N)\n",
    "\n",
    "# fit and transform train set\n",
    "X_train_bow = vectorizer.fit_transform(df_train.review.values)\n",
    "\n",
    "# Only tranform test set: never fit your vectorizer on the test set (it is cheating). Out-of-Vocabulary words are handled automatically be sklearn's vectorizer.\n",
    "X_test_bow = vectorizer.transform(df_test.review.values)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWjW40BsJa0v"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "print(X_train_bow.shape)\n",
    "print(len(vectorizer.vocabulary_))\n",
    "X_train_bow[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 74849)\n",
      "74849\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<1x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 238 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "qD-cXAT4s9jB",
    "outputId": "c0b7fd90-93ca-449a-cdf3-51aea72bdf82"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only 238 nonzero elements in the vector of the first text, i.e. 238 unique features/words out of 74,849."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "X_train_bow[0].toarray() # the vector is very sparse.."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R-iHjUpktdr1",
    "outputId": "d0bf4e38-efe9-45af-8c11-6d9e05dcb6d8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(X_train_bow[0]) # here we take a look at the non-zero elements. \n",
    "#The first element is the word with ID 25459, which appears two times in the text."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(vectorizer.get_feature_names()[25459]) # this is one way to get the word with the ID 25459"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#df_train.review.values[0] # you can check that the word \"forbidden\" appears two times in the first text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### N-grams \n",
    "\n",
    "- Collection of 1 or more tokens.\n",
    "\n",
    "- Bag of words lacks word order and context (semantics). n-grams to the rescue!"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "spPzY15wh381"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "example = \"The cat in the hat\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def make_ngrams(sentence,n):\n",
    "    tokens = sentence.split(\" \")\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "print(\"Unigrams:\",make_ngrams(example,1))\n",
    "print(\"Bigrams:\",make_ngrams(example,2))\n",
    "print(\"Trigrams:\",make_ngrams(example,3))\n",
    "print(\"4-grams:\",make_ngrams(example,4))\n",
    "print(\"5-grams:\",make_ngrams(example,5))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unigrams: ['The', 'cat', 'in', 'the', 'hat']\n",
      "Bigrams: ['The cat', 'cat in', 'in the', 'the hat']\n",
      "Trigrams: ['The cat in', 'cat in the', 'in the hat']\n",
      "4-grams: ['The cat in the', 'cat in the hat']\n",
      "5-grams: ['The cat in the hat']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BoW with ngrams using CountVectorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# this block can take a few minutes to run\n",
    "\n",
    "# n=1-5\n",
    "vectorizer = CountVectorizer(ngram_range=(1,5))\n",
    "\n",
    "X_train_5gram = vectorizer.fit_transform(df_train.review.values)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hikCoI04iPn1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "print(X_train_5gram.shape)\n",
    "X_train_5gram[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 15490990)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<1x15490990 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1811 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "4E5cs2uSxkN-",
    "outputId": "74789b75-a6a6-448a-9056-382c197da22e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- But now we have problems with high dimensionalty and uniqeness of features!\n",
    "\n",
    "- N-grams are used for many applications but are especially known from **Language Models**: In short, probalistic models that learn to predict the next word in a sequence of words, given the \"history\" (the previous words), simply by storing the probability of this event occuring in the given text, e.g. P(hat|The cat in the). But instead of using ALL previous word (which would be a combination of words unique to the given text), the history is approximated by a few previous words (n-grams), e.g. P(hat|the). This is the n-gram language model.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2: Inspect most frequent bigrams\n",
    "\n",
    "Inspection of word collocations (bigrams, trigrams etc.) can be a good first step in exploring a new text dataset, or a single document for that matter. Such collocations will be very indicative of what the texts are about.\n",
    "\n",
    "Make a new CountVectorizer that only includes bigrams and with max_features=1000. Fit it on the train set.\n",
    "\n",
    "Then, practise you Googling and find a way to print the most frequent bigrams in the train set. Print the bigrams along with their frequencies in a descending order. What do you see?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# YOUR CODE HERE\n",
    "vectorizer2 = CountVectorizer(ngram_range=(2,2), max_features = 1000)\n",
    "\n",
    "X_train_bigram = vectorizer2.fit_transform(df_train.review.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "vocab = vectorizer2.vocabulary_\n",
    "\n",
    "count_values = X_train_bigram.toarray().sum(axis=0)\n",
    "\n",
    "# output n-grams\n",
    "for ng_count, ng_text in sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True):\n",
    "    print(ng_count, ng_text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50973 br br\n",
      "39166 of the\n",
      "25224 in the\n",
      "15746 this movie\n",
      "13543 the film\n",
      "13353 and the\n",
      "12000 to the\n",
      "11883 to be\n",
      "11877 the movie\n",
      "10925 this film\n",
      "10145 it is\n",
      "9480 this is\n",
      "8988 on the\n",
      "8637 it was\n",
      "8094 one of\n",
      "8005 for the\n",
      "7891 with the\n",
      "7814 br the\n",
      "6980 is the\n",
      "6907 if you\n",
      "6827 at the\n",
      "6164 from the\n",
      "6156 in this\n",
      "5932 as the\n",
      "5524 that the\n",
      "5365 the story\n",
      "5167 to see\n",
      "5144 out of\n",
      "4872 by the\n",
      "4658 movie is\n",
      "4629 the first\n",
      "4448 all the\n",
      "4425 there is\n",
      "4317 have been\n",
      "4249 of this\n",
      "4128 film is\n",
      "4080 is not\n",
      "4017 but it\n",
      "3806 the end\n",
      "3735 that it\n",
      "3724 there are\n",
      "3686 and it\n",
      "3682 the same\n",
      "3537 the most\n",
      "3478 but the\n",
      "3468 the best\n",
      "3286 about the\n",
      "3284 to make\n",
      "3265 the only\n",
      "3222 of his\n",
      "3202 that is\n",
      "3161 he is\n",
      "3130 this one\n",
      "3122 to watch\n",
      "3092 the plot\n",
      "2852 the characters\n",
      "2777 some of\n",
      "2723 was the\n",
      "2677 have to\n",
      "2673 to get\n",
      "2635 you can\n",
      "2625 they are\n",
      "2600 want to\n",
      "2577 is that\n",
      "2557 would have\n",
      "2552 the acting\n",
      "2521 the other\n",
      "2510 at least\n",
      "2485 to do\n",
      "2478 into the\n",
      "2445 the way\n",
      "2415 lot of\n",
      "2381 as well\n",
      "2369 would be\n",
      "2354 is one\n",
      "2325 kind of\n",
      "2310 br this\n",
      "2308 this was\n",
      "2293 that he\n",
      "2266 to have\n",
      "2232 of course\n",
      "2223 the whole\n",
      "2222 trying to\n",
      "2201 movie was\n",
      "2181 like the\n",
      "2181 he was\n",
      "2179 that this\n",
      "2129 is very\n",
      "2124 and his\n",
      "2109 the original\n",
      "2108 most of\n",
      "2104 the worst\n",
      "2091 in his\n",
      "2035 the time\n",
      "2034 which is\n",
      "2022 at all\n",
      "2021 is an\n",
      "2021 and that\n",
      "1986 more than\n",
      "1966 of it\n",
      "1964 could have\n",
      "1959 to say\n",
      "1945 you re\n",
      "1925 going to\n",
      "1896 who is\n",
      "1833 all of\n",
      "1824 but this\n",
      "1808 of them\n",
      "1803 of all\n",
      "1789 and then\n",
      "1768 when the\n",
      "1761 is so\n",
      "1753 the world\n",
      "1751 movie and\n",
      "1736 over the\n",
      "1730 it has\n",
      "1729 seems to\n",
      "1725 it not\n",
      "1723 that was\n",
      "1703 the director\n",
      "1696 as it\n",
      "1694 the fact\n",
      "1693 br it\n",
      "1689 part of\n",
      "1681 it the\n",
      "1678 she is\n",
      "1672 in my\n",
      "1658 like this\n",
      "1651 the rest\n",
      "1650 is just\n",
      "1620 the main\n",
      "1610 you have\n",
      "1607 with his\n",
      "1604 that they\n",
      "1582 can be\n",
      "1580 fact that\n",
      "1573 it just\n",
      "1573 for this\n",
      "1565 film and\n",
      "1553 the two\n",
      "1546 they were\n",
      "1544 because it\n",
      "1538 and he\n",
      "1530 you are\n",
      "1528 the actors\n",
      "1527 has been\n",
      "1500 through the\n",
      "1497 the show\n",
      "1496 the script\n",
      "1493 it to\n",
      "1483 there was\n",
      "1480 the last\n",
      "1470 it and\n",
      "1454 in fact\n",
      "1450 be the\n",
      "1447 it would\n",
      "1440 so much\n",
      "1437 and is\n",
      "1427 of her\n",
      "1420 film was\n",
      "1410 of my\n",
      "1408 see the\n",
      "1389 movie that\n",
      "1388 story is\n",
      "1388 he has\n",
      "1385 to find\n",
      "1381 will be\n",
      "1371 about this\n",
      "1365 not to\n",
      "1364 it all\n",
      "1361 ve seen\n",
      "1360 rest of\n",
      "1357 supposed to\n",
      "1350 the cast\n",
      "1348 film that\n",
      "1337 should be\n",
      "1335 sort of\n",
      "1333 does not\n",
      "1325 when he\n",
      "1324 watch it\n",
      "1317 is in\n",
      "1311 with this\n",
      "1310 that you\n",
      "1308 the book\n",
      "1308 ever seen\n",
      "1303 to go\n",
      "1303 is no\n",
      "1301 you ll\n",
      "1301 because of\n",
      "1287 you will\n",
      "1279 the audience\n",
      "1275 not the\n",
      "1272 and this\n",
      "1271 hard to\n",
      "1270 to this\n",
      "1261 they re\n",
      "1260 and you\n",
      "1258 see it\n",
      "1252 up to\n",
      "1241 in it\n",
      "1234 was not\n",
      "1232 end of\n",
      "1229 than the\n",
      "1229 for me\n",
      "1227 has to\n",
      "1225 such as\n",
      "1224 watch this\n",
      "1223 is also\n",
      "1223 and not\n",
      "1220 to his\n",
      "1201 able to\n",
      "1200 had to\n",
      "1198 each other\n",
      "1195 acting is\n",
      "1194 very good\n",
      "1193 see this\n",
      "1190 and they\n",
      "1188 to me\n",
      "1187 not only\n",
      "1185 where the\n",
      "1180 as an\n",
      "1176 have seen\n",
      "1175 when it\n",
      "1173 say that\n",
      "1168 after the\n",
      "1166 do not\n",
      "1162 should have\n",
      "1158 on this\n",
      "1152 if it\n",
      "1150 the ending\n",
      "1150 back to\n",
      "1149 well as\n",
      "1149 it br\n",
      "1148 of their\n",
      "1146 what the\n",
      "1143 did not\n",
      "1142 in which\n",
      "1139 are the\n",
      "1135 about it\n",
      "1133 up with\n",
      "1129 of those\n",
      "1128 and there\n",
      "1127 seem to\n",
      "1121 enough to\n",
      "1116 in an\n",
      "1113 special effects\n",
      "1109 you want\n",
      "1107 to take\n",
      "1105 the one\n",
      "1105 better than\n",
      "1104 played by\n",
      "1103 so many\n",
      "1100 with her\n",
      "1097 this show\n",
      "1097 in their\n",
      "1097 but that\n",
      "1092 the entire\n",
      "1092 during the\n",
      "1086 in all\n",
      "1069 movie it\n",
      "1068 that she\n",
      "1067 based on\n",
      "1065 of an\n",
      "1064 and even\n",
      "1054 if the\n",
      "1053 and in\n",
      "1051 tries to\n",
      "1050 was so\n",
      "1047 as he\n",
      "1044 story of\n",
      "1042 are not\n",
      "1041 too much\n",
      "1041 even though\n",
      "1038 but he\n",
      "1038 and was\n",
      "1034 what is\n",
      "1033 is about\n",
      "1032 but not\n",
      "1030 movie the\n",
      "1026 piece of\n",
      "1026 it in\n",
      "1023 she was\n",
      "1020 film the\n",
      "1018 people who\n",
      "1017 instead of\n",
      "1015 br there\n",
      "1013 don know\n",
      "1010 the music\n",
      "1006 br in\n",
      "997 out the\n",
      "996 even the\n",
      "993 who has\n",
      "993 try to\n",
      "990 is good\n",
      "981 movie br\n",
      "978 is great\n",
      "973 his own\n",
      "970 br if\n",
      "968 the beginning\n",
      "966 watching this\n",
      "964 of these\n",
      "962 sense of\n",
      "960 as if\n",
      "959 fan of\n",
      "957 the viewer\n",
      "956 the very\n",
      "956 movie but\n",
      "949 when they\n",
      "946 that are\n",
      "945 was very\n",
      "942 think that\n",
      "941 get the\n",
      "939 wanted to\n",
      "938 time and\n",
      "938 have the\n",
      "937 characters are\n",
      "936 we are\n",
      "932 the real\n",
      "930 saw this\n",
      "929 in her\n",
      "927 like it\n",
      "926 because the\n",
      "920 or the\n",
      "918 off the\n",
      "914 up in\n",
      "914 out to\n",
      "912 the second\n",
      "909 was in\n",
      "909 the scene\n",
      "909 of 10\n",
      "909 between the\n",
      "905 the character\n",
      "904 we have\n",
      "903 much of\n",
      "903 in love\n",
      "903 for his\n",
      "902 could be\n",
      "902 and her\n",
      "896 for it\n",
      "894 film it\n",
      "893 the top\n",
      "893 film br\n",
      "893 couple of\n",
      "891 when you\n",
      "891 was just\n",
      "891 they have\n",
      "890 up the\n",
      "887 with an\n",
      "885 but they\n",
      "882 lack of\n",
      "880 make it\n",
      "880 low budget\n",
      "879 you know\n",
      "879 way to\n",
      "878 there were\n",
      "878 look at\n",
      "877 is really\n",
      "877 is it\n",
      "877 all in\n",
      "876 year old\n",
      "875 no one\n",
      "873 of what\n",
      "872 the people\n",
      "871 as they\n",
      "868 it seems\n",
      "864 br but\n",
      "863 at times\n",
      "861 is to\n",
      "858 the series\n",
      "857 plot is\n",
      "857 if they\n",
      "855 to give\n",
      "855 not even\n",
      "854 that there\n",
      "853 of that\n",
      "852 it doesn\n",
      "847 on his\n",
      "845 much more\n",
      "845 have ever\n",
      "843 in that\n",
      "838 version of\n",
      "838 it for\n",
      "837 those who\n",
      "833 need to\n",
      "832 wants to\n",
      "832 and she\n",
      "831 the next\n",
      "829 to it\n",
      "828 you don\n",
      "823 it really\n",
      "822 up and\n",
      "821 his wife\n",
      "819 the screen\n",
      "818 with some\n",
      "818 of its\n",
      "817 what it\n",
      "817 but in\n",
      "813 any of\n",
      "812 to her\n",
      "812 for her\n",
      "811 very well\n",
      "811 it as\n",
      "808 like to\n",
      "806 and some\n",
      "803 it but\n",
      "799 full of\n",
      "799 due to\n",
      "797 movie with\n",
      "794 the camera\n",
      "794 may be\n",
      "793 rather than\n",
      "793 film but\n",
      "789 who are\n",
      "788 the man\n",
      "787 br and\n",
      "786 movie to\n",
      "783 the point\n",
      "781 throughout the\n",
      "781 looks like\n",
      "781 even if\n",
      "780 and all\n",
      "779 think it\n",
      "776 watch the\n",
      "775 and have\n",
      "772 is still\n",
      "771 looking for\n",
      "769 away from\n",
      "768 of film\n",
      "767 know what\n",
      "765 the title\n",
      "764 of time\n",
      "763 go to\n",
      "760 the great\n",
      "759 the dvd\n",
      "754 that would\n",
      "754 many of\n",
      "754 and so\n",
      "753 to keep\n",
      "752 him to\n",
      "747 the final\n",
      "747 it also\n",
      "746 for some\n",
      "745 it it\n",
      "745 give it\n",
      "744 to show\n",
      "743 the action\n",
      "743 new york\n",
      "743 my favorite\n",
      "743 lots of\n",
      "741 it on\n",
      "741 and what\n",
      "739 story and\n",
      "737 so bad\n",
      "737 much better\n",
      "736 the old\n",
      "736 has the\n",
      "731 while the\n",
      "728 what was\n",
      "727 along with\n",
      "726 to know\n",
      "725 with it\n",
      "725 one is\n",
      "725 br as\n",
      "723 you ve\n",
      "723 the big\n",
      "721 first time\n",
      "719 it does\n",
      "718 in some\n",
      "718 but there\n",
      "715 when she\n",
      "715 had the\n",
      "715 bit of\n",
      "714 he can\n",
      "712 not be\n",
      "711 ve ever\n",
      "710 it had\n",
      "705 we see\n",
      "705 man who\n",
      "704 had been\n",
      "702 all that\n",
      "698 that has\n",
      "695 film to\n",
      "694 waste of\n",
      "691 that we\n",
      "690 movie has\n",
      "690 make the\n",
      "689 was made\n",
      "687 none of\n",
      "687 in one\n",
      "686 who was\n",
      "686 the scenes\n",
      "686 get to\n",
      "686 bunch of\n",
      "684 are so\n",
      "676 is quite\n",
      "673 the new\n",
      "673 film has\n",
      "672 must have\n",
      "672 how to\n",
      "672 as much\n",
      "671 my opinion\n",
      "670 to kill\n",
      "667 do with\n",
      "666 might be\n",
      "666 it wasn\n",
      "666 film with\n",
      "666 but if\n",
      "665 the right\n",
      "665 seen in\n",
      "664 how the\n",
      "662 the good\n",
      "662 me to\n",
      "661 their own\n",
      "661 the bad\n",
      "661 made me\n",
      "661 it could\n",
      "661 is more\n",
      "661 can see\n",
      "660 time to\n",
      "660 of people\n",
      "659 you get\n",
      "659 to come\n",
      "659 in its\n",
      "658 you see\n",
      "658 are all\n",
      "655 in order\n",
      "655 characters and\n",
      "654 film in\n",
      "652 sci fi\n",
      "650 point of\n",
      "650 other than\n",
      "649 more of\n",
      "649 and more\n",
      "648 watching it\n",
      "648 read the\n",
      "648 it so\n",
      "648 around the\n",
      "647 him and\n",
      "646 what they\n",
      "644 the role\n",
      "644 is pretty\n",
      "643 and to\n",
      "642 used to\n",
      "640 going on\n",
      "639 movie in\n",
      "636 it will\n",
      "634 good and\n",
      "634 for all\n",
      "633 the idea\n",
      "632 life and\n",
      "631 years ago\n",
      "631 all time\n",
      "629 which was\n",
      "629 if he\n",
      "628 was one\n",
      "627 character is\n",
      "627 and as\n",
      "626 she has\n",
      "626 just as\n",
      "624 with no\n",
      "624 know that\n",
      "623 don think\n",
      "622 movie for\n",
      "621 believe that\n",
      "621 and we\n",
      "620 minutes of\n",
      "619 was an\n",
      "617 for example\n",
      "617 before the\n",
      "616 watching the\n",
      "616 think the\n",
      "616 real life\n",
      "616 good movie\n",
      "616 even more\n",
      "615 the house\n",
      "615 but then\n",
      "613 recommend this\n",
      "613 is what\n",
      "612 use of\n",
      "612 seen it\n",
      "612 and how\n",
      "610 seen the\n",
      "610 of movie\n",
      "609 think of\n",
      "608 and if\n",
      "607 you like\n",
      "606 and has\n",
      "605 the young\n",
      "604 and very\n",
      "601 on and\n",
      "600 what he\n",
      "600 he had\n",
      "599 order to\n",
      "598 they had\n",
      "595 on dvd\n",
      "595 and one\n",
      "594 that have\n",
      "593 because he\n",
      "592 love with\n",
      "592 as to\n",
      "590 the middle\n",
      "589 anyone who\n",
      "588 to play\n",
      "588 acting was\n",
      "587 might have\n",
      "586 like that\n",
      "586 back in\n",
      "584 good as\n",
      "584 are some\n",
      "583 the opening\n",
      "583 nothing to\n",
      "582 group of\n",
      "581 on screen\n",
      "581 and their\n",
      "580 in any\n",
      "579 he does\n",
      "579 also the\n",
      "578 watched it\n",
      "578 this time\n",
      "577 pretty much\n",
      "577 look like\n",
      "577 can say\n",
      "576 is this\n",
      "576 has no\n",
      "574 to tell\n",
      "573 not have\n",
      "572 for those\n",
      "571 his life\n",
      "571 and just\n",
      "570 may have\n",
      "570 an excellent\n",
      "569 then the\n",
      "568 the more\n",
      "567 as good\n",
      "566 same time\n",
      "563 to think\n",
      "563 for an\n",
      "562 from this\n",
      "561 to look\n",
      "561 or even\n",
      "560 where he\n",
      "560 thought it\n",
      "560 br what\n",
      "560 at first\n",
      "559 must be\n",
      "558 out there\n",
      "557 out and\n",
      "557 made it\n",
      "557 have no\n",
      "557 br so\n",
      "556 the family\n",
      "555 for that\n",
      "554 main character\n",
      "554 made the\n",
      "553 to believe\n",
      "552 or not\n",
      "551 type of\n",
      "550 seemed to\n",
      "550 pretty good\n",
      "550 is as\n",
      "550 ever made\n",
      "549 and for\n",
      "548 high school\n",
      "548 down to\n",
      "548 all this\n",
      "547 time the\n",
      "547 him in\n",
      "547 acting and\n",
      "546 and an\n",
      "545 it out\n",
      "544 manages to\n",
      "544 come to\n",
      "543 plenty of\n",
      "542 attempt to\n",
      "541 you think\n",
      "541 was good\n",
      "541 so that\n",
      "541 because they\n",
      "540 that will\n",
      "540 and when\n",
      "539 and of\n",
      "538 tried to\n",
      "538 is going\n",
      "537 was that\n",
      "536 the lead\n",
      "535 thing that\n",
      "535 the guy\n",
      "533 the killer\n",
      "533 film as\n",
      "529 we re\n",
      "529 the girl\n",
      "529 not that\n",
      "529 decided to\n",
      "528 directed by\n",
      "526 scene where\n",
      "526 comes to\n",
      "525 watched this\n",
      "525 up for\n",
      "525 the greatest\n",
      "525 number of\n",
      "524 to work\n",
      "524 for you\n",
      "524 example of\n",
      "523 they can\n",
      "522 not as\n",
      "520 so it\n",
      "520 as one\n",
      "518 was really\n",
      "518 only one\n",
      "517 the past\n",
      "517 out in\n",
      "517 idea of\n",
      "515 that can\n",
      "515 is all\n",
      "514 saw it\n",
      "514 not just\n",
      "513 with all\n",
      "512 for him\n",
      "511 the dialogue\n",
      "508 too many\n",
      "508 though the\n",
      "508 though it\n",
      "508 close to\n",
      "507 was great\n",
      "506 well done\n",
      "506 this and\n",
      "506 scene in\n",
      "506 funny and\n",
      "505 makes it\n",
      "504 only thing\n",
      "502 his character\n",
      "501 your time\n",
      "501 after all\n",
      "500 is nothing\n",
      "500 down the\n",
      "499 on it\n",
      "498 we get\n",
      "498 ve been\n",
      "498 the day\n",
      "498 movies that\n",
      "498 it with\n",
      "497 made for\n",
      "497 find out\n",
      "496 set in\n",
      "496 on to\n",
      "496 is actually\n",
      "495 than this\n",
      "495 from his\n",
      "495 as for\n",
      "494 them to\n",
      "494 so the\n",
      "494 film for\n",
      "493 her and\n",
      "491 saw the\n",
      "491 but she\n",
      "490 made in\n",
      "490 it again\n",
      "488 what you\n",
      "488 was also\n",
      "488 it still\n",
      "487 until the\n",
      "486 it that\n",
      "485 the special\n",
      "485 the early\n",
      "485 good but\n",
      "483 was going\n",
      "482 give this\n",
      "482 but you\n",
      "481 that one\n",
      "481 that in\n",
      "480 on her\n",
      "480 of life\n",
      "479 to help\n",
      "479 of your\n",
      "477 very much\n",
      "477 but also\n",
      "476 to my\n",
      "476 that makes\n",
      "476 place in\n",
      "475 people in\n",
      "475 bad it\n",
      "475 an old\n",
      "473 to an\n",
      "473 the kind\n",
      "473 over and\n",
      "473 of any\n",
      "473 as she\n",
      "472 if this\n",
      "470 makes the\n",
      "470 be in\n",
      "469 to put\n",
      "469 since the\n",
      "469 her to\n",
      "469 has some\n",
      "469 as this\n",
      "468 you could\n",
      "468 of us\n",
      "468 much as\n",
      "468 movies and\n",
      "467 with their\n",
      "467 is probably\n",
      "466 movie about\n",
      "466 it up\n",
      "466 far as\n",
      "465 only to\n",
      "465 make up\n",
      "465 it should\n",
      "464 and with\n",
      "463 there no\n",
      "463 of which\n",
      "463 movie as\n",
      "463 just to\n",
      "463 is well\n",
      "460 the case\n",
      "460 that could\n",
      "460 scenes are\n",
      "459 horror movie\n",
      "459 can get\n",
      "458 the part\n",
      "458 go on\n",
      "458 as far\n",
      "458 and no\n",
      "457 who have\n",
      "457 could not\n",
      "455 the problem\n",
      "455 one thing\n",
      "455 and at\n",
      "454 of being\n",
      "454 not in\n",
      "454 and its\n",
      "453 under the\n",
      "453 seen this\n",
      "453 is only\n",
      "453 goes to\n",
      "453 films that\n",
      "453 as you\n",
      "453 and over\n",
      "452 br my\n",
      "452 an hour\n",
      "451 of how\n",
      "451 can only\n",
      "451 an interesting\n",
      "450 years later\n",
      "450 with him\n",
      "450 here is\n",
      "449 the late\n",
      "448 interested in\n",
      "448 and don\n",
      "446 this story\n",
      "446 but still\n",
      "445 see how\n",
      "445 it very\n",
      "445 here and\n",
      "444 to save\n",
      "444 came out\n",
      "443 to their\n",
      "443 plot and\n",
      "443 and can\n",
      "442 to mention\n",
      "442 that all\n",
      "442 however the\n",
      "442 got to\n",
      "441 it like\n",
      "440 of one\n",
      "440 horror film\n",
      "440 for their\n",
      "439 you would\n",
      "439 series of\n",
      "439 make this\n",
      "439 is on\n",
      "439 compared to\n",
      "439 but when\n",
      "439 and also\n",
      "438 one that\n",
      "438 long time\n",
      "438 it makes\n",
      "438 his best\n",
      "438 first of\n",
      "438 br all\n",
      "438 br 10\n",
      "437 when was\n",
      "437 they do\n",
      "437 it isn\n",
      "437 if not\n",
      "436 may not\n",
      "436 amount of\n",
      "435 were the\n",
      "435 the production\n",
      "435 more like\n",
      "435 by his\n",
      "434 you to\n",
      "434 must see\n",
      "434 make you\n",
      "434 do it\n",
      "434 are in\n",
      "433 too bad\n",
      "433 not so\n",
      "433 it can\n",
      "433 guy who\n",
      "432 think this\n",
      "432 are very\n",
      "432 are just\n",
      "430 up on\n",
      "430 something that\n",
      "430 meant to\n",
      "430 it comes\n",
      "429 way of\n",
      "428 the future\n",
      "428 much to\n",
      "427 way the\n",
      "427 role of\n",
      "426 the police\n",
      "426 many people\n",
      "425 behind the\n",
      "424 to him\n",
      "424 the three\n",
      "424 out for\n",
      "424 and most\n",
      "422 on tv\n",
      "421 we can\n",
      "421 then you\n",
      "421 me the\n",
      "421 is too\n",
      "421 and on\n",
      "420 would not\n",
      "419 world of\n",
      "419 probably the\n",
      "419 know the\n",
      "418 they don\n",
      "418 plays the\n",
      "418 but what\n",
      "417 time it\n",
      "417 on their\n",
      "417 movies like\n",
      "417 just like\n",
      "417 it an\n",
      "416 my life\n",
      "416 in and\n",
      "416 don get\n",
      "416 but as\n",
      "415 who can\n",
      "415 get it\n",
      "415 find it\n",
      "414 except for\n",
      "414 chance to\n",
      "414 any other\n",
      "413 they could\n",
      "413 love the\n",
      "411 that his\n",
      "411 once again\n",
      "411 it may\n",
      "411 as his\n",
      "410 to understand\n",
      "410 it at\n",
      "410 despite the\n",
      "409 and are\n",
      "408 you might\n",
      "408 will not\n",
      "408 how much\n",
      "408 half of\n",
      "408 ends up\n",
      "407 worth watching\n",
      "407 thing about\n",
      "407 than that\n",
      "407 one and\n",
      "407 movie you\n",
      "407 just the\n",
      "407 fans of\n",
      "407 at this\n",
      "406 out that\n",
      "406 is definitely\n",
      "406 but is\n",
      "406 br one\n",
      "406 again and\n",
      "405 to life\n",
      "405 the dead\n",
      "403 decides to\n",
      "402 find the\n",
      "402 bad movie\n",
      "401 which the\n",
      "401 was no\n",
      "401 it didn\n",
      "401 and had\n",
      "400 something to\n",
      "400 side of\n",
      "400 only the\n",
      "400 but for\n",
      "399 that were\n",
      "399 script is\n",
      "399 let me\n",
      "399 for one\n",
      "398 the movies\n",
      "398 is supposed\n",
      "398 found it\n",
      "398 characters in\n",
      "397 his father\n",
      "397 films and\n",
      "397 film making\n",
      "397 black and\n",
      "396 of some\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Term Frequency–Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The tf-idf value of term t in document d is:\n",
    "\n",
    "TF-IDF(t,d) = tf(t,d) x idf(t)\n",
    "\n",
    "where,\n",
    "\n",
    "tf(t,d) = count of term t in document d\n",
    "\n",
    "N = total number of documents\n",
    "\n",
    "df(t) = number of documents that term t occurs in\n",
    "\n",
    "idf(t) = N/df(t)\n",
    "\n",
    "A \"term\" is a token in our case."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "n5oAIE_suXQb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Image(\"tf_idf.png\") # (Jurafsky, D., & Martin, J. H., 2019. Chap. 6, p. 14)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(df_train.review.values)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kpyX2wKuWxY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Document similarity with tf-idf vectors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "X_train_tfidf[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 74849)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n",
    "cosine_similarities = cosine_similarity(X_train_tfidf[0], X_train_tfidf).flatten()\n",
    "\n",
    "indices = cosine_similarities.argsort()[::-1] # in descending order \n",
    "print(\"most similar:\",indices[:10])\n",
    "print(\"least similar\", indices[-9:])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "most similar: [    0 23558 23470 17790  4719 24385 20550  6001  6096 20886]\n",
      "least similar [14867 11975 17497  5623  9340 14494 18802 16480 23603]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "print(df_train.review.values[0])\n",
    "print()\n",
    "print(\"most similar: \", df_train.review.values[23558])\n",
    "print()\n",
    "print(\"least similar: \", df_train.review.values[23603])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Forbidden Siren is based upon the Siren 2 Playstation 2 (so many 2s) game. Like most video game turned movies, I would say the majority don't translate into a different medium really well. And that goes for this one too, painfully.<br /><br />There's a pretty long prologue which explains and sets the premise for the story, and the mysterious island on which a writer (Leo Morimoto) and his children, daughter Yuki (Yui Ichikawa) and son Hideo (Jun Nishiyama) come to move into. The villagers don't look all too friendly, and soon enough, sound advice is given about the siren on the island, to stay indoors once the siren starts wailing.<br /><br />Naturally and slowly, things start to go bump, and our siblings go on a mission beating around the bush to discover exactly what is happening on this unfriendly island with its strange inhabitants. But in truth, you will not bother with what's going on, as folklore and fairy tales get thrown in to convolute the plot even more. What was really pushing it into the realm of bad comedy are its unwittingly ill-placed-out-of-the-norm moments which just drew pitiful giggles at its sheer stupidity, until it's explained much later. It's one thing trying to come up and present something smart, but another thing doing it convincingly and with loopholes covered.<br /><br />Despite it clocking in under 90 minutes - I think it's a horror movie phenomenon to have that as a runtime benchmark - it gives that almost two hour feel with its slow buildup to tell what it wants to. Things begin to pick up toward the last 20 minutes, but it's a classic case of too little too late.<br /><br />What saves the movie is how it changes tack and its revelation at the end. Again this is a common device used to try and elevate a seemingly simple horror movie into something a little bit extra in the hope of wowing an audience. It turned out rather satisfactorily, but leaves a bad aftertaste as you'll feel cheated somewhat. There are two ways a twist will make you feel - it either elevates the movie to a memorable level, or provides you with that hokey feeling. Unfortunately Forbidden Siren belonged more to the latter.<br /><br />The saving grace will be its cinematography with its use of light, shadows and mirrors, but I will be that explicit - it's still not worth the time, so better to avoid this.\n",
      "\n",
      "most similar:  Okay, so I'm not a big video game buff, but was the game House of the Dead really famous enough to make a movie from? Sure, they went as far as to actually put in quick video game clips throughout the movie, as though justifying any particular scene of violence, but there are dozens and dozens of games that look exactly the same, with the hand in the bottom on the screen, supposedly your own, holding whatever weapon and goo-ing all kinds of aliens or walking dead or snipers or whatever the case may be.<br /><br />It's an interesting premise in House of the Dead, with a lot of college kids (LOADED college kids, as it were, kids who are able to pay some fisherman something like $1,500 just for a ride after they miss their boat) trying to get out to this island for what is supposed to be the rave of the year. The first thing that comes to mind about House of the Dead after watching it is that it has become increasingly clear that modern horror movies have become nothing more than an exercise in coming up with creative ways to get a lot of scantily clad teenagers into exactly the same situations. At least in this case, the fact that they were on their way to a rave excuses the way the girls are dressed. They look badly out of place running around the woods in cute little halter-tops, but at least they THOUGHT they were dressed for the occasion.<br /><br />Clint Howard, tellingly the most interesting character in the film by far, delivers an absolutely awful performance, the greatness of which overshadows every other actor in the movie. I can't stand it when well-known actors change their accents in movies, it is so rarely effective, and Howard here shows that it is equally flat to have an well-known actor pretend that he's this hardened fisherman with a raspy voice from years of breathing salty air. He didn't even rasp well. It sounded like he was eating a cinnamon roll before shooting and accidentally inhaled some powdered sugar or something. Real tough there, Clint! I expected more from him, but then again, he did agree to a part in this mess.<br /><br />Once we get to the island, the movie temporarily turns into any one of the Friday the 13th movies that took place at Camp Crystal Lake. Lots of teenagers played by actors who were way too old for their parts getting naked and then killed. The nudity was impressive, I guess, but let's consider something for a minute. These kids pay almost two grand to get out to this island to go to the Rave Of The Year, find NO ONE, and say, well, who wants a beer! Even the guy who pulled that stack of hundreds out of his wallet to get them all over there didn't think anything of it that they found a full bar and not a single solitary person in sight. Here you have the input from director Uwe Boll - There's alcohol! They won't notice that the party they came for consists of no one but themselves!<br /><br />So not only do they start drinking, not minding the fact that the whole party seems to have vacated the island, but when one of the girls goes off into the dark woods to find out where everyone is (dragging one other girl and one of the guys reluctantly along), the guy and the girl who stay behind to get smashed decide that it would be a great idea to strip down for a quickie now that they're alone. It's like they expected to find the island empty, and now that they rest of the people that they came over with were gone for a little while, they would have some privacy since there's no one else around. Brilliant!<br /><br />Now for the things that everyone hated, judging by the reviews that I've read about the movie. Yes, intersplicing shots from the video game into the movie, mostly in order to show that, yes, the movie was being faithful to/directly copying the video game. Sure, it was a stupid idea. I can't imagine who thought up that little nugget, but worse than that is the Matrix-style bullet time scenes that were thrown in over and over and over and over. After the first time (at which point I found it pretentious and cheesy for a movie like this to have a shot like that as though it was something original) it is noticeable more for the technique of the shot itself rather than any dramatic meaning or creation of any kind of tension for the film.<br /><br />One of the things that makes a zombie film scary and gets you on the edge of your seat is to have them slowly but relentlessly coming after the living humans, who are much faster but getting tired, running out of places to run, and with a terrifying shortage of things with which to fight the zombies off with. The first two are done right in the movie, the kids are terrified and don't have a lot of places to run since they're on an island, but since they caught a ride over with a smuggler, they find themselves heavily armed. And I mean that very strongly. I mean, these people have everything from machine guns to hand grenades, which removes most of the tension of the impending walking dead.<br /><br />Then you have what I call the techno-slasher scene. Since the rave never happened, and I guess since Uwe Boll thought people were going to be disappointed at not hearing any techno music in the movie, there's one scene right in the middle where all the humans are fighting off the living dead, and amazingly enough it turns into something of a music video. There's techno music blasting as the shots are edited together faster and faster until it's nothing but a blur of gory shot, mostly only about 5 frames long (which is about 1/6 of a second) flashing across the screen in time with the speed techno music. Clever, I guess, but it has no place in a horror movie because it completely removes any sense of scariness or tension of even the gross-out effect because you can't see any one thing for long enough to react to it. You're just watching these shots fly across the screen and wondering what the hell the director was thinking when he decided that it would be a good idea to put something like this in the movie.<br /><br />I've seen a lot of people compare this movie to Resident Evil, mostly claiming that it copies the premise of it, and they're exactly right. I appreciate that at least here, as was not the case in Resident Evil, it wasn't some man-made virus that turned people into walking dead that were able to infect other people, changing them the way vampires turn others into vampires. 28 Days Later was also clearly an inspiration for this movie, it's just too bad that House of the Dead didn't do a single original thing, except for the somewhat moronic idea of putting in quick shots of the video game on which it is based, just in case you forget. I really think that this should have been a much better movie. While obviously I can't say that I know much about the game it's based on, just the title and the movie poster deserve a much better movie, but unfortunately I think that's more often the case than not with horror movies. It's really kind of sad when a movie comes out that is so obviously advertised as a no-holds-barred horror film, and the scariest thing in the entire movie is the closing shot, which suggests the possibility of a sequel.\n",
      "\n",
      "least similar:  Cool idea... botched writing, botched directing, botched editing, botched acting. Sorta makes me wish I could play God and strike everyone involved in making this film with several bolts of lightning.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The similar text is also about a movie based on a video games! Also notice the similarity of lengths and that the least similar review is very short."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vector semantics / Vector Space Models / (word) embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "YouTubeVideo('hcfgqgGHF4M', width=740, height=460)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1daa315e0>"
      ],
      "image/jpeg": "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",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"740\"\n",
       "            height=\"460\"\n",
       "            src=\"https://www.youtube.com/embed/hcfgqgGHF4M\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Semantics: The *meaning* or catagory of a word.\n",
    "\n",
    "Context is important to understand a word and give it a descriptive representation (to encoding it's meaning). \n",
    "\n",
    "Assumption/intution: Two words are similar if they appear in similar contexts.\n",
    "\n",
    "Now, instead just counting the words, we use a seperate model that should learn good feature vectors based on the above assumption. A popular model is called Word2Vec, which can learn feature vectors (i.e. word embeddings) in two different ways:\n",
    "\n",
    "\n",
    "Word2Vec architectures:\n",
    "\n",
    "- CBOW: Learn word embeddings by predicting a focus word given a context. \"The _ in the hat\".\n",
    "\n",
    "- Skip-gram: Learn word embeddings by predicting context words given focus word. \"_ cat _ _ _\".\n",
    "\n",
    "(https://arxiv.org/pdf/1301.3781.pdf ,\n",
    "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf, https://radimrehurek.com/gensim/models/word2vec.html)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "LxxVF50zJSbF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# using the train_sents from earlier (the lowercased and tokenized sentences)\n",
    "model = Word2Vec(train_sents, vector_size=50)#the default learning algorithm is CBOW. To use skip-gram use the paramter sg=1.\n",
    "\n",
    "# You can load pretrained embeddings downloaded from: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing \n",
    "# BUT this takes up a lot of space (the file is over 1 GB) and a lot of RAM when you try to use it.\n",
    "# If you want to try it, write this:\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(directory_path+'GoogleNews-vectors-negative300.bin', binary=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYhUesNDJZJd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "print(model.wv['good'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.36020932  0.5237038   0.09203231 -0.7985827  -0.47821242  1.533309\n",
      "  0.6636292  -2.1551619  -2.168445   -0.38255715  3.0424228   0.5550893\n",
      " -1.0563012  -4.4051547  -1.0041999   0.6110338  -2.0641377   2.840002\n",
      "  2.5321064   2.224186   -3.4814236   0.57272595 -0.5314392   5.6455235\n",
      " -0.89771605 -1.8570007   1.0396284  -1.2279559   1.4617805  -0.8374904\n",
      " -2.7190177  -0.75452673 -1.5104675   0.10205208 -1.2518384  -4.6131387\n",
      " -2.5576777   1.003671    0.35106316 -0.4714529   6.2724032  -2.2110791\n",
      " -1.3814138  -0.7281033   1.0273894  -2.2830987   0.66250896 -0.7005316\n",
      " -3.5397406  -1.8675348 ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# Reducing the 50-dimensional vectors to 2 dimensions in order to visualise selected words.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "words = [\"drama\",\"comedy\", \"good\", \"great\", \"bad\", \"horrible\", \"cat\", \"dog\"]\n",
    "\n",
    "X = [model.wv['drama'], model.wv['comedy'], \n",
    "     model.wv['good'], model.wv['great'], \n",
    "     model.wv['bad'], model.wv['horrible'], \n",
    "     model.wv['cat'], model.wv['dog']]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X_r[:,0], X_r[:,1], edgecolors='k', c='r')\n",
    "for word, (x,y) in zip(words, X_r):\n",
    "    plt.text(x+0.2, y+0.1, word)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a unique embedding vector mapped to each word. But what if we want to work with sentences/documents that contain many words and with a variying number of words in them? If you are doing machine learning (and are not using some fancy model that can handle varying lengts), the input vectors for your model have to be of same length. And the input of one element (for example one document), should preferably be a vector rather than a matrix (i.e. having an array with arrays of word embeddings as input will not work in most cases). So you need to make a decision of how to to use word embeddings for such cases. \n",
    "\n",
    "You could, for example, concatenate the vectors and choose a cut-off point for a given maximum number of words, where every sentence longer than that will be cut short, and every sentence shorter will be appended vectors containing zeroes. Or you can simply take the mean at axis 0, such that each document is represented by a single vector of length 50. I show the latter here:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# Take the mean of the word vectors to get one vector for each text \n",
    "# (rather than a list of word vectors for each text)\n",
    "\n",
    "embedded_text = np.array([np.mean([model.wv[w] if w in model.wv.key_to_index.keys() else np.zeros(50) for w in words], axis=0) for words in train_sents])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "embedded_text.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(25000, 50)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now check similarities between documents/reviews again and see how the results differ. You can also you the document embeddings in a Machine Learning model or for some type of clustering.\n",
    "\n",
    "Optional: Check whether increasing the vector_size for the word2vec model makes the similarites more meaningful."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "# Using the averaged word2vec document embeddings to find similar documents:\n",
    "cosine_similarities = cosine_similarity(embedded_text[0].reshape(1,50), embedded_text[:]).flatten()\n",
    "indices = cosine_similarities.argsort()[::-1] # in descending order \n",
    "print(\"most similar:\",indices[:10])\n",
    "print(\"least similar\", indices[-9:])\n",
    "print(df_train.review.values[0])\n",
    "print()\n",
    "print(\"most similar: \", df_train.review.values[5052])\n",
    "print()\n",
    "print(\"least similar: \", df_train.review.values[22007])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "most similar: [    0 24254  5853 13599  5052  2360 24385 17951 23832  9144]\n",
      "least similar [22972 12780  9527 22666 10288 18819  7270 19309 22007]\n",
      "Forbidden Siren is based upon the Siren 2 Playstation 2 (so many 2s) game. Like most video game turned movies, I would say the majority don't translate into a different medium really well. And that goes for this one too, painfully.<br /><br />There's a pretty long prologue which explains and sets the premise for the story, and the mysterious island on which a writer (Leo Morimoto) and his children, daughter Yuki (Yui Ichikawa) and son Hideo (Jun Nishiyama) come to move into. The villagers don't look all too friendly, and soon enough, sound advice is given about the siren on the island, to stay indoors once the siren starts wailing.<br /><br />Naturally and slowly, things start to go bump, and our siblings go on a mission beating around the bush to discover exactly what is happening on this unfriendly island with its strange inhabitants. But in truth, you will not bother with what's going on, as folklore and fairy tales get thrown in to convolute the plot even more. What was really pushing it into the realm of bad comedy are its unwittingly ill-placed-out-of-the-norm moments which just drew pitiful giggles at its sheer stupidity, until it's explained much later. It's one thing trying to come up and present something smart, but another thing doing it convincingly and with loopholes covered.<br /><br />Despite it clocking in under 90 minutes - I think it's a horror movie phenomenon to have that as a runtime benchmark - it gives that almost two hour feel with its slow buildup to tell what it wants to. Things begin to pick up toward the last 20 minutes, but it's a classic case of too little too late.<br /><br />What saves the movie is how it changes tack and its revelation at the end. Again this is a common device used to try and elevate a seemingly simple horror movie into something a little bit extra in the hope of wowing an audience. It turned out rather satisfactorily, but leaves a bad aftertaste as you'll feel cheated somewhat. There are two ways a twist will make you feel - it either elevates the movie to a memorable level, or provides you with that hokey feeling. Unfortunately Forbidden Siren belonged more to the latter.<br /><br />The saving grace will be its cinematography with its use of light, shadows and mirrors, but I will be that explicit - it's still not worth the time, so better to avoid this.\n",
      "\n",
      "most similar:  Before the regular comments, my main curiosity about THIS IS NOT A LOVE SONG is that while there's a running time listed on IMDb of 94 minutes, the DVD from Wellspring Media in the United States runs 88 minutes. Any input on this is appreciated!<br /><br />Two friends with very rough lives take on the road for an adventure. What they wind up in is just that, with one accidentally shooting a girl and the two escaping by foot into the countryside. Rather than just a big chase, the film is complicated by the the daft and rather childlike Spike behaving inappropriately, and clutching his boom box like a teddy bear. Some viewers may dislike the story based solely upon the character Spike, but without a bit of frustration added to the story, the film would have been too easy. You'll notice the way the more stable character Heaton refers to Spike as \"big man\" in contrast to Spike's \"kid out of control\" attitude and behavior. Frankly, I too was aggravated by Spike's ridiculous actions, especially the spray can sniffing, but in a desperate situation it's apparent someone of his mentality would choose an temporary escape. But, Heaton was there to keep things in check up until things get way over his head as well.<br /><br />Kenny Glenaan as Heaton is a marvel, and after a while I quit wondering why in the heck he would want to pick Spike up from prison and continue a friendship, due to Glenaan's great performance. After all, there are many many reasons during their run that would be a good idea for Heaton to just ditch Spike and try to save himself. I suppose Heaton felt like a protective older brother to Spike, and the loyalty between the two is hard to break -- until things get too desperate.<br /><br />While some of the cinematography is indeed artsy, it does offer more flavor to story instead of just shots of the men running through the wilderness. The beautiful landscapes, rain, and vast gray skies offer a somber tone that increases the feel of the tragic circumstances. The score is unusual as well, and the use of Public Image Ltd.'s song \"This Is Not A Love Song\" and as the title of the film is quite smart.<br /><br />Overall, it's understandable if you don't care for THIS IS NOT A LOVE SONG as it's focused on two contrasting personalities escaping from another man determined to hunt them down (played by a cool, quiet David Bradley). It's not big-budget action entertainment. For the rest of us that enjoy seeking out something minimal and dramatic, it's time worthwhile spent, and it DOES offer some extremely tense moments that have you holding your breath a bit.<br /><br />I'm really enjoying the films coming out of Scotland recently, with the likes of this one, Dog Soldiers, and The Devil's Tattoo. I'm also a bit thankful for the subtitles offered on this DVD, as the accents are sometimes lightning fast and difficult for some viewers like me to understand. <br /><br />Frustrating, dark, and often tense, THIS IS NOT A LOVE SONG is very tragic yet engrossing storytelling.<br /><br />\n",
      "\n",
      "least similar:  Creative use of modern and mystical elements: 1956 Cadillac convertible to transport evil stepmother Kathleen Turner (John Waters' \"Serial Mom\") and the 2 twisted sisters; Queen Mab as the faerie godmother; David Warner (Evil in \"Time Bandits\") in redcoat at court; Cinderella (she's a babe) shovelling coal into an insatiable furnace; Cinderella and her prince charming both look like (and act like) rock stars. Isle of Man locations.\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There exists many methods for learning good word embeddings, and a lot of pre-trained options as well, but these a primarily available for English text. Here are three other popular options besides Word2Vec, for those who are interested in diving more into that (but I do not expect you to do so for this course):\n",
    "- FastText\n",
    "- Glove\n",
    "- BERT\n",
    "\n",
    "BERT is currently the most widely used model for representing text, especially it's large pretrained embeddings. But it is perhaps also more complex to get started with, and definetly less interpretable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text classification\n",
    "\n",
    "We will now build a classifier that will classify text as having a positive (1) or negative (0) sentiment!"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "G6VqM8OkhssH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "YouTubeVideo('1OFSYHkecbA', width=740, height=460)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x17bfa4550>"
      ],
      "image/jpeg": "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",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"740\"\n",
       "            height=\"460\"\n",
       "            src=\"https://www.youtube.com/embed/1OFSYHkecbA\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text classification with logistic regression"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "AVSmfjPKbmkG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# Get feature vectors\n",
    "tfidf = TfidfVectorizer()\n",
    "# use your own preprocessing function in the vectorizer when you've finished that exercise:\n",
    "#tfidf = TfidfVectorizer(tokenizer=preprocess)\n",
    "X_train_tfidf = tfidf.fit_transform(df_train.review.values)\n",
    "X_test_tfidf = tfidf.transform(df_test.review.values)\n",
    "\n",
    "# labels\n",
    "y_train = df_train.sentiment.values\n",
    "y_test = df_test.sentiment.values\n",
    "\n",
    "# classifier\n",
    "lr = LogisticRegression(random_state=0)\n",
    "\n",
    "#training\n",
    "lr.fit(X_train_tfidf,y_train)\n",
    "\n",
    "#testing\n",
    "train_preds = lr.predict(X_train_tfidf)\n",
    "test_preds = lr.predict(X_test_tfidf)\n",
    "print(\"training accuracy:\", np.mean([(train_preds==y_train)]))\n",
    "print(\"testing accuracy:\", np.mean([(test_preds==y_test)]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training accuracy: 0.93328\n",
      "testing accuracy: 0.88316\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3E28RXfbiR3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at the features' coeffiecients\n",
    "\n",
    "Training a logistic regression classifier and inspecting the coefficients is a great - and simple - way to learn what words are important for a class/category. You can use the result in a word cloud! Or if you have texts written by, for example, two political parties and with sentiment labels (or some other labels) you can train a model on each party and compare most important words - this can tell you something about the language use in different parties, and if using higher orders of n-grams, you may see which subjects are more often talked about in a positive/negative manner."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "features = ['_'.join(s.split()) for s in tfidf.get_feature_names()]\n",
    "coefficients = lr.coef_\n",
    "coefs_df = pd.DataFrame.from_records(coefficients, columns=features)\n",
    "coefs_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         00       000  0000000000001     00001     00015      000s       001  \\\n",
       "0 -0.096569 -0.305337      -0.033966 -0.038521 -0.005573  0.024453 -0.032348   \n",
       "\n",
       "     003830       006       007  ...     était      état       étc     évery  \\\n",
       "0  0.031637  0.013828  0.056278  ...  0.041276  0.047509  0.034055 -0.081397   \n",
       "\n",
       "     êxtase        ís      ísnt   østbye      über  üvegtigris  \n",
       "0  0.016038  0.002182 -0.035297  0.01018 -0.084577   -0.053029  \n",
       "\n",
       "[1 rows x 74849 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000000000001</th>\n",
       "      <th>00001</th>\n",
       "      <th>00015</th>\n",
       "      <th>000s</th>\n",
       "      <th>001</th>\n",
       "      <th>003830</th>\n",
       "      <th>006</th>\n",
       "      <th>007</th>\n",
       "      <th>...</th>\n",
       "      <th>était</th>\n",
       "      <th>état</th>\n",
       "      <th>étc</th>\n",
       "      <th>évery</th>\n",
       "      <th>êxtase</th>\n",
       "      <th>ís</th>\n",
       "      <th>ísnt</th>\n",
       "      <th>østbye</th>\n",
       "      <th>über</th>\n",
       "      <th>üvegtigris</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.096569</td>\n",
       "      <td>-0.305337</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.038521</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>0.024453</td>\n",
       "      <td>-0.032348</td>\n",
       "      <td>0.031637</td>\n",
       "      <td>0.013828</td>\n",
       "      <td>0.056278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041276</td>\n",
       "      <td>0.047509</td>\n",
       "      <td>0.034055</td>\n",
       "      <td>-0.081397</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>-0.035297</td>\n",
       "      <td>0.01018</td>\n",
       "      <td>-0.084577</td>\n",
       "      <td>-0.053029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 74849 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "print(coefs_df.T.sort_values(by=[0], ascending=False).head(20))\n",
    "print()\n",
    "print(coefs_df.T.sort_values(by=[0], ascending=True).head(20))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                   0\n",
      "great       7.597993\n",
      "excellent   6.185474\n",
      "best        5.127967\n",
      "perfect     4.818600\n",
      "wonderful   4.676444\n",
      "amazing     4.164898\n",
      "well        4.061736\n",
      "loved       3.835260\n",
      "fun         3.805573\n",
      "today       3.782701\n",
      "love        3.782314\n",
      "favorite    3.743822\n",
      "enjoyed     3.513010\n",
      "highly      3.390157\n",
      "brilliant   3.353821\n",
      "it          3.330147\n",
      "superb      3.321679\n",
      "and         3.176546\n",
      "definitely  3.031838\n",
      "still       2.983629\n",
      "\n",
      "                      0\n",
      "worst         -9.242321\n",
      "bad           -7.956060\n",
      "awful         -6.493183\n",
      "waste         -6.278238\n",
      "boring        -6.020011\n",
      "poor          -5.448303\n",
      "terrible      -4.848693\n",
      "nothing       -4.761642\n",
      "worse         -4.577197\n",
      "no            -4.394958\n",
      "horrible      -4.208934\n",
      "poorly        -4.167156\n",
      "dull          -4.155190\n",
      "unfortunately -3.984437\n",
      "script        -3.871557\n",
      "annoying      -3.827654\n",
      "stupid        -3.818522\n",
      "ridiculous    -3.668310\n",
      "minutes       -3.600162\n",
      "instead       -3.526372\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Earlier during this course, you have been taught how to do cross-validation. If you want to develop a good model for sentiment classification, it is good practice to use cross-validation and experiment with different vectorizers and classifiers. We will not be doing it in this notebook, but in the PML textbook page 268 you will find an example of how to use sklearn's GridSearchCV to experiment with the count and tfidf vectorizer and with different hyperparamters for both the vectorizers (such as the ngram_range) and the logistic regression classifier, to find the best model."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "X7x4TpmbcHHk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 3: Cross-domain evaluation (to assess generalizability)\n",
    "\n",
    "This is a larger exercise where you will put together what you have learned above and start doing an error analysis to become familiar with some limitations of text classification. In this exercise we are including a new dataset, and we will be reusing the vectorizer and classifier defined above (tfidf and lr, respectively) - if you haven't yet run the cells above, containing this code, then do that now. Then follow these steps:\n",
    "\n",
    "1. Download the Airline sentiment dataset from Kaggle (you may need to login to download it): https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "\n",
    "General note on tweets: Tweets are messy (generally require a lot a of preproccesing if used to train a ML model).\n",
    "\n",
    "2. Load the data into a dataframe named tweet_df. \n",
    "\n",
    "3. As you can see, the samples have three labels (neg, pos, neutral). You have to either relabel neutral tweets as positive or remove neutral tweets from the dataframe entirely.\n",
    "\n",
    "4. Extract features from the tweets using the tf-idf vectorizer previously fitted on the Imdb review training data -- meaning, you should NOT use fit_transform(), but only use .transform().\n",
    "\n",
    "5. Use the previously trained logistic regression classifier and make predictions over your vectorized tweets -- meaning, you should NOT use .fit(), but only use .predict(). Report testing accuracy.\n",
    "\n",
    "6. Finally, answer these questions:\n",
    "\n",
    "    A) How well does your model perform in this other domain? \n",
    "    \n",
    "    B) Can you identify the types of errors it makes? (hint: you can make a confusion matrix and you can look at the words, and their respective coefficients, of relevant examples of misclassification). If you don't know what to do, just skip this.\n",
    "    \n",
    "    C) Now, **fit a new vectorizer and logistic regression classifier** on the airline tweets (remove mentions, @user, as the very least amount of preprocessing. To do this, you can write ```[re.sub(\"@[A-Za-z0-9]+\",\"\",sent) for sent in tweet_df.text.values]```). What are the difference between most important features (high and low coeffiecients) between this model and your original model trained on the Imdb reviews? And what does this mean for the models' ability to generalize to new data?"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "jHLDsg-Cvo8o"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "# YOUR CODE HERE\n",
    "df_tweets = pd.read_csv(\"tweets.csv\")\n",
    "df_tweets = df_tweets[df_tweets['airline_sentiment'] != 'neutral']\n",
    "df_tweets['sentiment'] = (df_tweets['airline_sentiment'] == 'positive')\n",
    "df_tweets_tfidf = tfidf.transform(df_tweets.text.values)\n",
    "tweet_preds = lr.predict(df_tweets_tfidf)\n",
    "print(\"training accuracy:\", np.mean([(tweet_preds==df_tweets['sentiment'])]))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PH1DuKA5cq6Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "df_tweets['CM'] = 0\n",
    "\n",
    "df_tweets.loc[(df_tweets['sentiment'] == tweet_preds) & (df_tweets['sentiment'] == True), 'CM'] = 'TP'\n",
    "df_tweets.loc[(df_tweets['sentiment'] != tweet_preds) & (df_tweets['sentiment'] == True), 'CM'] = 'FP'\n",
    "df_tweets.loc[(df_tweets['sentiment'] == tweet_preds) & (df_tweets['sentiment'] == False), 'CM'] = 'TN'\n",
    "df_tweets.loc[(df_tweets['sentiment'] != tweet_preds) & (df_tweets['sentiment'] == False), 'CM'] = 'FN'\n",
    "\n",
    "df_tweets['CM'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TN    6141\n",
       "FN    3037\n",
       "TP    1837\n",
       "FP     526\n",
       "Name: CM, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "df_tweets[df_tweets['CM'].isin(['FP', 'FN'])][['airline_sentiment', 'text']].values"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['positive',\n",
       "        '@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)'],\n",
       "       ['positive',\n",
       "        '@VirginAmerica I &lt;3 pretty graphics. so much better than minimal iconography. :D'],\n",
       "       ['negative', '@VirginAmerica SFO-PDX schedule is still MIA.'],\n",
       "       ...,\n",
       "       ['negative',\n",
       "        '@AmericanAir How do I change my flight if the phone system keeps telling me that the representatives are busy?'],\n",
       "       ['negative',\n",
       "        '@AmericanAir my flight was Cancelled Flightled, leaving tomorrow morning. Auto rebooked for a Tuesday night flight but need to arrive Monday.'],\n",
       "       ['negative',\n",
       "        \"@AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\"]],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lexicons for sentiment analysis "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vader "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Vader lexicon\n",
    "analyser = vaderSentiment.SentimentIntensityAnalyzer()\n",
    "snt = analyser.polarity_scores(sentence)\n",
    "#print(sentence)\n",
    "print(snt)\n",
    "compound = snt[\"compound\"] # how to scoring works: https://github.com/cjhutto/vaderSentiment#about-the-scoring"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Afinn "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Afinn lexicon\n",
    "afn = Afinn(emoticons=True)\n",
    "print(afn.score(sentence))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "example_neg = \"congrats on making an all-together predictable movie\"\n",
    "example_pos = \"OMG I literally died!\"\n",
    "\n",
    "print(afn.score(example_neg))\n",
    "print(afn.score(example_pos))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SentiWordNet "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# lexicon can be accessed with NLTK:\n",
    "nltk.download('sentiwordnet')\n",
    "good = nltk.corpus.sentiwordnet.senti_synsets('good', 'a') # a=adjective\n",
    "for i in good:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"good.a.01 Definition: \", nltk.corpus.wordnet.synset('good.a.01').definition())\n",
    "print(\"good.a.03 Definition: \", nltk.corpus.wordnet.synset('good.a.03').definition())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 4: Lexicon-based sentiment classification (no Machine Learning)\n",
    "Use the Vader lexicon to classify each review in the test set. Use the \"compound\" score where compound<=0 = negative review (0) and compound>0=positive review (1). Report the classification accuracy."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "tUvd_GflbPfz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Your code here"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0insarXzbO9v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EXTRA \n",
    "\n",
    "### Topic Modelling example\n",
    "\n",
    "See https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.models import ldamodel\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(train_sents)\n",
    "\n",
    "# Create Corpus\n",
    "texts = train_sents\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_model = ldamodel.LdaModel(corpus=corpus,\n",
    "                               id2word=id2word,\n",
    "                                           num_topics=20,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lda_model.print_topics()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some NLP resources\n",
    "\n",
    "**NLTK** (as we have seen): Basic NLP tools such as tokenization, stemming, lemmatization, part-of-speech tagging (though not as good as the one from stanfordNLP), dependency parsing. https://www.nltk.org. Good introductory material here: https://www.nltk.org/book/\n",
    "\n",
    "**Wordnet**: A large lexicon of \"synsets\"; groups of words that are semantically closely related (synonyms). It can be used to infer word senses/meanings among other things https://wordnet.princeton.edu. \n",
    "\n",
    "**StanfordNLP (stanza)**: Tokenization, part-of-speech tagging, dependency parsing, named entity recognition, and more. https://stanfordnlp.github.io/stanza/.\n",
    "\n",
    "**Spacy**: More advanced NLP pipelines for feature extraction and more https://spacy.io\n",
    "\n",
    "**Gensim**: Popular models for word embeddings, e.g. word2vec and fastText, and more. https://radimrehurek.com/gensim/auto_examples/index.html\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Project ideas\n",
    "\n",
    "- Predicting election outcomes or market trends from sentiment\n",
    "- Stance or sentiment towards political parties\n",
    "- Hate speech detection\n",
    "- Analysing overaching topics in a debate\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "You've learned:\n",
    "- What to consider when preprocessing text\n",
    "- How to represent texts with numerical vectors\n",
    "- That you can find similar texts by calculating the distance between their vectors.\n",
    "- What word embeddings are and the basic intuition of it\n",
    "- Text classification\n",
    "- Possibilities with lexicons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Social Data Science: Text as Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}