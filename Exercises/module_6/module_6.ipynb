{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "> **DO YOU USE GITHUB?**  \n",
    "If True: print('Remember to make your edits in a personal copy of this notebook')  \n",
    "Else: print('You don't have to understand. Continue your life.')"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Module 6: Web Scraping 1\n",
    "\n",
    "In this module you will be introduced to `web scraping`: \n",
    "- What it web scraping?\n",
    "- How to web scrape?\n",
    "- Why is web scrpaing important to master as a data scientist?\n",
    "\n",
    "Readings for `session 6+7+8`:\n",
    "- [Python for Data Analysis, chapter 6](https://bedford-computing.co.uk/learning/wp-content/uploads/2015/10/Python-for-Data-Analysis.pdf)\n",
    "- [A Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "- [An introduction to web scraping with Python](https://towardsdatascience.com/an-introduction-to-web-scraping-with-python-a2601e8619e5)\n",
    "- [Introduction to Web Scraping using Selenium](https://medium.com/the-andela-way/introduction-to-web-scraping-using-selenium-7ec377a8cf72)\n",
    "\n",
    "Video materiale from `ISDS 2020`:\n",
    "- [Web Scraping 1](https://bit.ly/ISDS2021_6)\n",
    "- [Web Scraping 2](https://bit.ly/ISDS2021_7)\n",
    "- [Web Scraping 3](https://bit.ly/ISDS2021_8)\n",
    "\n",
    "Other ressources:\n",
    "- [Nicklas Webpage](https://nicklasjohansen.netlify.app/)\n",
    "- [Data Driven Organizational Analysis, Fall 2021](https://efteruddannelse.kurser.ku.dk/course/2021-2022/ASTK18379U)\n",
    "- [Master of Science (MSc) in Social Data Science](https://www.socialdatascience.dk/education)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ethical Considerations\n",
    "* If a regular user can’t access it, we shouldn’t try to get it [That is considered hacking](https://www.dr.dk/nyheder/penge/gjorde-opmaerksom-paa-cpr-hul-nu-bliver-han-politianmeldt-hacking). \n",
    "* Don't hit it to fast: Essentially a DENIAL OF SERVICE attack (DOS). [Again considered hacking](https://www.dr.dk/nyheder/indland/folketingets-hjemmeside-ramt-af-hacker-angreb). \n",
    "* Add headers stating your name and email with your requests to ensure transparency. \n",
    "* Be careful with copyrighted material.\n",
    "* Fair use (take only the stuff you need)\n",
    "* If monetizing on the data, be careful not to be in direct competition with whom you are taking the data from."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://github.com/snorreralund/images/raw/master/Sk%C3%A6rmbillede%202017-08-03%2014.46.32.png\"/>"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Web Scraping Recipe\n",
    "\n",
    "To scrape information from the web is:\n",
    "1. **MAPPING**: Finding URLs of the pages containing the information you want.\n",
    "2. **DOWNLOAD**: Fetching the pages via HTTP.\n",
    "3. **PARSE**: Extracting the information from HTML.  \n",
    "  \n",
    "  \n",
    "You could also add `connection`, `storing`, `logging`, etc.        \n",
    "   \n",
    "\n",
    "\n",
    "### Packages used\n",
    "Today we will mainly build on the python skills you have gotten so far, and tomorrow we will look into more specialized packages.\n",
    "\n",
    "* for connecting to the internet we use: **requests**\n",
    "* for parsing: **beautifulsoup** and **regex**\n",
    "* for automatic browsing / screen scraping: **selenium** \n",
    "* for mitigating errors we use: **time**\n",
    "\n",
    "We will write our scrapers with basic python, for larger projects consider looking into the packages **scrapy**"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check that you can import these lbraries\n",
    "# otherwise you they can easily be installed using pip\n",
    "# example: https://pypi.org/project/beautifulsoup4/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import selenium\n",
    "import time\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connecting to the Internet\n",
    "\n",
    "\n",
    "**Connecting to the internet** **HTTP**\n",
    "\n",
    "*URL* : the adressline in our browser.\n",
    "\n",
    "Via HTTP we send a **get** request to an *address* with *instructions* ( - or rather our dns service provider redirects our request to the right address)\n",
    "*Address / Domain*: www.google.com\n",
    "\n",
    "*Instructions*: /search?q=who+is+mister+miyagi\n",
    "\n",
    "*Header*: information send along with the request, including user agent (operating system, browser), cookies, and prefered encoding.\n",
    "\n",
    "*HTML*: HyperTextMarkupLanguage the language of displaying web content. More on this tomorrow.\n"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "response = requests.get('https://www.google.com')\n",
    "#response.text"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "response = requests.get('https://isdsucph.github.io/isds2021/')\n",
    "#response.text"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Static Webpage Example\n",
    "\n",
    "Visit the following website (https://www.basketball-reference.com/leagues/NBA_2018.html).\n",
    "\n",
    "The page displays tables of data that we want to collect.\n",
    "Tomorrow you will see how to parse such a table, but for now I want to show you a neat function that has already implemented this."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url = 'https://www.basketball-reference.com/leagues/NBA_2018.html' # link to the website\n",
    "dfs = pd.read_html(url) # parses all tables found on the page.\n",
    "dfs[0]"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we did not have a neat function we would have to navigate the website to point at the data we wanted to collect. Below I show how to find the headline of the table. This is something you will learn about in session 7."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url = 'https://www.basketball-reference.com/leagues/NBA_2018.html'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup.find_all('h2')[0].text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Navigating websites to collect links\n",
    "Now I will show you a few common ways of finding the links to the pages you want to scrape."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building URLS using a recognizable pattern.\n",
    "A nice trick is to understand how urls are constructed to communicate with a server. \n",
    "\n",
    "Lets look at how [jobindex.dk](https://www.jobindex.dk/) does it. We simply click around and take note at how the addressline changes.\n",
    "\n",
    "This will allow us to navigate the page, without having to parse information from the html or click any buttons.\n",
    "\n",
    "* / is like folders on your computer.\n",
    "* ? entails the start of a query with parameters \n",
    "* = defines a variable: e.g. page=1000 or offset = 100 or showNumber=20\n",
    "* & separates different parameters.\n",
    "* \\+ is html for whitespace"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mapping exercise\n",
    "url = 'https://www.jobindex.dk/jobsoegning/storkoebenhavn?page=2&q=python'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#soup"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jobs = int(soup.find('span',attrs={'class':'d-md-none'}).text[0:3])\n",
    "jobs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 20 jobs per page\n",
    "for i in range(round(jobs/20)+1):\n",
    "    print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(round(jobs/20)+1):\n",
    "    print('https://www.jobindex.dk/jobsoegning/storkoebenhavn?page=' + str(i+1) +'&q=python')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Good practices\n",
    "* Transparency: send your email and name in the header so webmasters will know you are not a malicious actor.\n",
    "* Ratelimiting: Make sure you don't hit their servers to hard.\n",
    "* Reliability: \n",
    "    * Make sure the scraper can handle exceptions (e.g. bad connection) without crashing.\n",
    "    * Keep a log.\n",
    "    * Store raw data.\n"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logging\n",
    "Even if logging is not important for the below exercises, get in the habit of using this class for connecting to the internet, to practice logging your activity.\n",
    "\n",
    "You should run `pip install scraping_class` to install the module to be used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import scraping_class\n",
    "logfile = 'log.csv'## name your log file.\n",
    "connector = scraping_class.Connector(logfile)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise Set 6: Web Scraping 1\n",
    "\n",
    "In this Exercise Set we shall practice our webscraping skills utiilizing only basic python.  \n",
    "We shall cover variations between static and dynamic pages and build. "
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise Section 6.1: Scraping Jobnet.dk\n",
    "\n",
    "This exercise you get to practice locating the request that the JavaScript sends to get the job data that it builds the joblistings from. You should use the **>Network Monitor<** tool in your browser. I recomend using Chrome.\n",
    "\n",
    "Furthermore you practice spotting how the pagination is done, without clicking on the next page button, but instead changing a small parameter in the URL."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex. 6.1.1:** Go to  www.jobnet.dk and investigate the page. Start your `mapping`. Figure our what url you need to scrape to collect jobposting data. Sometimes this can be hard and requires you to inspect the page.\n",
    "\n",
    "> **Ex. 6.1.2.:** Use the `request` module to collect the first 20 results and unpack the relevant `json` data into a `pandas` DataFrame.\n",
    "\n",
    "> **Ex. 6.1.3.:** How many results do you find in total? Store this number as 'n_listings' for later use."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#test_url = 'https://job.jobnet.dk/CV/FindWork?SearchString=python&Offset=0&SortValue=BestMatch'\n",
    "test_url = 'https://job.jobnet.dk/CV/FindWork/Search?&Offset=&SortValue=BestMatch'\n",
    "response = requests.get(test_url)\n",
    "res_json = response.json()\n",
    "df = pd.DataFrame(res_json['JobPositionPostings'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "col1 = df.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "col1 = sorted(df.columns)\n",
    "col = ['Abroad', 'AnonymousEmployer', 'AssignmentStartDate', 'AutomatchType', 'Country', \n",
    "                              'DetailsUrl', 'EmploymentType', 'FormattedLastDateApplication', 'HasLocationValues', \n",
    "                              'HiringOrgCVR', 'HiringOrgName', 'ID', 'IsExternal', 'IsHotjob', 'JobAnnouncementType', \n",
    "                              'JobHeadline', 'JobLogUrl', 'JoblogWorkTime', 'LastDateApplication', 'Latitude', 'Location',\n",
    "                              'Longitude', 'Municipality', 'Occupation', 'OccupationArea', 'OccupationGroup', \n",
    "                              'OrganisationId', 'PostalCode', 'PostalCodeName', 'PostingCreated', 'Presentation',\n",
    "                              'Region', 'ShareUrl', 'Title', 'Url', 'UseWorkPlaceAddressForJoblog', 'UserLoggedIn',\n",
    "                              'Weight', 'WorkHours', 'WorkPlaceAbroad', 'WorkPlaceAddress', 'WorkPlaceCity',\n",
    "                              'WorkPlaceNotStatic', 'WorkPlaceOtherAddress', 'WorkPlacePostalCode', 'WorkplaceID']\n",
    "set(col).symmetric_difference(col1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "assert sorted(df.columns) == ['Abroad', 'AnonymousEmployer', 'AssignmentStartDate', 'AutomatchType', 'Country', \n",
    "                              'DetailsUrl', 'EmploymentType', 'FormattedLastDateApplication', 'HasLocationValues', \n",
    "                              'HiringOrgCVR', 'HiringOrgName', 'ID', 'IsExternal', 'IsHotjob', 'JobAnnouncementType', \n",
    "                              'JobHeadline', 'JobLogUrl', 'JoblogWorkTime', 'LastDateApplication', 'Latitude', 'Location',\n",
    "                              'Longitude', 'Municipality', 'Occupation', 'OccupationArea', 'OccupationGroup', \n",
    "                              'OrganisationId', 'PostalCode', 'PostalCodeName', 'PostingCreated', 'Presentation',\n",
    "                              'Region', 'ShareUrl', 'Title', 'Url', 'UseWorkPlaceAddressForJoblog', 'UserLoggedIn',\n",
    "                              'Weight', 'WorkHours', 'WorkPlaceAbroad', 'WorkPlaceAddress', 'WorkPlaceCity',\n",
    "                              'WorkPlaceNotStatic', 'WorkPlaceOtherAddress', 'WorkPlacePostalCode', 'WorkplaceID']\n",
    "assert len(df) == 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_json = response.json()\n",
    "res_json.keys()\n",
    "n_listings = res_json['TotalResultCount']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex. 6.1.4:** This exercise is about paging the results. We need to understand the websites pagination scheme. \n",
    "\n",
    "> Now scroll down the webpage and press the next page button. See how the parameters of the url changes as you turn the pages.\n",
    "\n",
    "> **Ex. 6.1.5:** Design a`for` loop using the `range` function that changes this paging parameter in the URL. Use 'n_listings' from before to define the limits of the range function. Store these urls in a container. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_of_urls = []\n",
    "for i in range(0,n_listings+1,20):\n",
    "    url = f'https://job.jobnet.dk/CV/FindWork/Search?&Offset={i}&SortValue=BestMatch'\n",
    "    list_of_urls.append(url)\n",
    "\n",
    "#list_of_urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex.6.1.6:** Pick 20 random links using the `random.sample()` function and scrape their content. Use the `time.sleep()` function to limit the rate of your calls. Load all the results into a DataFrame. ***extra***: monitor the time left to completing the loop by using `tqdm.tqdm()` function.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random.sample(list_of_urls, 20)\n",
    "df = pd.DataFrame()\n",
    "for i in random.sample(list_of_urls, 20):\n",
    "    response = requests.get(i)\n",
    "    res_json = response.json()\n",
    "    df1 = pd.json_normalize(res_json['JobPositionPostings'])\n",
    "    df = pd.concat([df,df1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex.6.1.7:** Snorre Ralund, a resaercher at SODAS, has build a connector class. Repeat 6.1.6 but try to to use Snorres connector to log your activity.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "raise NotImplementedError()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise Section 6.2: Scraping Trustpilot.com\n",
    "Now for a slightly more elaborate, yet still simple scraping problem. Here we want to scrape trustpilot for user reviews. This data is very nice since it provides free labeled data (rating) to train a machine learning model to understand positive and negative sentiment. \n",
    "\n",
    "Here you will practice crawling a website collecting the links to each company review page, and finally locate another behind the scenes JavaScript request that gets the review data in a neat json format."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex. 6.2.1:** Visit the https://www.trustpilot.com/ website and locate the categories page.\n",
    "From this page you find links to company listings.\n",
    "\n",
    "> **Ex. 6.2.2:**\n",
    "Get the category page using the `requests` module and extract each link to a specific category page from the HTML. This can be done using the basic python `.split()` string method. Make sure only links within the ***/categories/*** section are kept, checking each string using the ```if 'pattern' in string``` condition. \n",
    "\n",
    "*(Hint: The links are relative. You need to add the domain name)*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Incorrect solution\n",
    "\"\"\"\n",
    "trust_url = 'https://www.trustpilot.com/categories'\n",
    "trust_response = requests.get(trust_url)\n",
    "trust_soup = BeautifulSoup(trust_response.content, 'html.parser')\n",
    "find_ = trust_soup.find_all('span', attrs={'class':'categories_categoryListItem__1dO4P'})\n",
    "#qwer = [x.split('<span class=\"categories_categoryListItem__1dO4P\">') for x in find_]\n",
    "#qwer\n",
    "listy = []\n",
    "for i in find_:\n",
    "    listy.append(\" \".join(str(i)[49:].split(' &amp; ')))\n",
    "    \n",
    "listy = [x[:-7] for x in listy]\n",
    "listy\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CORRECT SOLUTION\n",
    "new_url = 'https://www.trustpilot.com/categories'\n",
    "new_res = requests.get(new_url)\n",
    "new_soup = BeautifulSoup(new_res.content, 'html.parser')\n",
    "categories = new_soup.findAll('div',{'class':'categories_subCategoryItem__2Qwj8'})\n",
    "\n",
    "list_of_urls_trust = []\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    list_of_urls_trust.append(f\"https://www.trustpilot.com{categories[i].a['href']}\")\n",
    "\n",
    "list_of_urls_trust"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Incorrect solution\n",
    "\"\"\"\n",
    "links_with_text = []\n",
    "for a in trust_soup.find_all('a', href=True): \n",
    "    if a.text:\n",
    "        links_with_text.append(a['href'])\n",
    "links_with_text\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex. 6.2.3:** Get one of the category section links. Write a function to extract the links to the company review page from the HTML.\n",
    "\n",
    "> **Ex. 6.2.4:** Figure out how the pagination is done, by following how the url changes when pressing the **next page**-button to obtain more company listings. Write a function that builds links to paging all the company listing results of each category. This includes parsing the number of subpages of each category and changing the correct parameter in the url.\n",
    "\n",
    "(Hint: Find the maximum number of result pages, right before the next page button and make a loop change the page parameter of the url.)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex. 6.2.5:** Loop through all categories and build the paging links using the above defined function.\n",
    "\n",
    "> **Ex. 6.2.6:** Randomly pick one of category listing links you have generated, and get the links to the companies listed using the other function defined. \n",
    "\n",
    "> **Ex. 6.2.7:** Visit one of these links and inspect the **>Network Monitor<** to locate the request that loads the review data. Use the requests module to retrieve this link and unpack the json results to a pandas DataFrame.\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "328px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}