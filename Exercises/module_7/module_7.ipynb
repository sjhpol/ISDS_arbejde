{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "> **DO YOU USE GITHUB?**  \n",
    "If True: print('Remember to make your edits in a personal copy of this notebook')  \n",
    "Else: print('You don't have to understand. Continue your life.')"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Module 7: Web Scraping 2\n",
    "\n",
    "In module_6 your learned some powerful tricks. Tricks that will work when the data is already shipped in a neat format. However this is often not the case. In this session we shall learn the art of parsing unstructured text and a more principled and advanced method of parsing HTML.\n",
    "\n",
    "This will help you build ***custom datasets*** within just a few hours or days work, that would have taken ***months*** to curate and clean manually.\n",
    "\n",
    "\n",
    "\n",
    "Readings for `module 6+7+8`:\n",
    "- [Python for Data Analysis, chapter 6](https://bedford-computing.co.uk/learning/wp-content/uploads/2015/10/Python-for-Data-Analysis.pdf)\n",
    "- [A Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "- [An introduction to web scraping with Python](https://towardsdatascience.com/an-introduction-to-web-scraping-with-python-a2601e8619e5)\n",
    "- [Introduction to Web Scraping using Selenium](https://medium.com/the-andela-way/introduction-to-web-scraping-using-selenium-7ec377a8cf72)\n",
    "\n",
    "Video materiale from `ISDS 2020`:\n",
    "- [Web Scraping 1](https://bit.ly/ISDS2021_6)\n",
    "- [Web Scraping 2](https://bit.ly/ISDS2021_7)\n",
    "- [Web Scraping 3](https://bit.ly/ISDS2021_8)\n",
    "\n",
    "Other ressources:\n",
    "- [Nicklas Webpage](https://nicklasjohansen.netlify.app/)\n",
    "- [Data Driven Organizational Analysis, Fall 2021](https://efteruddannelse.kurser.ku.dk/course/2021-2022/ASTK18379U)\n",
    "- [Master of Science (MSc) in Social Data Science](https://www.socialdatascience.dk/education)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction to HTML\n",
    "[What is HTML?](https://www.w3schools.com/whatis/whatis_html.asp)  \n",
    "\n",
    "HTML has a Tree structure. \n",
    "\n",
    "Each node in the tree has:\n",
    "- Children, siblings, parents, descendants. \n",
    "- Ids and attributes\n",
    "\n",
    "<img src=\"http://www.openbookproject.net/tutorials/getdown/css/images/lesson4/HTMLDOMTree.png\"/>\n"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Important syntax and patterns\n",
    "_______________\n",
    "```html \n",
    "<p>The p tag indicates a paragraph <p/>\n",
    "```\n",
    "_______________\n",
    "```html \n",
    "<b>The b tag makes the text bold, giving us a clue to its importance </b>\n",
    "```\n",
    "output: <b>The b tag makes the text bold, giving us a clue to its importance </b>\n",
    "```html \n",
    "\n",
    "<em>The em tag emphasize the text</em>, giving us a clue to its importance\n",
    "```\n",
    "output: <em>The em tag makes emphasize the text</em>, giving us a clue to its importance\n",
    "___________\n",
    "```html \n",
    "<h1>h1</h1><h2>h2</h2><h3>h3</h3><b>Headers give similar clues</b>\n",
    "```\n",
    "output:\n",
    "<h1>h1</h1><h2>h2</h2><h3>h3</h3><b>Headers give similar clues</b>  \n",
    "  \n",
    "```html \n",
    "<a href=\"www.google.com\">The a tag creates a hyperlink <a/>\n",
    "```\n",
    "output: <a href=\"www.google.com\">The a tag creates a hyperlink <a/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How do we find our way around this tree?\n",
    "1. Regex: Extracting string patterns using .split and regular expresssions\n",
    "2. CSS-selectors: Specifying paths using css-selectors, xpath syntax.\n",
    "3. ```BeautifulSoup```: A more powerful, principled and readable way to parse data and navigate HTML"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import selenium\n",
    "import time\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regex\n",
    "- [What is regex?](https://en.wikipedia.org/wiki/Regular_expression)\n",
    "- The brute force way is to parse by convering your downloded matriale into a large string\n",
    "- Now you can create standard string operations\n",
    "- And apply smart regex to identify the data you are looking for e.g. links."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url = 'https://www.theguardian.com/us-news/2019/aug/14/taco-eating-contest-death-fresno-california'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "#html.split('\\n')\n",
    "#re.findall(\"(?P<url>https?://[^\\s]+)\", html)[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CSS Selectors \n",
    "- [What is a CSS Selector?](https://www.w3schools.com/css/css_selectors.asp)\n",
    "- Another way to browse through the HTML tree\n",
    "- Define a unique path to an element in the HTML tree.\n",
    "- It is quick but has to be hardcoded and also more likely to break.\n",
    "- [Nicklas recommend using this free Google Chrome CSS Selector](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url = 'https://www.theguardian.com/us-news/2019/aug/14/taco-eating-contest-death-fresno-california'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "soup.select('.dcr-125vfar')[0].text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing HTML with BeautifulSoup\n",
    "BeautifulSoup makes the html tree navigable. \n",
    "It allows you to:\n",
    "    * Search for elements by tag name and/or by attribute.\n",
    "    * Iterate through them, go up, sideways or down the tree.\n",
    "    * Furthermore it helps you with standard tasks such as extracting raw text from html,\n",
    "    which would be a very tedious task if you had to hardcode it using `.split` commands and using your own regular expressions will be unstable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url = 'https://www.theguardian.com/us-news/2019/aug/14/taco-eating-contest-death-fresno-california'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "soup.find('h1')['class'][0].strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# example: finding hyperlinks\n",
    "links = soup.find_all('a') # find all a tags -connoting a hyperlink.\n",
    "[link['href'] for link in links if link.has_attr('href')][0:5] # unpack the hyperlink from the a nodes."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# example: finding headline\n",
    "headline = soup.find('h1') # search for the first headline: h1 tag. \n",
    "name = headline['class'][0].strip() # use the class attribute name as column name.\n",
    "value = headline.text.strip() # extract text using build in method.\n",
    "print(name,':',value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# example: finding article_text\n",
    "article_text = soup.findAll('div', {'class':'dcr-185kcx9'})[0]\n",
    "article_text.text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article_text.findAll(['a'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Say we are interested in how articles cite sources to back up their story i.e. their hyperlink behaviour within the article, and we want to see if the media has changed their behaviour.\n",
    "\n",
    "We know how to search for links. But the cool part is that we can search from anywhere in the HTML tree. This means that once we have located the article content node - as above - we can search from there. This results in hyperlinks used within the article text.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# example: finding citation links\n",
    "citations = article_text.findAll('a')\n",
    "\n",
    "citation_links = [] # define container to the hyperlinks\n",
    "for citation in citations: # iterate through each citation node\n",
    "    if citation.has_attr('data-link-name'): # check if it has the right attribute\n",
    "        if citation['data-link-name'] =='in body link': # and if the value of that attribute is correct\n",
    "            print(citation['href'])\n",
    "            citation_links.append(citation['href']) #  add link to the container\n",
    "\n",
    "citation_links"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a dataset from www.bold.dk\n",
    "\n",
    "### Let's put together some of the stuff we have learned so far\n",
    "1. **Investigate:** In this example we will try to investigate the website to uderstand its structure. \n",
    "2. **Mapping:** Then we will try to collect all the urls and save them into a list\n",
    "3. **Parsing:** At last, we will try to collect the information in each url in a simpel loop.\n",
    "\n",
    "#### First, we pay around with the site trying to understand its structure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(soup.find_all('div',{'class':'news_list_item'}))\n",
    "articles[0].a['href']\n",
    "type(articles)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define our URL\n",
    "url = 'https://www.bold.dk/' \n",
    "\n",
    "# connects to site\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse data with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text,'lxml') # parse the HTML\n",
    "\n",
    "#identify articles to scrape by inspecting site\n",
    "articles = soup.find_all('div',{'class':'news_list_item'}) # search for the ul node\n",
    "\n",
    "# checking if articles match website\n",
    "for i in range(len(articles)):\n",
    "    print(articles[i].text.strip())\n",
    "\n",
    "# identifying how to find an url from an article\n",
    "article_url = articles[0].attrs['data-vr-contentbox-url']\n",
    "print(article_url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "articles[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Second, we create a list of urls that we want to scrape"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url = 'https://www.bold.dk/' \n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "articles = soup.find_all('div',{'class':'news_list_item'})\n",
    "\n",
    "#create an empty list\n",
    "list_of_article_urls = []\n",
    "\n",
    "# creating a loop that appends the article url to the list above\n",
    "for i in range(len(articles)):\n",
    "    list_of_article_urls.append(articles[i].attrs['data-vr-contentbox-url'])\n",
    "\n",
    "#printing the list\n",
    "list_of_article_urls\n",
    "\n",
    "#printing one example\n",
    "#print(list_of_article_urls[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Third, we scrape each site from the url list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this step usually reuqiere a new step of investigation\n",
    "# to figure out what information you want to download\n",
    "# in this example we want the title, the lead and time posted\n",
    "\n",
    "# creatig empty list for the infomation we want to extract for every article\n",
    "h1_list = []\n",
    "lead = []\n",
    "time_posted = []\n",
    "\n",
    "for i in range(len(list_of_article_urls)): # 10 #len(list_of_article_urls)\n",
    "    \n",
    "    # this time we scrape for each news article in the url list we created before\n",
    "    url = list_of_article_urls[i]\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "    \n",
    "    # pedagogical way of append title to list\n",
    "    temp_1 = soup.find_all('h1')\n",
    "    temp_1 = temp_1[1]\n",
    "    temp_1 = temp_1.text.strip()\n",
    "    h1_list.append(temp_1)\n",
    "    \n",
    "    # how I would actually do it\n",
    "    lead.append(soup.find_all('div',{'class':'lead'})[0].text.strip())\n",
    "    \n",
    "    # sometimes you make wierd things - that works\n",
    "    temp_3 = soup.find_all('time')\n",
    "    temp_3 = temp_3[0]\n",
    "    temp_3 = str(temp_3)[16:32]\n",
    "    time_posted.append(temp_3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# h1 \n",
    "soup.find_all('h1')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "soup.find_all('h1')[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "soup.find_all('h1')[1].text.strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# lead\n",
    "soup.find_all('div',{'class':'lead'})[0].text.strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# time_posted\n",
    "soup.find_all('time')[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Lastly, we put our collected information into a dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'title':h1_list, 'lead':lead, 'time':time_posted})\n",
    "df"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# saving df\n",
    "df.to_csv('df_bold.dk.csv')\n",
    "\n",
    "# loading df\n",
    "pd.read_csv('df_bold.dk.csv', index_col=0)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise Set 7: Web Scraping 2\n",
    "\n",
    "In this Exercise Set we shall develop our webscraping skills even further by practicing **parsing** and navigating html trees using `BeautifoulSoup` and furthermore train extracting information from raw text with no html tags to help, using regular expressions. \n",
    "\n",
    "But just as importantly you will get a chance to think about **data quality issues** and how to ensure reliability when curating your own webdata. "
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise Section 7.1: Logging and data quality\n",
    "\n",
    "> **Ex. 7.1.1:** *`Why` is it important to log processes in your data collection?*\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex. 7.1.2:**\n",
    "*`How` does logging help with both ensuring and documenting the quality of your data?*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise Section 7.2: Parsing a Table from HTML using BeautifulSoup.\n",
    "\n",
    "In module_6 I showed you a neat little prepackaged function in pandas that did all the work. However today we should learn the mechanics of it. *(It is not just for educational purposes, sometimes the package will not do exactly as you want.)*\n",
    "\n",
    "We hit the Basketball stats page from yesterday again: https://www.basketball-reference.com/leagues/NBA_2018.html.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex. 7.2.1:** Here we practice simply locating the table node of interest using the `find` method build into BeautifoulSoup. But first we have to fetch the HTML using the `requests` module. Parse the tree using `BeautifulSoup`. And then use the **>Inspector<** tool (* right click on the table < press inspect element *) in your browser to see how to locate the Eastern Conference table node - i.e. the *tag* name of the node, and maybe some defining *attributes*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "url = 'https://www.basketball-reference.com/leagues/NBA_2018.html' # link to the website\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table_E = soup.findAll('table', {'id' : 'confs_standings_E'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "table_E[0].text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You have located the table should now build a function that starts at a \"table node\" and parses the information, and outputs a pandas DataFrame. \n",
    "\n",
    "Inspect the element either within the notebook or through the **>Inspector<** tool and start to see how a table is written in html. Which tag names can be used to locate rows? How will you iterate through columns. Were is the header located?\n",
    "\n",
    "> **Ex. 7.2.2:** First you parse the header which can be found in the canonical tag name: thead. \n",
    "Next you use the `find_all` method to search for the tag, and iterate through each of the elements extracting the text, using the `.text` method builtin to the the node object. Store the header values in a list container. \n",
    "\n",
    "> **Ex. 7.2.3:** Next you locate the rows, using the canonical tag name: tbody. And from here you search for all rows tags. Fiugre out the tag name yourself, inspecting the tbody node in python or using the **Inspector**. \n",
    "\n",
    "> **Ex. 7.2.4:** Next run through all the rows and extract each value, similar to how you extracted the header. However here is a slight variation: Since each value node can have a different tag depending on whether it is a digit or a string, you should use the `.children` method instead of the `.find_all` - (or write compile a regex that matches both the td tag and the th tag.) \n",
    ">Once the value nodes of each row has been located using the `.children` method you should extract the value. Store the extracted rows as a list of lists: ```[[val1,val2,...valk],...]```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def bb_func(table_soup):\n",
    "    # Define empty lists\n",
    "    header_list = []\n",
    "    rows_list = []\n",
    "    columns = []\n",
    "    dict_bb = defaultdict(list)\n",
    "\n",
    "    # Add the header to the list\n",
    "    header_list.append(table_soup.findAll('th')[0].text)\n",
    "\n",
    "    # Find all the columns and add to the columns list\n",
    "    col = table_soup.find('thead').findAll('th', {'scope': 'col'})\n",
    "    for i in range(len(col)):\n",
    "        columns.append(col[i].text)\n",
    "    \n",
    "    # Add the rows to the row list\n",
    "    row = table_soup.find('tbody').findAll('a')\n",
    "    for i in range(len(row)):\n",
    "        rows_list.append(row[i].text)\n",
    "    \n",
    "    # Get the row values\n",
    "    values = table_soup.find('tbody').findAll('td')\n",
    "    for i in range(len(values)):\n",
    "        key = table_soup.find('tbody').findAll('td')[i]['data-stat']\n",
    "        value = table_soup.find('tbody').findAll('td')[i].text\n",
    "        dict_bb[key].append(value)\n",
    "\n",
    "    # Make the DataFrame to return\n",
    "    #new_cols = list(set(columns).difference(header_list))\n",
    "    new_cols = columns[1:]\n",
    "    df_bb = pd.DataFrame( data = dict_bb.values()).transpose()\n",
    "    df_bb.index = rows_list\n",
    "    df_bb.columns = new_cols\n",
    "\n",
    "    return df_bb\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "source": [
    "df = bb_func(soup)\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                      W   L  W/L%    GB   PS/G   PA/G    SRS\n",
       "Toronto Raptors      59  23  .720     —  111.7  103.9   7.29\n",
       "Boston Celtics       55  27  .671   4.0  104.0  100.4   3.23\n",
       "Philadelphia 76ers   52  30  .634   7.0  109.8  105.3   4.30\n",
       "Cleveland Cavaliers  50  32  .610   9.0  110.9  109.9   0.59\n",
       "Indiana Pacers       48  34  .585  11.0  105.6  104.2   1.18\n",
       "Miami Heat           44  38  .537  15.0  103.4  102.9   0.15\n",
       "Milwaukee Bucks      44  38  .537  15.0  106.5  106.8  -0.45\n",
       "Washington Wizards   43  39  .524  16.0  106.6  106.0   0.53\n",
       "Detroit Pistons      39  43  .476  20.0  103.8  103.9  -0.26\n",
       "Charlotte Hornets    36  46  .439  23.0  108.2  108.0   0.07\n",
       "New York Knicks      29  53  .354  30.0  104.5  108.0  -3.53\n",
       "Brooklyn Nets        28  54  .341  31.0  106.6  110.3  -3.67\n",
       "Chicago Bulls        27  55  .329  32.0  102.9  110.0  -6.84\n",
       "Orlando Magic        25  57  .305  34.0  103.4  108.2  -4.92\n",
       "Atlanta Hawks        24  58  .293  35.0  103.4  108.8  -5.30"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>W/L%</th>\n",
       "      <th>GB</th>\n",
       "      <th>PS/G</th>\n",
       "      <th>PA/G</th>\n",
       "      <th>SRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Toronto Raptors</th>\n",
       "      <td>59</td>\n",
       "      <td>23</td>\n",
       "      <td>.720</td>\n",
       "      <td>—</td>\n",
       "      <td>111.7</td>\n",
       "      <td>103.9</td>\n",
       "      <td>7.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston Celtics</th>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "      <td>.671</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.4</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia 76ers</th>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>.634</td>\n",
       "      <td>7.0</td>\n",
       "      <td>109.8</td>\n",
       "      <td>105.3</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cleveland Cavaliers</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>.610</td>\n",
       "      <td>9.0</td>\n",
       "      <td>110.9</td>\n",
       "      <td>109.9</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana Pacers</th>\n",
       "      <td>48</td>\n",
       "      <td>34</td>\n",
       "      <td>.585</td>\n",
       "      <td>11.0</td>\n",
       "      <td>105.6</td>\n",
       "      <td>104.2</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miami Heat</th>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>.537</td>\n",
       "      <td>15.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>102.9</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Milwaukee Bucks</th>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>.537</td>\n",
       "      <td>15.0</td>\n",
       "      <td>106.5</td>\n",
       "      <td>106.8</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Wizards</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>.524</td>\n",
       "      <td>16.0</td>\n",
       "      <td>106.6</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detroit Pistons</th>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>.476</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103.8</td>\n",
       "      <td>103.9</td>\n",
       "      <td>-0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charlotte Hornets</th>\n",
       "      <td>36</td>\n",
       "      <td>46</td>\n",
       "      <td>.439</td>\n",
       "      <td>23.0</td>\n",
       "      <td>108.2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Knicks</th>\n",
       "      <td>29</td>\n",
       "      <td>53</td>\n",
       "      <td>.354</td>\n",
       "      <td>30.0</td>\n",
       "      <td>104.5</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brooklyn Nets</th>\n",
       "      <td>28</td>\n",
       "      <td>54</td>\n",
       "      <td>.341</td>\n",
       "      <td>31.0</td>\n",
       "      <td>106.6</td>\n",
       "      <td>110.3</td>\n",
       "      <td>-3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicago Bulls</th>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>.329</td>\n",
       "      <td>32.0</td>\n",
       "      <td>102.9</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-6.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orlando Magic</th>\n",
       "      <td>25</td>\n",
       "      <td>57</td>\n",
       "      <td>.305</td>\n",
       "      <td>34.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>108.2</td>\n",
       "      <td>-4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlanta Hawks</th>\n",
       "      <td>24</td>\n",
       "      <td>58</td>\n",
       "      <td>.293</td>\n",
       "      <td>35.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>108.8</td>\n",
       "      <td>-5.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 229
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex. 7.2.5:** Now locate all tables from the page, using the `.find_all` method searching for the table tag name. Iterate through the table nodes and apply the function created for parsing html tables. Store each table in a dictionary using the table name as key. The name is found by accessing the id attribute of each table node, using dictionary-style syntax - i.e. `table_node['id']`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "source": [
    "tables = soup.findAll('table', {'class' : 'suppress_all sortable stats_table'})\n",
    "table_dict = {}\n",
    "for i in range(len(tables)):\n",
    "    table_dict[f'table{i}'] = bb_func(tables[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "source": [
    "table_dict['table0']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                      W   L  W/L%    GB   PS/G   PA/G    SRS\n",
       "Toronto Raptors      59  23  .720     —  111.7  103.9   7.29\n",
       "Boston Celtics       55  27  .671   4.0  104.0  100.4   3.23\n",
       "Philadelphia 76ers   52  30  .634   7.0  109.8  105.3   4.30\n",
       "Cleveland Cavaliers  50  32  .610   9.0  110.9  109.9   0.59\n",
       "Indiana Pacers       48  34  .585  11.0  105.6  104.2   1.18\n",
       "Miami Heat           44  38  .537  15.0  103.4  102.9   0.15\n",
       "Milwaukee Bucks      44  38  .537  15.0  106.5  106.8  -0.45\n",
       "Washington Wizards   43  39  .524  16.0  106.6  106.0   0.53\n",
       "Detroit Pistons      39  43  .476  20.0  103.8  103.9  -0.26\n",
       "Charlotte Hornets    36  46  .439  23.0  108.2  108.0   0.07\n",
       "New York Knicks      29  53  .354  30.0  104.5  108.0  -3.53\n",
       "Brooklyn Nets        28  54  .341  31.0  106.6  110.3  -3.67\n",
       "Chicago Bulls        27  55  .329  32.0  102.9  110.0  -6.84\n",
       "Orlando Magic        25  57  .305  34.0  103.4  108.2  -4.92\n",
       "Atlanta Hawks        24  58  .293  35.0  103.4  108.8  -5.30"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>W/L%</th>\n",
       "      <th>GB</th>\n",
       "      <th>PS/G</th>\n",
       "      <th>PA/G</th>\n",
       "      <th>SRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Toronto Raptors</th>\n",
       "      <td>59</td>\n",
       "      <td>23</td>\n",
       "      <td>.720</td>\n",
       "      <td>—</td>\n",
       "      <td>111.7</td>\n",
       "      <td>103.9</td>\n",
       "      <td>7.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston Celtics</th>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "      <td>.671</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.4</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia 76ers</th>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>.634</td>\n",
       "      <td>7.0</td>\n",
       "      <td>109.8</td>\n",
       "      <td>105.3</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cleveland Cavaliers</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>.610</td>\n",
       "      <td>9.0</td>\n",
       "      <td>110.9</td>\n",
       "      <td>109.9</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana Pacers</th>\n",
       "      <td>48</td>\n",
       "      <td>34</td>\n",
       "      <td>.585</td>\n",
       "      <td>11.0</td>\n",
       "      <td>105.6</td>\n",
       "      <td>104.2</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miami Heat</th>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>.537</td>\n",
       "      <td>15.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>102.9</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Milwaukee Bucks</th>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>.537</td>\n",
       "      <td>15.0</td>\n",
       "      <td>106.5</td>\n",
       "      <td>106.8</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Wizards</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>.524</td>\n",
       "      <td>16.0</td>\n",
       "      <td>106.6</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detroit Pistons</th>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>.476</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103.8</td>\n",
       "      <td>103.9</td>\n",
       "      <td>-0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charlotte Hornets</th>\n",
       "      <td>36</td>\n",
       "      <td>46</td>\n",
       "      <td>.439</td>\n",
       "      <td>23.0</td>\n",
       "      <td>108.2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Knicks</th>\n",
       "      <td>29</td>\n",
       "      <td>53</td>\n",
       "      <td>.354</td>\n",
       "      <td>30.0</td>\n",
       "      <td>104.5</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brooklyn Nets</th>\n",
       "      <td>28</td>\n",
       "      <td>54</td>\n",
       "      <td>.341</td>\n",
       "      <td>31.0</td>\n",
       "      <td>106.6</td>\n",
       "      <td>110.3</td>\n",
       "      <td>-3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicago Bulls</th>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>.329</td>\n",
       "      <td>32.0</td>\n",
       "      <td>102.9</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-6.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orlando Magic</th>\n",
       "      <td>25</td>\n",
       "      <td>57</td>\n",
       "      <td>.305</td>\n",
       "      <td>34.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>108.2</td>\n",
       "      <td>-4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlanta Hawks</th>\n",
       "      <td>24</td>\n",
       "      <td>58</td>\n",
       "      <td>.293</td>\n",
       "      <td>35.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>108.8</td>\n",
       "      <td>-5.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 232
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Ex. 7.2.6. (extra) :** Compare your results to the pandas implementation [pd.read_html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise Section 7.3: The European Research Counsel (optional)\n",
    "**NOTE** Exercise 7.3 is difficult and therefore also optional. I expect less than 10% of you being able to solve this one.\n",
    "\n",
    "Imagine we wanted to analyze whether the European funding behaviour was biased towards certain countries and gender. We might decide to scrape who has received funding from the ERC.\n",
    "https://erc.europa.eu/\n",
    "\n",
    "* First we figure find navigate the grant listings.\n",
    "* Next we figure out how to page these results. \n",
    "* And finally we want to grab the information."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Storage and operating system interactions\n",
    "\n",
    "> **Ex. 7.3.1:** *Import the python library `os`. Write pyhon code in this jupyter notebook creating a new folder in your directory called \"erc_funding\". Inside your new folder create 3 subfolders called 'mapping', 'raw_data' and 'parsed_data'.*\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mapping\n",
    "\n",
    "> **Ex. 7.3.2:** *Investigate [https://erc.europa.eu/projects-figures/erc-funded-projects/results?items_per_page=100&search_api_views_fulltext=&](https://erc.europa.eu/projects-figures/erc-funded-projects/results?items_per_page=100&search_api_views_fulltext=&). Figure out how many sites you need to loop thorugh. Save the response for each site using in `condecs` in your \"mapping\" subfolder. Use the `tqdm` to track your loop.\n",
    "Use the Snorre Ralund Connector class to log your activity.*\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parsing\n",
    "\n",
    "> **Ex. 7.3.3:** *Write a function that takes a filename (from our mapping subfolder) as and input and returns (in our parsed_data subfolder) a `pandas`dataframe of parsed information. Use `os` library to navigate your operating system (paths) and `condecs` library to read files inside your function. Last, concatenate all your dataframes into one dataframe you call \"df\" consisting of all parsed data.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reliability and Data Quality\n",
    "\n",
    "##### Inspect the data\n",
    "> **Ex. 7.3.4:** *Investigate your dataframe \"df\". Check for dublicates. Count NaN values. Create a `matplotlib` histrogram plot for every column of \"df\" illustrating the lenght of the string (x-axis) and row count( y-axis).*\n",
    "\n",
    "##### Do simple descriptives\n",
    "> **Ex. 7.3.5:** *Create a value_counts() for each of the three columns (Host Institution (HI), Researcher (PI) and Project acronym) in your \"df\". What can counting do for us in this exercise in terms of Reliability and Data Quality?*\n",
    "\n",
    "##### Visualize the Log\n",
    "> **Ex. 7.3.6:** *Load your \"erc_log.csv\". Convert the time column 't' to datetime. Use `matplotlib` to create three plots: (1) time it took to make the call, (2) the response size over time, and (3) the delta_t against the response_size .)*"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "328px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}