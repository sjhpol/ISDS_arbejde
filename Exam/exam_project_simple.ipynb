{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Exam project of group 26\n",
                "### Group members:\n",
                "- Baltazar Dydensborg\n",
                "- Johan Kielgast Ladelund\n",
                "- Laura Weile\n",
                "- Simon Juul Hansen\n",
                "\n",
                "### Research Question:\n",
                "Vi skal have fundet på noget.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### All of the dependency imports"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "import pandas as pd\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import datetime\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "import time\n",
                "from seleniumwire import webdriver\n",
                "from webdriver_manager.chrome import ChromeDriverManager\n",
                "from selenium.webdriver import ActionChains\n",
                "from collections import defaultdict\n",
                "import scraping_class\n",
                "from statsmodels.tsa.tsatools import lagmat\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "s = requests.Session()\n",
                "# update headers to the requests\n",
                "s.headers.update({'contact_details': 'qxd466@alumni.ku.dk', 'purpose': \"Dear recipient, we've scraped your data with the purpose of creating an academic report on electricity prices and if it's possible to predict these. Please contact us at via the contact details, if there's any issue with this. Best regards.\"})\n",
                "conn = scraping_class.Connector(logfile=\"log.csv\", overwrite_log=True, session = s)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Nordpool data\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We import the required packages"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We identify the [webpage](https://www.nordpoolgroup.com/historical-market-data/) with the desired information stored as seperate htlm files.\n",
                "\n",
                "However, we are not able to scrape Nordpool to obtain the individual links for the files as the webpage call an internal API. Luckily we are able to call the API and obtain the JSON. \n",
                "\n",
                "## Below we define function for data gathering and handling of all Nordpool data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "### We define a function for getting the electricity spot prices ###\n",
                "def getElspotPrices():\n",
                "    # Get the json data from Nordpool\n",
                "    url = 'https://www.nordpoolgroup.com/api/downloads/4675'\n",
                "    el_res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    el_json = el_res.json()\n",
                "\n",
                "    # Selects the urls for the hourly elspot prices in DKK\n",
                "    hourly = [i for i in el_json['Files'] if i['Resolution'] == 'Hourly']\n",
                "    elspot = [i for i in hourly if i['Categories'] == ['Elspot Prices']]\n",
                "    elspot = [i for i in elspot if i['Name'].endswith('DKK')]\n",
                "    elspot_url = ['https://www.nordpoolgroup.com'+i['Url'] for i in elspot]\n",
                "\n",
                "    # Create a dictonary with all of the data for each year and put it into a DataFrame\n",
                "    d = {}\n",
                "    for i in range(len(elspot_url)):\n",
                "        d[f'DF_{2013+i}'] = pd.read_html(elspot_url[i])[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1)\n",
                "\n",
                "    # Standardize the 2021 dataset\n",
                "    #d['DF_2021'] = d['DF_2021'].droplevel(level = 0, axis = 1).rename(columns= {'Unnamed: 0_level_3': 'Unnamed: 0_level_2'})\n",
                "\n",
                "    # Concat the DataFrames into one DF\n",
                "    df_concat = pd.concat(d)\n",
                "\n",
                "    # Structure the DataFrame\n",
                "    df_new = df_concat[['Unnamed: 0_level_2', 'Hours','DK1']].droplevel(level = 0).reset_index(drop = True)\n",
                "    df_new[df_new.columns[2:]] = df_new[df_new.columns[2:]]/100\n",
                "    df_new = df_new.rename(columns = {'Unnamed: 0_level_2': 'date'})\n",
                "\n",
                "    # Add datetime columns\n",
                "    df_new['date'] = pd.to_datetime(df_new['date'], format = '%d-%m-%Y')\n",
                "    df_new['hour'] = df_new['Hours'].str[5:]\n",
                "    df_new.drop('Hours', axis = 1, inplace = True)\n",
                "    df_new['hour'] = pd.to_datetime(df_new['hour'], format = '%H').dt.hour\n",
                "\n",
                "    # Make the index into datetime\n",
                "    df_new.index = pd.to_datetime(df_new['date'].apply(str)+' '+pd.to_datetime(df_new['hour'], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_new\n",
                "\n",
                "### We define a function for getting the electricity consumption prognosis ###\n",
                "def ConsumptionPrognosis():\n",
                "    # Get the json from the internal API on Nordpool.com\n",
                "    url = \"https://www.nordpoolgroup.com/api/downloads/4675\"\n",
                "    res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    page = res.json()\n",
                "\n",
                "    # Get the URLs for the html files containing the hourly consumption prognosis' and transform them into a list\n",
                "    # of dataframes \n",
                "    hourly = [i for i in page[\"Files\"] if i[\"Resolution\"] == \"Hourly\"]\n",
                "    Categories = [i for i in hourly if i[\"Categories\"] == [\"Consumption prognosis\"]]\n",
                "    DKK = [i for i in Categories if \"DK\" in i[\"Name\"]]\n",
                "    URLS_CP = [\"https://www.nordpoolgroup.com\"+i[\"Url\"] for i in DKK]\n",
                "    df_con_prog = [pd.read_html(i)[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1) for i in URLS_CP]\n",
                "\n",
                "    # Concatenate the dataframes, rename, and format time stamp\n",
                "    df_CP = pd.concat(df_con_prog)\n",
                "    cols = [\"Unnamed: 0_level_2\", \"Hours\", \"DK1\", \"DK2\"]\n",
                "    df_CP = df_CP[cols].rename(columns = {\"Unnamed: 0_level_2\": \"date\", \"DK1\": \"DK1_CP\", \"DK2\": \"DK2_CP\"})\n",
                "    df_CP[\"date\"] = pd.to_datetime(df_CP[\"date\"], format = \"%d-%m-%Y\")\n",
                "    df_CP[\"hour\"] = df_CP[\"Hours\"].str[5:]\n",
                "    df_CP.drop('Hours', axis = 1, inplace = True)\n",
                "    df_CP[\"hour\"] = pd.to_datetime(df_CP[\"hour\"], format = '%H').dt.hour\n",
                "\n",
                "    # Set index equal to time stamp \n",
                "    df_CP.index = pd.to_datetime(df_CP[\"date\"].apply(str)+' '+pd.to_datetime(df_CP[\"hour\"], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_CP\n",
                "\n",
                "### Production Prognosis ###\n",
                "# We extract the production prognosis. The decimal pointer are correct for the consumption prognosis data sets.\n",
                "def ProductionPrognosis(): \n",
                "    # Get the json from the internal API on Nordpool.com\n",
                "    url = \"https://www.nordpoolgroup.com/api/downloads/4675\"\n",
                "    res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    page = res.json()\n",
                "\n",
                "    # Get the URLs for the html files containing the hourly consumption prognosis' and transform them into a list\n",
                "    # of dataframes \n",
                "    hourly = [i for i in page[\"Files\"] if i[\"Resolution\"] == \"Hourly\"]\n",
                "    Categories = [i for i in hourly if i[\"Categories\"] == [\"Production prognosis\"]]\n",
                "    URLS_PP = [\"https://www.nordpoolgroup.com\"+i[\"Url\"] for i in Categories]\n",
                "    df_pro_prog = [pd.read_html(i)[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1) for i in URLS_PP]\n",
                "\n",
                "    # Concatenate the dataframes, rename, and format time stamp\n",
                "    df_PP = pd.concat(df_pro_prog)\n",
                "    cols = [\"Unnamed: 0_level_2\", \"Hours\", \"DK1\", \"DK2\"]\n",
                "    df_PP = df_PP[cols].rename(columns = {\"Unnamed: 0_level_2\": \"date\", \"DK1\": \"DK1_PP\", \"DK2\": \"DK2_PP\"})\n",
                "    df_PP[\"date\"] = pd.to_datetime(df_PP[\"date\"], format = \"%d-%m-%Y\")\n",
                "    df_PP[\"hour\"] = df_PP[\"Hours\"].str[5:]\n",
                "    df_PP.drop('Hours', axis = 1, inplace = True)\n",
                "    df_PP[\"hour\"] = pd.to_datetime(df_PP[\"hour\"], format = '%H').dt.hour\n",
                "\n",
                "\n",
                "    # Set index equal to time stamp \n",
                "    df_PP.index = pd.to_datetime(df_PP[\"date\"].apply(str)+' '+pd.to_datetime(df_PP[\"hour\"], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_PP\n",
                "\n",
                "### Wind Power Prognosis ###\n",
                "# We extract the wind power prognosis. The decimal pointer are correct for the consumption prognosis data sets.\n",
                "def WindPrognosis():\n",
                "    # Get the json from the internal API on Nordpool.com\n",
                "    url = \"https://www.nordpoolgroup.com/api/downloads/4675\"\n",
                "    res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    page = res.json()\n",
                "\n",
                "    # Get the URLs for the html files containing the hourly consumption prognosis' and transform them into a list\n",
                "    # of dataframes \n",
                "    hourly = [i for i in page[\"Files\"] if i[\"Resolution\"] == \"Hourly\"]\n",
                "    Categories = [i for i in hourly if i[\"Categories\"] == [\"Wind power prognosis\"]]\n",
                "    DKK = [i for i in Categories if \"DK\" in i[\"Name\"]]\n",
                "    URLS_WP = [\"https://www.nordpoolgroup.com\"+i[\"Url\"] for i in DKK]\n",
                "    df_wind_prog = [pd.read_html(i)[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1) for i in URLS_WP]\n",
                "\n",
                "    # Concatenate the dataframes, rename, and format time stamp\n",
                "    df_WP = pd.concat(df_wind_prog)\n",
                "    cols = [\"Unnamed: 0_level_2\", \"Hours\", \"DK1\", \"DK2\"]\n",
                "    df_WP = df_WP[cols].rename(columns = {\"Unnamed: 0_level_2\": \"date\", \"DK1\": \"DK1_WP\", \"DK2\": \"DK2_WP\"})\n",
                "    df_WP[\"date\"] = pd.to_datetime(df_WP[\"date\"], format = \"%d-%m-%Y\")\n",
                "    df_WP[\"hour\"] = df_WP[\"Hours\"].str[5:]\n",
                "    df_WP.drop('Hours', axis = 1, inplace = True)\n",
                "    df_WP[\"hour\"] = pd.to_datetime(df_WP[\"hour\"], format = '%H').dt.hour\n",
                "\n",
                "\n",
                "    # Set index equal to time stamp \n",
                "    df_WP.index = pd.to_datetime(df_WP[\"date\"].apply(str)+' '+pd.to_datetime(df_WP[\"hour\"], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_WP\n",
                "\n",
                "### Power Exchange ###\n",
                "# We extract the power exchange. We change the value og observation to fix the misplaced decimal pointer when relevant. \n",
                "def ExchangeConnections():\n",
                "    # Get the json from the internal API on Nordpool.com\n",
                "    url = \"https://www.nordpoolgroup.com/api/downloads/4675\"\n",
                "    res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    page = res.json()\n",
                "\n",
                "    # Get the URLs for the html files containing the hourly consumption prognosis' and transform them into a list\n",
                "    # of dataframes \n",
                "    hourly = [i for i in page[\"Files\"] if i[\"Resolution\"] == \"Hourly\"] \n",
                "    Categories = [i for i in hourly if i[\"Categories\"] == [\"Exchange connections\"]]\n",
                "    DKK = [i for i in Categories if \"DK\" in i[\"Name\"]]\n",
                "    URLS_Ex = [\"https://www.nordpoolgroup.com\"+i[\"Url\"] for i in DKK]\n",
                "\n",
                "    # We obain two datasets for 2015 and by inspecting the data sets we realize that the set called: \n",
                "    # 'https://www.nordpoolgroup.com/48e277/globalassets/marketdata-excel-files/exchange-dk-connections_2015_hourly2.xls',\n",
                "    # is the data of intereset\n",
                "    \n",
                "    URLS_Ex.pop(2)\n",
                "    df_Exchange = [pd.read_html(i)[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1) for i in URLS_Ex]\n",
                "\n",
                "    # Concatenate the dataframes, rename, fix misplaced decimal pointer, and format time stamp\n",
                "    df_Ex = pd.concat(df_Exchange)\n",
                "    df_Ex = df_Ex.rename(columns = {\"Unnamed: 0_level_2\": \"date\"})\n",
                "    df_Ex[\"date\"] = pd.to_datetime(df_Ex[\"date\"], format = \"%d-%m-%Y\")\n",
                "\n",
                "    df_Ex[\"hour\"] = df_Ex[\"Hours\"].str[5:]\n",
                "    df_Ex.drop('Hours', axis = 1, inplace = True)\n",
                "    df_Ex[\"hour\"] = pd.to_datetime(df_Ex[\"hour\"], format = '%H').dt.hour\n",
                "\n",
                "    cols = [\"DK - DE\", \"DK - NO\", \"DK - SE\", \"DK1 - SE3\", \"DK2 - SE4\", \"DK1 - DE\", \"DK2 - DE\", \"DK1 - DK2\", \"DK1 - NL\"]\n",
                "    df_Ex[cols] = df_Ex[cols]/100\n",
                "\n",
                "    # Set index equal to time stamp \n",
                "    df_Ex.index = pd.to_datetime(df_Ex[\"date\"].apply(str)+' '+pd.to_datetime(df_Ex[\"hour\"], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_Ex\n",
                "\n",
                "### Merging all Nordpool data together into one DataFrame ###\n",
                "# Creating function to merge dataframes from the Nordpool database\n",
                "def mergeNordpool(list_of_dfs):\n",
                "    df_Nordpool = list_of_dfs[0]\n",
                "    for df in list_of_dfs[1:]:\n",
                "        df_Nordpool = pd.merge(df_Nordpool, df, how = \"outer\", on = [\"date\", \"hour\"])\n",
                "    df_Nordpool.reset_index(drop = True)\n",
                "    return df_Nordpool\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Weather data from DMI API"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "# Constants\n",
                "\n",
                "metObsAPIKey = 'c4503ba1-28d4-45c5-850a-974e98bbb3e0'\n",
                "climateDataAPIKey = 'ac27b332-bde2-4138-a53e-f0ca82cf3667'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Defining functions to be used for gathering and handling the DMI weather data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "### Defining a function to extract data from the DMI metObs APi ### \n",
                "def getMetObsData(stat, start_date, end_date, stationId = '06183', apiKey = metObsAPIKey):\n",
                "    \"\"\"\n",
                "    Function that takes a parameter ID and searches for all observations from DMI\n",
                "    for the given dates, where the date format is given as 'YYYY-MM-DD'\n",
                "    \"\"\"\n",
                "\n",
                "    # Define constants to be used in the function\n",
                "    url= lambda API_KEY, id, startDate, endDate, stationId: f'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items?limit=300000&stationId={stationId}&parameterId={id}&bbox=7,54,16,58&datetime={startDate}T00:00:00Z/{endDate}T00:00:00Z&api-key={API_KEY}'\n",
                "    local_counter = 1\n",
                "    ErrorCounter = 0\n",
                "    stationCounter = 0\n",
                "\n",
                "    while True:\n",
                "        try:\n",
                "            temp_url = url(apiKey, stat, start_date, end_date, stationId)\n",
                "            ErrorCounter += 1\n",
                "\n",
                "            res, _ = conn.get(temp_url, 'DMI Data')\n",
                "            json_dmi = res.json()\n",
                "            prop = [item['properties'] for item in json_dmi['features']]\n",
                "            temp_df = pd.DataFrame(prop)\n",
                "            \n",
                "            temp_df['observed'].drop_duplicates(inplace = True)\n",
                "            temp_df.index = pd.to_datetime(temp_df['observed'])\n",
                "            temp_df = temp_df.tz_convert('CET')\n",
                "\n",
                "            if temp_df.index[-1].date() != datetime.date.fromisoformat(start_date):\n",
                "                            \n",
                "                if local_counter == 1:\n",
                "                    df_met_obs_func = temp_df.copy()\n",
                "                    local_counter += 1\n",
                "                elif local_counter > 1:\n",
                "                    df_met_obs_func = pd.concat([df_met_obs_func,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in IF\")\n",
                "\n",
                "                end_date = temp_df.index[-1].date().strftime(\"%Y-%m-%d\")\n",
                "                continue\n",
                "            else:\n",
                "                if local_counter == 1:\n",
                "                    df_met_obs_func = temp_df\n",
                "                elif local_counter > 1:\n",
                "                    df_met_obs_func = pd.concat([df_met_obs_func,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in ELSE\")\n",
                "\n",
                "                break\n",
                "            \n",
                "\n",
                "\n",
                "        except KeyError as e:\n",
                "\n",
                "            try:\n",
                "                df_met_obs_func\n",
                "            except NameError:\n",
                "                return temp_df\n",
                "            else:\n",
                "                return df_met_obs_func\n",
                "        \n",
                "    return df_met_obs_func\n",
                "\n",
                "### Defining a function that will fill in the NaN with data from other DMI weather stations for the metObs data###        \n",
                "def metObsNaFiller(stat, start_date, end_date):\n",
                "    stationList = ['06184', '06186', '06187', '06188']\n",
                "\n",
                "    # Create the DF for the first and default station\n",
                "    df = getMetObsData(stat, start_date, end_date)\n",
                "\n",
                "    # Fill in the NAs with observations from all stations listed above\n",
                "    for station in stationList:\n",
                "        df = df.combine_first(getMetObsData(stat, start_date, end_date, stationId = station))\n",
                "\n",
                "    return df\n",
                "\n",
                "### Defining a function to extract data from the DMI ClimateData APi ### \n",
                "def getClimateData(stat, start_date, end_date, stationId = '06184',apiKey = climateDataAPIKey):\n",
                "    \"\"\"\n",
                "    Function that takes a parameter ID and searches for all observations from DMI\n",
                "    for the given dates, where the date format is given as 'YYYY-MM-DD'\n",
                "    \"\"\"\n",
                "\n",
                "    # Define constants to be used in the function\n",
                "    url= lambda API_KEY, id, startDate, endDate, stationId: f'https://dmigw.govcloud.dk/v2/climateData/collections/stationValue/items?timeResolution=hour&limit=300000&stationId={stationId}&parameterId={id}&datetime={startDate}T00:00:00Z/{endDate}T00:00:00Z&api-key={API_KEY}'\n",
                "    local_counter = 1\n",
                "    ErrorCounter = 0\n",
                "    stationCounter = 0\n",
                "\n",
                "    while True:\n",
                "        try:\n",
                "            temp_url = url(apiKey, stat, start_date, end_date, stationId)\n",
                "            ErrorCounter += 1\n",
                "\n",
                "            res, _ = conn.get(temp_url, 'DMI Data')\n",
                "            json_dmi = res.json()\n",
                "            prop = [item['properties'] for item in json_dmi['features']]\n",
                "            temp_df = pd.DataFrame(prop)\n",
                "            \n",
                "            temp_df['to'].drop_duplicates(inplace = True)\n",
                "            temp_df.index = pd.to_datetime(temp_df['to'])\n",
                "\n",
                "            if temp_df.index[-1].date() != datetime.date.fromisoformat(start_date):\n",
                "                            \n",
                "                if local_counter == 1:\n",
                "                    df_climate_func = temp_df.copy()\n",
                "                    local_counter += 1\n",
                "                elif local_counter > 1:\n",
                "                    df_climate_func = pd.concat([df_climate_func,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in IF\")\n",
                "\n",
                "                end_date = temp_df.index[-1].date().strftime(\"%Y-%m-%d\")\n",
                "                continue\n",
                "            else:\n",
                "                if local_counter == 1:\n",
                "                    df_climate_func = temp_df\n",
                "                elif local_counter > 1:\n",
                "                    df_climate_func = pd.concat([df_climate_func,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in ELSE\")\n",
                "\n",
                "                break\n",
                "            \n",
                "\n",
                "\n",
                "        except KeyError as e:\n",
                "\n",
                "            try:\n",
                "                df_climate_func\n",
                "            except NameError:\n",
                "                return temp_df\n",
                "            else:\n",
                "                return df_climate_func\n",
                "        \n",
                "    return df_climate_func\n",
                "\n",
                "### Defining a function that will fill in the NaN with data from other DMI weather stations for the Climate data ###        \n",
                "def ClimateNaFiller(stat, start_date, end_date):\n",
                "    stationList = ['06181', '06186', '06187', '06188']\n",
                "\n",
                "    # Create the DF for the first and default station\n",
                "    df = getClimateData(stat, start_date, end_date)\n",
                "\n",
                "    # Fill in the NAs with observations from all stations listed above\n",
                "    for station in stationList:\n",
                "        df = df.combine_first(getClimateData(stat, start_date, end_date, stationId = station))\n",
                "\n",
                "    return df\n",
                "\n",
                "### Transforming the metObs data so that it goes from long format to wide format and has date and hour columns ###\n",
                "def transformMetObsData(df):\n",
                "    df = df\n",
                "    df['hour'] = df.index.hour\n",
                "    df['date'] = df.index.date\n",
                "    df = df.drop_duplicates(['parameterId', 'date','hour'])\\\n",
                "            .drop(['created', 'stationId'], axis = 1)\\\n",
                "            .sort_values(by = ['date', 'hour'], ascending = [False, False])\\\n",
                "            .copy()\n",
                "    \n",
                "    df_new = df.groupby(['parameterId', 'date', 'hour'])['value'].mean()\\\n",
                "            .unstack(level = 0)\\\n",
                "            .reset_index().rename(columns={df.index.name:None})\n",
                "\n",
                "    return df_new\n",
                "    \n",
                "### Transforming the Climate data so that it goes from long format to wide format and has date and hour columns ###\n",
                "def transformClimateData(df):\n",
                "    df = df\n",
                "    df = df.tz_convert('CET')\n",
                "    df['hour'] = df.index.hour\n",
                "    df['date'] = df.index.date\n",
                "    df = df.drop_duplicates(['parameterId', 'date','hour'])\\\n",
                "            .drop(['calculatedAt', 'created', 'from', 'qcStatus', 'timeResolution', 'validity'], axis = 1)\\\n",
                "            .sort_values(by = ['date', 'hour'], ascending = [False, False])\\\n",
                "            .copy()\n",
                "    \n",
                "    df_new = df.groupby(['parameterId', 'date', 'hour'])['value'].mean()\\\n",
                "            .unstack(level = 0)\\\n",
                "            .reset_index().rename(columns={df.index.name:None})\n",
                "\n",
                "    return df_new\n",
                "\n",
                "### Function that handles the total procesing of metObs data ###\n",
                "def metObsPipeline(stats, start_date, end_date):\n",
                "    dict_df = dict()\n",
                "    for stat in tqdm(stats):\n",
                "        dict_df[stat] = metObsNaFiller(stat, start_date, end_date)\n",
                "\n",
                "    new_df_met = pd.concat(dict_df.values())\n",
                "    transform_df_met = transformMetObsData(new_df_met)\n",
                "    return transform_df_met\n",
                "\n",
                "### Function that handles the total procesing of Climate data ###\n",
                "def climatePipeline(stats, start_date, end_date):\n",
                "    dict_df = dict()\n",
                "    for stat in tqdm(stats):\n",
                "        dict_df[stat] = ClimateNaFiller(stat, start_date, end_date)\n",
                "\n",
                "    new_df_climate = pd.concat(dict_df.values())\n",
                "    transform_df_climate = transformClimateData(new_df_climate)\n",
                "    return transform_df_climate\n",
                "\n",
                "### Function that merges the metObs and Climate data together into a single DataFrame ###\n",
                "def merger(df_met, df_climate, start_date, end_date):\n",
                "    index1 = pd.date_range(start=start_date, end=end_date, freq = 'H').to_pydatetime().tolist()\n",
                "    dfindex = pd.DataFrame(index = index1)\n",
                "    dfindex['date'] = dfindex.index.date\n",
                "    dfindex['hour'] = dfindex.index.hour\n",
                "    merge_df = dfindex.reset_index()\\\n",
                "            .merge(df_met, how=\"left\", on = ['date', 'hour'])\\\n",
                "            .merge(df_climate, how = 'left', on = ['date', 'hour'])\\\n",
                "            .set_index('index')\\\n",
                "            .sort_index()\n",
                "\n",
                "    return merge_df\n",
                "\n",
                "### This function handles all of the DMI data handling by calling the above functions ###\n",
                "def total_DMI_pipeline(met_stats, climate_stats, start_date, end_date):\n",
                "    print('INFO: Running metObs pipeline')\n",
                "    df_met = metObsPipeline(met_stats, start_date, end_date)\n",
                "    print('INFO: Running Climate data pipeline')\n",
                "    df_climate = climatePipeline(climate_stats, start_date, end_date)\n",
                "    print('INFO: Merging the climate and metObs data together')\n",
                "    df_total = merger(df_met, df_climate, start_date, end_date)\n",
                "    print('INFO: Done')\n",
                "    return df_total"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Getting commodity data from https://www.investing.com/\n",
                "In the below code, you will need to change the ***path*** variable to fit the location of your ChromeDriver, otherwise the code will fail."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "### List of all the commodities we will exctract data on from the website ###\n",
                "list_of_commodities = ['gold', 'silver', 'crude-oil', 'rotterdam-coal-futures', 'natural-gas', 'carbon-emissions']\n",
                "\n",
                "### Function that scrapes data for a given commodity and date range from the website using Selenium ###\n",
                "def getCommodityData(commodity, startDate, endDate):\n",
                "    # The commodity has to be specified in the form of '-'.join(commodity_name) e.g. rotterdam-coal-futures or crude-oil.\n",
                "    url = lambda id: f'https://www.investing.com/commodities/{id}-historical-data'\n",
                "    temp_url = url(commodity)\n",
                "    startDate = datetime.datetime.strptime(startDate, \"%Y-%m-%d\").strftime('%m/%d/%Y')\n",
                "    endDate = datetime.datetime.strptime(endDate, \"%Y-%m-%d\").strftime('%m/%d/%Y')\n",
                "\n",
                "    # Insert the path to your Chrome Driver.\n",
                "    path = '/Users/simonjuulhansen/Desktop/Polit/ISDS/chromedriver'\n",
                "    driver = webdriver.Chrome(executable_path=path)\n",
                "\n",
                "    # Create a request interceptor that will contain the request header for Selenium requests\n",
                "    def interceptor(request):\n",
                "        request.headers['Contact_details'] = 'qxd466@alumni.ku.dk'\n",
                "        request.headers['Purpose'] = \"Dear recipient, we've scraped your data with the purpose of creating an academic report on electricity prices and if it's possible to predict these. Please contact us at via the contact details, if there's any issue with this. Best regards.\"\n",
                "\n",
                "    # Set the interceptor on the driver\n",
                "    driver.request_interceptor = interceptor\n",
                "    \n",
                "    driver.get(temp_url)\n",
                "    \n",
                "    time.sleep(3)\n",
                "\n",
                "    # Accept cookies\n",
                "    cookies = driver.find_element_by_css_selector('#onetrust-accept-btn-handler')\n",
                "    ActionChains(driver).click(cookies).perform()\n",
                "\n",
                "    time.sleep(1)\n",
                "\n",
                "    # Opens the date range menu\n",
                "    menu = driver.find_element_by_css_selector('#widgetFieldDateRange')\n",
                "    ActionChains(driver).click(menu).perform()\n",
                "\n",
                "    # Enters the date values. The dates have to be in the form of DD/MM/YYYY e.g. 01/01/2000.\n",
                "    # This formatting is done in the above conversion of the start and end date from YYYY-MM-DD.\n",
                "    inputElement1 = driver.find_element_by_id(\"startDate\")\n",
                "    inputElement1.clear()\n",
                "    inputElement1.send_keys(startDate)\n",
                "\n",
                "    inputElement2 = driver.find_element_by_id(\"endDate\")\n",
                "    inputElement2.clear()\n",
                "    inputElement2.send_keys(endDate)\n",
                "\n",
                "    # Applies the new dates\n",
                "    driver.find_element_by_id(\"applyBtn\").click();\n",
                "\n",
                "    time.sleep(3)\n",
                "\n",
                "    # Gets the results from the webpage\n",
                "    results = driver.find_elements_by_xpath('//*[(@id = \"curr_table\")]//td')\n",
                "\n",
                "    # Store the results in a DataFrame\n",
                "    d = defaultdict(list)\n",
                "    date_counter = 0\n",
                "    value_counter = 1\n",
                "    while True:\n",
                "        try:\n",
                "            d['date'].append(results[date_counter].text)\n",
                "            d[commodity].append(results[value_counter].text)\n",
                "            date_counter += 7\n",
                "            value_counter += 7\n",
                "        except IndexError:\n",
                "            break\n",
                "    df = pd.DataFrame(d)\n",
                "    df['date'] = pd.to_datetime(df['date'], format = '%b %d, %Y')\n",
                "\n",
                "    driver.quit()\n",
                "    \n",
                "    return df\n",
                "\n",
                "### Function that gathers all of the commodity data and puts them into one single DataFrame, where NaN has been forward filled (daily data) ###\n",
                "def getAllCommodities(commodities, startDate, endDate):\n",
                "    dict_df = dict()\n",
                "\n",
                "    for i in tqdm(commodities):\n",
                "        print(f'INFO: Gathering commodity data for {i}')\n",
                "        dict_df[i] = getCommodityData(i, startDate, endDate)\n",
                "\n",
                "    date_range = pd.date_range(start=startDate, end=endDate, freq = 'D').to_pydatetime().tolist()\n",
                "    df = pd.DataFrame(date_range, columns = ['date'])\n",
                "\n",
                "    for i in dict_df.values():\n",
                "        df = df.merge(i, on = 'date', how = 'left')\n",
                "        df[i.columns[-1]] = df[i.columns[-1]].str.replace(',','').astype('float')\n",
                "\n",
                "    df = df.ffill()\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Total data gathering and handling"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Running the below cell will gather and wrangle all of the data from DMI, Nordpool and investing.com.\n",
                "This might take a while, e.g. 20-30 min.\n",
                "Remember that the commodity price functions will fail unless you have amended the path in the above cell to match the path to your ChromeDriver"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "startDate = '2016-04-01'\n",
                "endDate = '2021-01-01'\n",
                "\n",
                "# Gathering Nordpool data \n",
                "#df_Nordpool = mergeNordpool([getElspotPrices(), ConsumptionPrognosis(), ProductionPrognosis(), WindPrognosis(), ExchangeConnections()])\n",
                "\n",
                "# Gathering DMI weather data\n",
                "metObs_listV2 = [ 'wind_max_per10min_past1h', 'temp_soil_max_past1h', 'cloud_cover']\n",
                "climate_listV1 = ['bright_sunshine', 'mean_radiation', 'mean_pressure', 'acc_precip', 'temp_grass', 'mean_relative_hum', 'mean_temp', 'mean_wind_speed' ,'mean_wind_dir']\n",
                "df_dmi = total_DMI_pipeline(metObs_listV2, climate_listV1, startDate, endDate)\n",
                "\n",
                "# Gathering commodity data\n",
                "list_of_commodities = ['gold', 'silver', 'crude-oil', 'rotterdam-coal-futures', 'natural-gas', 'carbon-emissions']\n",
                "commodities_df = getAllCommodities(list_of_commodities, startDate, endDate)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "  0%|          | 0/3 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "INFO: Running metObs pipeline\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 3/3 [01:26<00:00, 28.68s/it]\n",
                        "  0%|          | 0/9 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "INFO: Running Climate data pipeline\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 9/9 [02:44<00:00, 18.24s/it]\n",
                        "  0%|          | 0/6 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "INFO: Merging the climate and metObs data together\n",
                        "INFO: Done\n",
                        "INFO: Gathering commodity data for gold\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        " 17%|█▋        | 1/6 [01:26<07:12, 86.54s/it]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "INFO: Gathering commodity data for silver\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "127.0.0.1:53715: Traceback (most recent call last):\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/modes/http_proxy.py\", line 9, in __call__\n",
                        "    layer()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/tls.py\", line 285, in __call__\n",
                        "    layer()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/http1.py\", line 100, in __call__\n",
                        "    layer()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/http.py\", line 206, in __call__\n",
                        "    if not self._process_flow(flow):\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/http.py\", line 285, in _process_flow\n",
                        "    return self.handle_regular_connect(f)\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/http.py\", line 224, in handle_regular_connect\n",
                        "    layer()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/tls.py\", line 278, in __call__\n",
                        "    self._establish_tls_with_client_and_server()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/tls.py\", line 357, in _establish_tls_with_client_and_server\n",
                        "    self.ctx.connect()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/tls.py\", line 299, in connect\n",
                        "    self.ctx.connect()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/base.py\", line 174, in connect\n",
                        "    self.server_conn.connect()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/connections.py\", line 270, in connect\n",
                        "    tcp.TCPClient.connect(self)\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/net/tcp.py\", line 465, in connect\n",
                        "    self.ip_address = connection.getpeername()\n",
                        "OSError: [Errno 22] Invalid argument\n",
                        "\n",
                        "During handling of the above exception, another exception occurred:\n",
                        "\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/server.py\", line 113, in handle\n",
                        "    root_layer()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/modes/http_proxy.py\", line 12, in __call__\n",
                        "    self.disconnect()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/server/protocol/base.py\", line 156, in disconnect\n",
                        "    self.server_conn.finish()\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/connections.py\", line 302, in finish\n",
                        "    tcp.TCPClient.finish(self)\n",
                        "  File \"/usr/local/lib/python3.9/site-packages/seleniumwire/thirdparty/mitmproxy/net/tcp.py\", line 323, in finish\n",
                        "    self.wfile.flush()\n",
                        "AttributeError: 'NoneType' object has no attribute 'flush'\n",
                        "\n",
                        " 33%|███▎      | 2/6 [02:56<05:53, 88.46s/it]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "INFO: Gathering commodity data for crude-oil\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        " 50%|█████     | 3/6 [04:12<04:08, 82.90s/it]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "INFO: Gathering commodity data for rotterdam-coal-futures\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        " 67%|██████▋   | 4/6 [05:10<02:25, 72.84s/it]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "INFO: Gathering commodity data for natural-gas\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        " 83%|████████▎ | 5/6 [06:23<01:13, 73.13s/it]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "INFO: Gathering commodity data for carbon-emissions\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 6/6 [07:35<00:00, 75.97s/it]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## The below transforms all of the above dataframes into a single master DataFrame"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "def total_data_gathering_pipeline(df_nordpool, df_dmi, df_commodities):\n",
                "\n",
                "    # Below we're merging all of the data together into one DataFrame from all 3 input DF's\n",
                "    df_nordpool_new = df_nordpool.copy()\n",
                "    df_nordpool_new['date'] = df_nordpool_new['date'].astype('string')\n",
                "\n",
                "    df_dmi_new = df_dmi.copy()\n",
                "    df_dmi_new['date'] = df_dmi_new['date'].astype('string')\n",
                "\n",
                "    df_commodities_new = df_commodities.copy()\n",
                "    df_commodities_new['date'] = df_commodities_new['date'].astype('string')\n",
                "\n",
                "    first_merge = df_dmi_new.merge(df_nordpool_new, on = ['date', 'hour'], how = 'left')\n",
                "    second_merge = first_merge.merge(df_commodities_new, on = 'date', how = 'left')\n",
                "\n",
                "    # Deleting unneeded columns, NaN's and duplicates.\n",
                "    # The duplicates were only for changes to wintertime, where there were one duplicate for each year at 3 AM, but with different values\n",
                "    # for each of the 3 AM observations due to winter time.\n",
                "    # The mean of those two were taken and are replacing the duplicated hours\n",
                "    second_merge.drop(['DK1 - NL'], axis = 1, inplace = True)\n",
                "    second_merge.dropna(inplace = True)\n",
                "    new_mean = second_merge[second_merge.duplicated(['date', 'hour'])].groupby(['date', 'hour']).mean().reset_index()\n",
                "    second_merge.drop_duplicates(['date', 'hour'],  keep = False, inplace = True)\n",
                "    df_concat = pd.concat([second_merge, new_mean]).sort_values(['date', 'hour']).reset_index(drop = True)\n",
                "\n",
                "    df_concat.ffill(inplace = True)\n",
                "    df_concat.dropna(inplace = True)\n",
                "\n",
                "    # Below we're generating hour, weekday and month dummies and appending them to the DataFrame \n",
                "    # while also converting the index to a Datetime object in the main DataFrame\n",
                "    df_concat.index = pd.to_datetime(df_concat['date'].apply(str)+' '+pd.to_datetime(df_concat['hour'], format = '%H').dt.time.apply(str))\n",
                "    df_concat['month'] = df_concat.index.month\n",
                "    df_concat['weekday'] = df_concat.index.weekday\n",
                "\n",
                "    d = defaultdict(list)\n",
                "\n",
                "    d[0] = pd.get_dummies(df_concat['hour'], prefix = 'hour')\n",
                "    d[1] = pd.get_dummies(df_concat['weekday'], prefix = 'weekday')\n",
                "    d[2] = pd.get_dummies(df_concat['month'], prefix = 'month')\n",
                "    df_join = df_concat.copy()\n",
                "    for i in d.values():\n",
                "        df_join = df_join.join(i)\n",
                "\n",
                "\n",
                "    # Below we're lagging the variables to comply with the time series element of our regression\n",
                "    # as to not create future leakage\n",
                "\n",
                "    # Lagging the DK1 prices 48 times and only keeping the 24-48 lags\n",
                "    lagged_DK1 = lagmat(df_join['DK1'], 48, use_pandas = True, trim = 'both')\n",
                "    lagged_DK1 = lagged_DK1[lagged_DK1.columns[23:]]\n",
                "    \n",
                "    # Lagging DMI data once\n",
                "    d_lag = {}\n",
                "    metObs_listV2 = [ 'wind_max_per10min_past1h', 'temp_soil_max_past1h', 'cloud_cover']\n",
                "    climate_listV1 = ['bright_sunshine', 'mean_radiation', 'mean_pressure', 'acc_precip', 'temp_grass', 'mean_relative_hum', 'mean_temp', 'mean_wind_speed' ,'mean_wind_dir']\n",
                "    dmi_list = metObs_listV2 + climate_listV1\n",
                "    for i in dmi_list:\n",
                "        d_lag[i] = lagmat(df_join[i], 24, use_pandas = True, trim = 'both')\n",
                "        d_lag[i] = d_lag[i][d_lag[i].columns[23]]\n",
                "    df_lag_dmi = pd.concat(d_lag.values(), axis = 1)\n",
                "\n",
                "    # Lagging the commodity data once by 24 hours\n",
                "    d_lag_commodity = {}\n",
                "    list_of_commodities = ['gold', 'silver', 'crude-oil', 'rotterdam-coal-futures', 'natural-gas', 'carbon-emissions']\n",
                "    for i in list_of_commodities:\n",
                "        d_lag_commodity[i] = lagmat(df_join[i], 1, use_pandas = True, trim = 'both')\n",
                "    df_lag_commodity = pd.concat(d_lag_commodity.values(), axis = 1)\n",
                "\n",
                "    # Lagging the exchange data once\n",
                "    exchange_list = ['DK net exchange', 'DK1 net exchange', 'DK2 net exchange', 'DK - DE',\n",
                "       'DK - NO', 'DK - SE', 'DK1 - SE3', 'DK2 - SE4', 'DK1 - DE', 'DK2 - DE',\n",
                "       'DK1 - DK2']\n",
                "    d_lag_exchange = {}\n",
                "    for i in exchange_list:\n",
                "        d_lag_exchange[i] = lagmat(df_join[i], 1, use_pandas = True, trim = 'both')\n",
                "    df_lag_exchange = pd.concat(d_lag_exchange.values(), axis = 1)\n",
                "\n",
                "    # Appending all lags to the DataFrame\n",
                "    list_of_df_lags = [df_lag_dmi, df_lag_commodity, df_lag_exchange, lagged_DK1]\n",
                "    for i in list_of_df_lags:\n",
                "        df_join = df_join.join(i)\n",
                "    \n",
                "    # Dropping original non-lagged columns except the target, DK1\n",
                "    df_join = df_join.dropna().drop(exchange_list+list_of_commodities+dmi_list+['month', 'weekday'], axis = 1)\n",
                "    df_join.drop(['weekday_0', 'hour_0', 'month_1'], axis = 1, inplace = True)\n",
                "\n",
                "    return df_join"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "df_all_data = total_data_gathering_pipeline(df_Nordpool, df_dmi, commodities_df)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "df_all_data.columns[5:]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "Index(['DK1_PP', 'DK2_PP', 'DK1_WP', 'DK2_WP', 'hour_1', 'hour_2', 'hour_3',\n",
                            "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
                            "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
                            "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
                            "       'hour_23', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\n",
                            "       'weekday_5', 'weekday_6', 'month_2', 'month_3', 'month_4', 'month_5',\n",
                            "       'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11',\n",
                            "       'month_12', 'wind_max_per10min_past1h.L.24',\n",
                            "       'temp_soil_max_past1h.L.24', 'cloud_cover.L.24', 'bright_sunshine.L.24',\n",
                            "       'mean_radiation.L.24', 'mean_pressure.L.24', 'acc_precip.L.24',\n",
                            "       'temp_grass.L.24', 'mean_relative_hum.L.24', 'mean_temp.L.24',\n",
                            "       'mean_wind_speed.L.24', 'mean_wind_dir.L.24', 'gold.L.1', 'silver.L.1',\n",
                            "       'crude-oil.L.1', 'rotterdam-coal-futures.L.1', 'natural-gas.L.1',\n",
                            "       'carbon-emissions.L.1', 'DK net exchange.L.1', 'DK1 net exchange.L.1',\n",
                            "       'DK2 net exchange.L.1', 'DK - DE.L.1', 'DK - NO.L.1', 'DK - SE.L.1',\n",
                            "       'DK1 - SE3.L.1', 'DK2 - SE4.L.1', 'DK1 - DE.L.1', 'DK2 - DE.L.1',\n",
                            "       'DK1 - DK2.L.1', 'DK1.L.24', 'DK1.L.25', 'DK1.L.26', 'DK1.L.27',\n",
                            "       'DK1.L.28', 'DK1.L.29', 'DK1.L.30', 'DK1.L.31', 'DK1.L.32', 'DK1.L.33',\n",
                            "       'DK1.L.34', 'DK1.L.35', 'DK1.L.36', 'DK1.L.37', 'DK1.L.38', 'DK1.L.39',\n",
                            "       'DK1.L.40', 'DK1.L.41', 'DK1.L.42', 'DK1.L.43', 'DK1.L.44', 'DK1.L.45',\n",
                            "       'DK1.L.46', 'DK1.L.47', 'DK1.L.48'],\n",
                            "      dtype='object')"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 49
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# DEPRECIATED FUNCTIONS"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def climateData(stat, start_date, end_date, stationId = '06184',apiKey = climateDataAPIKEy):\n",
                "    \"\"\"\n",
                "    Function that takes a parameter ID and searches for all observations from DMI\n",
                "    for the given dates, where the date format is given as 'YYYY-MM-DD'\n",
                "    \"\"\"\n",
                "\n",
                "    # Define constants to be used in the function\n",
                "    url= lambda API_KEY, id, startDate, endDate, stationId: f'https://dmigw.govcloud.dk/v2/climateData/collections/stationValue/items?timeResolution=hour&limit=300000&stationId={stationId}&parameterId={id}&datetime={startDate}T00:00:00Z/{endDate}T00:00:00Z&api-key={API_KEY}'\n",
                "    local_counter = 1\n",
                "    ErrorCounter = 0\n",
                "    stationCounter = 0\n",
                "    stationList = ['06181', '06186', '06187', '06188']\n",
                "\n",
                "    while True:\n",
                "        try:\n",
                "            temp_url = url(apiKey, stat, start_date, end_date, stationId)\n",
                "            ErrorCounter += 1\n",
                "\n",
                "            res, _ = conn.get(temp_url, 'DMI Data')\n",
                "            json_dmi = res.json()\n",
                "            prop = [item['properties'] for item in json_dmi['features']]\n",
                "            temp_df = pd.DataFrame(prop)\n",
                "            \n",
                "            temp_df['to'].drop_duplicates(inplace = True)\n",
                "            temp_df.index = pd.to_datetime(temp_df['to'])\n",
                "            #temp_df = temp_df.tz_convert('CET')\n",
                "\n",
                "            if temp_df.index[-1].date() != datetime.date.fromisoformat(start_date):\n",
                "                            \n",
                "                if local_counter == 1:\n",
                "                    df = temp_df.copy()\n",
                "                    local_counter += 1\n",
                "                elif local_counter > 1:\n",
                "                    df = pd.concat([df,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in IF\")\n",
                "\n",
                "                end_date = temp_df.index[-1].date().strftime(\"%Y-%m-%d\")\n",
                "                #warnings.warn('Loop loop date wrong')\n",
                "                continue\n",
                "            else:\n",
                "                if local_counter == 1:\n",
                "                    df = temp_df\n",
                "                elif local_counter > 1:\n",
                "                    df = pd.concat([df,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in ELSE\")\n",
                "\n",
                "                break\n",
                "            \n",
                "\n",
                "\n",
                "        except KeyError as e:\n",
                "\n",
                "            try:\n",
                "                stationId = stationList[stationCounter]\n",
                "                stationCounter += 1\n",
                "                #warnings.warn('Loop loop station wrong')\n",
                "                continue\n",
                "            except IndexError as i:\n",
                "                if ErrorCounter == 1:\n",
                "                    warnings.warn(f\"The requested stat {stat} isn't available for any default stations\")\n",
                "                else:\n",
                "                    warnings.warn(f\"The requested stat {stat} isn't available for any default stations for the daterange: {start_date} - {end_date}\")\n",
                "                return None\n",
                "        \n",
                "    return df\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def metObsData(stat, start_date, end_date, stationId = '06183', apiKey = metObsAPIKey):\n",
                "    \"\"\"\n",
                "    Function that takes a parameter ID and searches for all observations from DMI\n",
                "    for the given dates, where the date format is given as 'YYYY-MM-DD'\n",
                "    \"\"\"\n",
                "\n",
                "    # Define constants to be used in the function\n",
                "    url= lambda API_KEY, id, startDate, endDate, stationId: f'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items?limit=300000&stationId={stationId}&parameterId={id}&bbox=7,54,16,58&datetime={startDate}T00:00:00Z/{endDate}T00:00:00Z&api-key={API_KEY}'\n",
                "    local_counter = 1\n",
                "    ErrorCounter = 0\n",
                "    stationCounter = 0\n",
                "    stationList = ['06184', '06186', '06187', '06188']\n",
                "\n",
                "    while True:\n",
                "        try:\n",
                "            temp_url = url(apiKey, stat, start_date, end_date, stationId)\n",
                "            ErrorCounter += 1\n",
                "\n",
                "            res, _ = conn.get(temp_url, 'DMI Data')\n",
                "            json_dmi = res.json()\n",
                "            prop = [item['properties'] for item in json_dmi['features']]\n",
                "            temp_df = pd.DataFrame(prop)\n",
                "            \n",
                "            temp_df['observed'].drop_duplicates(inplace = True)\n",
                "            temp_df.index = pd.to_datetime(temp_df['observed'])\n",
                "            temp_df = temp_df.tz_convert('CET')\n",
                "\n",
                "            if temp_df.index[-1].date() != datetime.date.fromisoformat(start_date):\n",
                "                            \n",
                "                if local_counter == 1:\n",
                "                    df = temp_df.copy()\n",
                "                    local_counter += 1\n",
                "                elif local_counter > 1:\n",
                "                    df = pd.concat([df,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in IF\")\n",
                "\n",
                "                end_date = temp_df.index[-1].date().strftime(\"%Y-%m-%d\")\n",
                "                continue\n",
                "            else:\n",
                "                if local_counter == 1:\n",
                "                    df = temp_df\n",
                "                elif local_counter > 1:\n",
                "                    df = pd.concat([df,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in ELSE\")\n",
                "\n",
                "                break\n",
                "            \n",
                "\n",
                "\n",
                "        except KeyError as e:\n",
                "\n",
                "            try:\n",
                "                stationId = stationList[stationCounter]\n",
                "                stationCounter += 1\n",
                "                continue\n",
                "            except IndexError as i:\n",
                "                if ErrorCounter == 1:\n",
                "                    warnings.warn(f\"The requested stat {stat} isn't available for any default stations\")\n",
                "                else:\n",
                "                    warnings.warn(f\"The requested stat {stat} isn't available for any default stations for the daterange: {start_date} - {end_date}\")\n",
                "                return None\n",
                "        \n",
                "    return df\n",
                "\n",
                "        \n",
                "    "
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit"
        },
        "interpreter": {
            "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}