{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Exam project of group 26\n",
                "Exam numbers:\n",
                "- 144\n",
                "- 161\n",
                "- 177\n",
                "- 193"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### All of the dependency imports"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import datetime\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import re\n",
                "\n",
                "import time\n",
                "from seleniumwire import webdriver\n",
                "from webdriver_manager.chrome import ChromeDriverManager\n",
                "from selenium.webdriver import ActionChains\n",
                "from collections import defaultdict\n",
                "import scraping_class\n",
                "from statsmodels.tsa.tsatools import lagmat\n",
                "\n",
                "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
                "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, validation_curve\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "\n",
                "from sktime.forecasting.model_selection import ExpandingWindowSplitter, ForecastingGridSearchCV\n",
                "from sktime.forecasting.naive import NaiveForecaster\n",
                "from sktime.forecasting.base import ForecastingHorizon\n",
                "\n",
                "from sklearn.metrics import mean_absolute_error as MAE\n",
                "from sklearn.metrics import mean_squared_error as MSE\n",
                "from sklearn.metrics import r2_score, median_absolute_error\n",
                "from sklearn.metrics import make_scorer"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The below makes a header for all HTTP requests using the scraping_class. If you already have a file named log.csv in your folder, then the below might fail due to a bug in scraping_class"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "s = requests.Session()\n",
                "# update headers to the requests\n",
                "s.headers.update({'contact_details': 'qxd466@alumni.ku.dk', 'purpose': \"Dear recipient, we've scraped your data with the purpose of creating an academic report on electricity prices and if it's possible to predict these. Please contact us via the contact details, if there's any issue with this. Best regards.\"})\n",
                "conn = scraping_class.Connector(logfile=\"log.csv\", overwrite_log=False, session = s)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Data Collection and Wrangling\n",
                "The below collection and wrangling is all handled by functions to make the this process more seamless and reduce the number of globally defined variables to reduce the burden on RAM"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Nordpool data\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We identify the [webpage](https://www.nordpoolgroup.com/historical-market-data/) with the desired information stored as seperate HTML files.\n",
                "\n",
                "However, we are not able to scrape Nordpool to obtain the individual links for the files as the webpage call an internal API. Luckily we are able to call the API and obtain the JSON. \n",
                "\n",
                "## Below we define function for data gathering and handling of all Nordpool data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "### We define a function for getting the electricity spot prices ###\n",
                "def getElspotPrices():\n",
                "    \"\"\" This function downloads the Elspot prices from Nordpool, deletes unnecessary elements from the html response\n",
                "    and then adds datetime as index\"\"\"\n",
                "    # Get the json data from Nordpool\n",
                "    url = 'https://www.nordpoolgroup.com/api/downloads/4675'\n",
                "    el_res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    el_json = el_res.json()\n",
                "\n",
                "    # Selects the urls for the hourly elspot prices in DKK\n",
                "    hourly = [i for i in el_json['Files'] if i['Resolution'] == 'Hourly']\n",
                "    elspot = [i for i in hourly if i['Categories'] == ['Elspot Prices']]\n",
                "    elspot = [i for i in elspot if i['Name'].endswith('DKK')]\n",
                "    elspot_url = ['https://www.nordpoolgroup.com'+i['Url'] for i in elspot]\n",
                "\n",
                "    # Create a dictonary with all of the data for each year and put it into a DataFrame\n",
                "    d = {}\n",
                "    for i in range(len(elspot_url)):\n",
                "        d[f'DF_{2013+i}'] = pd.read_html(elspot_url[i])[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1)\n",
                "\n",
                "    # Sometimes Nordpool makes changes to their 2021 dataset, and this leads to them adding another header level.\n",
                "    # The following removes this header level, if this is present\n",
                "    try:\n",
                "        # Standardize the 2021 dataset\n",
                "        d['DF_2021'] = d['DF_2021'].droplevel(level = 0, axis = 1).rename(columns= {'Unnamed: 0_level_3': 'Unnamed: 0_level_2'})\n",
                "    except ValueError:\n",
                "        pass\n",
                "\n",
                "    # Concat the DataFrames into one DF\n",
                "    df_concat = pd.concat(d)\n",
                "\n",
                "    # Structure the DataFrame\n",
                "    df_new = df_concat[['Unnamed: 0_level_2', 'Hours','DK2']].droplevel(level = 0).reset_index(drop = True)\n",
                "    df_new[df_new.columns[2:]] = df_new[df_new.columns[2:]]/100\n",
                "    df_new = df_new.rename(columns = {'Unnamed: 0_level_2': 'date'})\n",
                "\n",
                "    # Add datetime columns\n",
                "    df_new['date'] = pd.to_datetime(df_new['date'], format = '%d-%m-%Y')\n",
                "    df_new['hour'] = df_new['Hours'].str[5:]\n",
                "    df_new.drop('Hours', axis = 1, inplace = True)\n",
                "    df_new['hour'] = pd.to_datetime(df_new['hour'], format = '%H').dt.hour\n",
                "\n",
                "    # Make the index into datetime\n",
                "    df_new.index = pd.to_datetime(df_new['date'].apply(str)+' '+pd.to_datetime(df_new['hour'], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_new\n",
                "\n",
                "### We define a function for getting the electricity consumption prognosis ###\n",
                "def ConsumptionPrognosis():\n",
                "    \"\"\"Function that gets all the available electricity consumption prognoses from nordpool\n",
                "    and does some initial data wrangling, e.g., deletes unnecessary elements and adds a datetime index\"\"\"\n",
                "    # Get the json from the internal API on Nordpool.com\n",
                "    url = \"https://www.nordpoolgroup.com/api/downloads/4675\"\n",
                "    res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    page = res.json()\n",
                "\n",
                "    # Get the URLs for the html files containing the hourly consumption prognosis' and transform them into a list\n",
                "    # of dataframes \n",
                "    hourly = [i for i in page[\"Files\"] if i[\"Resolution\"] == \"Hourly\"]\n",
                "    Categories = [i for i in hourly if i[\"Categories\"] == [\"Consumption prognosis\"]]\n",
                "    DKK = [i for i in Categories if \"DK\" in i[\"Name\"]]\n",
                "    URLS_CP = [\"https://www.nordpoolgroup.com\"+i[\"Url\"] for i in DKK]\n",
                "    df_con_prog = [pd.read_html(i)[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1) for i in URLS_CP]\n",
                "\n",
                "    # Concatenate the dataframes, rename, and format time stamp\n",
                "    df_CP = pd.concat(df_con_prog)\n",
                "    cols = [\"Unnamed: 0_level_2\", \"Hours\", \"DK1\", \"DK2\"]\n",
                "    df_CP = df_CP[cols].rename(columns = {\"Unnamed: 0_level_2\": \"date\", \"DK1\": \"DK1_CP\", \"DK2\": \"DK2_CP\"})\n",
                "    df_CP[\"date\"] = pd.to_datetime(df_CP[\"date\"], format = \"%d-%m-%Y\")\n",
                "    df_CP[\"hour\"] = df_CP[\"Hours\"].str[5:]\n",
                "    df_CP.drop('Hours', axis = 1, inplace = True)\n",
                "    df_CP[\"hour\"] = pd.to_datetime(df_CP[\"hour\"], format = '%H').dt.hour\n",
                "\n",
                "    # Set index equal to time stamp \n",
                "    df_CP.index = pd.to_datetime(df_CP[\"date\"].apply(str)+' '+pd.to_datetime(df_CP[\"hour\"], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_CP\n",
                "\n",
                "### Production Prognosis ###\n",
                "# We extract the production prognosis. The decimal pointer are correct for the consumption prognosis data sets.\n",
                "def ProductionPrognosis(): \n",
                "    \"\"\"Function that gets all the available electricity production prognoses from nordpool\n",
                "    and does some initial data wrangling, e.g., deletes unnecessary elements and adds a datetime index\"\"\"\n",
                "    # Get the json from the internal API on Nordpool.com\n",
                "    url = \"https://www.nordpoolgroup.com/api/downloads/4675\"\n",
                "    res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    page = res.json()\n",
                "\n",
                "    # Get the URLs for the html files containing the hourly consumption prognosis' and transform them into a list\n",
                "    # of dataframes \n",
                "    hourly = [i for i in page[\"Files\"] if i[\"Resolution\"] == \"Hourly\"]\n",
                "    Categories = [i for i in hourly if i[\"Categories\"] == [\"Production prognosis\"]]\n",
                "    URLS_PP = [\"https://www.nordpoolgroup.com\"+i[\"Url\"] for i in Categories]\n",
                "    df_pro_prog = [pd.read_html(i)[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1) for i in URLS_PP]\n",
                "\n",
                "    # Concatenate the dataframes, rename, and format time stamp\n",
                "    df_PP = pd.concat(df_pro_prog)\n",
                "    cols = [\"Unnamed: 0_level_2\", \"Hours\", \"DK1\", \"DK2\"]\n",
                "    df_PP = df_PP[cols].rename(columns = {\"Unnamed: 0_level_2\": \"date\", \"DK1\": \"DK1_PP\", \"DK2\": \"DK2_PP\"})\n",
                "    df_PP[\"date\"] = pd.to_datetime(df_PP[\"date\"], format = \"%d-%m-%Y\")\n",
                "    df_PP[\"hour\"] = df_PP[\"Hours\"].str[5:]\n",
                "    df_PP.drop('Hours', axis = 1, inplace = True)\n",
                "    df_PP[\"hour\"] = pd.to_datetime(df_PP[\"hour\"], format = '%H').dt.hour\n",
                "\n",
                "\n",
                "    # Set index equal to time stamp \n",
                "    df_PP.index = pd.to_datetime(df_PP[\"date\"].apply(str)+' '+pd.to_datetime(df_PP[\"hour\"], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_PP\n",
                "\n",
                "### Wind Power Prognosis ###\n",
                "# We extract the wind power prognosis. The decimal pointer are correct for the consumption prognosis data sets.\n",
                "def WindPrognosis():\n",
                "    \"\"\" Function that gets all the available wind prognoses from nordpool\n",
                "    and does some initial data wrangling, e.g., deletes unnecessary elements and adds a datetime index\"\"\"\n",
                "    # Get the json from the internal API on Nordpool.com\n",
                "    url = \"https://www.nordpoolgroup.com/api/downloads/4675\"\n",
                "    res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    page = res.json()\n",
                "\n",
                "    # Get the URLs for the html files containing the hourly consumption prognosis' and transform them into a list\n",
                "    # of dataframes \n",
                "    hourly = [i for i in page[\"Files\"] if i[\"Resolution\"] == \"Hourly\"]\n",
                "    Categories = [i for i in hourly if i[\"Categories\"] == [\"Wind power prognosis\"]]\n",
                "    DKK = [i for i in Categories if \"DK\" in i[\"Name\"]]\n",
                "    URLS_WP = [\"https://www.nordpoolgroup.com\"+i[\"Url\"] for i in DKK]\n",
                "    df_wind_prog = [pd.read_html(i)[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1) for i in URLS_WP]\n",
                "\n",
                "    # Concatenate the dataframes, rename, and format time stamp\n",
                "    df_WP = pd.concat(df_wind_prog)\n",
                "    cols = [\"Unnamed: 0_level_2\", \"Hours\", \"DK1\", \"DK2\"]\n",
                "    df_WP = df_WP[cols].rename(columns = {\"Unnamed: 0_level_2\": \"date\", \"DK1\": \"DK1_WP\", \"DK2\": \"DK2_WP\"})\n",
                "    df_WP[\"date\"] = pd.to_datetime(df_WP[\"date\"], format = \"%d-%m-%Y\")\n",
                "    df_WP[\"hour\"] = df_WP[\"Hours\"].str[5:]\n",
                "    df_WP.drop('Hours', axis = 1, inplace = True)\n",
                "    df_WP[\"hour\"] = pd.to_datetime(df_WP[\"hour\"], format = '%H').dt.hour\n",
                "\n",
                "\n",
                "    # Set index equal to time stamp \n",
                "    df_WP.index = pd.to_datetime(df_WP[\"date\"].apply(str)+' '+pd.to_datetime(df_WP[\"hour\"], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_WP\n",
                "\n",
                "### Power Exchange ###\n",
                "# We extract the power exchange. We change the value og observation to fix the misplaced decimal pointer when relevant. \n",
                "def ExchangeConnections():\n",
                "    \"\"\" Function that gets all the available electricity exchange connections from nordpool\n",
                "    and does some initial data wrangling, e.g., deletes unnecessary elements, adds decimal pointer and adds a datetime index \"\"\"\n",
                "    # Get the json from the internal API on Nordpool.com\n",
                "    url = \"https://www.nordpoolgroup.com/api/downloads/4675\"\n",
                "    res, _ = conn.get(url, \"Nordpool Data\")\n",
                "    page = res.json()\n",
                "\n",
                "    # Get the URLs for the html files containing the hourly consumption prognosis' and transform them into a list\n",
                "    # of dataframes \n",
                "    hourly = [i for i in page[\"Files\"] if i[\"Resolution\"] == \"Hourly\"] \n",
                "    Categories = [i for i in hourly if i[\"Categories\"] == [\"Exchange connections\"]]\n",
                "    DKK = [i for i in Categories if \"DK\" in i[\"Name\"]]\n",
                "    URLS_Ex = [\"https://www.nordpoolgroup.com\"+i[\"Url\"] for i in DKK]\n",
                "\n",
                "    # We obain two datasets for 2015 and by inspecting the data sets we realize that the set called: \n",
                "    # 'https://www.nordpoolgroup.com/48e277/globalassets/marketdata-excel-files/exchange-dk-connections_2015_hourly2.xls',\n",
                "    # is the data of intereset\n",
                "    \n",
                "    URLS_Ex.pop(2)\n",
                "    df_Exchange = [pd.read_html(i)[0].droplevel(level = 0, axis = 1).droplevel(level = 0, axis = 1) for i in URLS_Ex]\n",
                "\n",
                "    # Concatenate the dataframes, rename, fix misplaced decimal pointer, and format time stamp\n",
                "    df_Ex = pd.concat(df_Exchange)\n",
                "    df_Ex = df_Ex.rename(columns = {\"Unnamed: 0_level_2\": \"date\"})\n",
                "    df_Ex[\"date\"] = pd.to_datetime(df_Ex[\"date\"], format = \"%d-%m-%Y\")\n",
                "\n",
                "    df_Ex[\"hour\"] = df_Ex[\"Hours\"].str[5:]\n",
                "    df_Ex.drop('Hours', axis = 1, inplace = True)\n",
                "    df_Ex[\"hour\"] = pd.to_datetime(df_Ex[\"hour\"], format = '%H').dt.hour\n",
                "\n",
                "    cols = [\"DK - DE\", \"DK - NO\", \"DK - SE\", \"DK1 - SE3\", \"DK2 - SE4\", \"DK1 - DE\", \"DK2 - DE\", \"DK1 - DK2\", \"DK1 - NL\"]\n",
                "    df_Ex[cols] = df_Ex[cols]/100\n",
                "\n",
                "    # Set index equal to time stamp \n",
                "    df_Ex.index = pd.to_datetime(df_Ex[\"date\"].apply(str)+' '+pd.to_datetime(df_Ex[\"hour\"], format = '%H').dt.time.apply(str))\n",
                "\n",
                "    return df_Ex\n",
                "\n",
                "### Merging all Nordpool data together into one DataFrame ###\n",
                "# Creating function to merge dataframes from the Nordpool database\n",
                "def mergeNordpool(list_of_dfs):\n",
                "    \"\"\" Function that merges all of the Nordpool dataframes into one DataFrame on date and hour via an outer join \"\"\"\n",
                "    df_Nordpool = list_of_dfs[0]\n",
                "    for df in list_of_dfs[1:]:\n",
                "        df_Nordpool = pd.merge(df_Nordpool, df, how = \"outer\", on = [\"date\", \"hour\"])\n",
                "    df_Nordpool.reset_index(drop = True)\n",
                "    return df_Nordpool\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Weather data from DMI API"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Constants\n",
                "metObsAPIKey = 'c4503ba1-28d4-45c5-850a-974e98bbb3e0'\n",
                "climateDataAPIKey = 'ac27b332-bde2-4138-a53e-f0ca82cf3667'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Defining functions to be used for gathering and handling the DMI weather data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "### Defining a function to extract data from the DMI metObs APi ### \n",
                "def getMetObsData(stat, start_date, end_date, stationId = '06183', apiKey = metObsAPIKey):\n",
                "    \"\"\" Function that takes a parameter ID and searches for all observations from DMI metObs APi \n",
                "    for the given dates, where the date format is given as 'YYYY-MM-DD', and for the given station ID \"\"\"\n",
                "\n",
                "    # Define constants to be used in the function\n",
                "    url= lambda API_KEY, id, startDate, endDate, stationId: f'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items?limit=300000&stationId={stationId}&parameterId={id}&bbox=7,54,16,58&datetime={startDate}T00:00:00Z/{endDate}T00:00:00Z&api-key={API_KEY}'\n",
                "    local_counter = 1\n",
                "    ErrorCounter = 0\n",
                "\n",
                "    # Loops until it has observations for the start date. This is done due to the max response size being 300.000,\n",
                "    # and if one goes far enough back for observations with frequencies of 10 min., then we pass the response size,\n",
                "    # so we need to make another response and append this to the resulting DataFrame\n",
                "    while True:\n",
                "        try:\n",
                "            temp_url = url(apiKey, stat, start_date, end_date, stationId)\n",
                "            ErrorCounter += 1\n",
                "\n",
                "            res, _ = conn.get(temp_url, 'DMI Data')\n",
                "            json_dmi = res.json()\n",
                "            prop = [item['properties'] for item in json_dmi['features']]\n",
                "            temp_df = pd.DataFrame(prop)\n",
                "            \n",
                "            # Drops any accidental duplicates before converting to CET localization\n",
                "            temp_df['observed'].drop_duplicates(inplace = True)\n",
                "            temp_df.index = pd.to_datetime(temp_df['observed'])\n",
                "            temp_df = temp_df.tz_convert('CET')\n",
                "\n",
                "            # Checks if the above stated condition is fulfilled\n",
                "            if temp_df.index[-1].date() != datetime.date.fromisoformat(start_date):\n",
                "                            \n",
                "                if local_counter == 1:\n",
                "                    df_met_obs_func = temp_df.copy()\n",
                "                    local_counter += 1\n",
                "                elif local_counter > 1:\n",
                "                    df_met_obs_func = pd.concat([df_met_obs_func,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in IF\")\n",
                "\n",
                "                end_date = temp_df.index[-1].date().strftime(\"%Y-%m-%d\")\n",
                "                continue\n",
                "            else:\n",
                "                if local_counter == 1:\n",
                "                    df_met_obs_func = temp_df\n",
                "                elif local_counter > 1:\n",
                "                    df_met_obs_func = pd.concat([df_met_obs_func,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in ELSE\")\n",
                "\n",
                "                break\n",
                "        \n",
                "        # Checks if all observations were retrieved in the first response, and if so, return temp_df, otherwise df_met_obs_func\n",
                "        # would be defined, so return this DF then.\n",
                "        except KeyError as e:\n",
                "\n",
                "            try:\n",
                "                df_met_obs_func\n",
                "            except NameError:\n",
                "                return temp_df\n",
                "            else:\n",
                "                return df_met_obs_func\n",
                "        \n",
                "    return df_met_obs_func\n",
                "\n",
                "### Defining a function that will fill in the NaN with data from other DMI weather stations for the metObs data###        \n",
                "def metObsNaFiller(stat, start_date, end_date):\n",
                "    \"\"\"This function takes a list of parameters, a start date and a end date and then \n",
                "    fills in the NaN from metObs data with data from other DMI weather stations\"\"\"\n",
                "    stationList = ['06184', '06186', '06187', '06188']\n",
                "\n",
                "    # Create the DF for the first and default station\n",
                "    df = getMetObsData(stat, start_date, end_date)\n",
                "\n",
                "    # Fill in the NAs with observations from all stations listed above\n",
                "    for station in stationList:\n",
                "        df = df.combine_first(getMetObsData(stat, start_date, end_date, stationId = station))\n",
                "\n",
                "    return df\n",
                "\n",
                "### Defining a function to extract data from the DMI ClimateData APi ### \n",
                "def getClimateData(stat, start_date, end_date, stationId = '06184',apiKey = climateDataAPIKey):\n",
                "    \"\"\" Function that takes a parameter ID and searches for all observations from DMI Climate API\n",
                "    for the given dates, where the date format is given as 'YYYY-MM-DD', and for the given station ID \"\"\"\n",
                "\n",
                "    # Define constants to be used in the function\n",
                "    url= lambda API_KEY, id, startDate, endDate, stationId: f'https://dmigw.govcloud.dk/v2/climateData/collections/stationValue/items?timeResolution=hour&limit=300000&stationId={stationId}&parameterId={id}&datetime={startDate}T00:00:00Z/{endDate}T00:00:00Z&api-key={API_KEY}'\n",
                "    local_counter = 1\n",
                "    ErrorCounter = 0\n",
                "\n",
                "    # Loops until it has observations for the start date. This is done due to the max response size being 300.000,\n",
                "    # and if one goes far enough back for observations with frequencies of 10 min., then we pass the response size,\n",
                "    # so we need to make another response and append this to the resulting DataFrame\n",
                "    while True:\n",
                "        try:\n",
                "            temp_url = url(apiKey, stat, start_date, end_date, stationId)\n",
                "            ErrorCounter += 1\n",
                "\n",
                "            res, _ = conn.get(temp_url, 'DMI Data')\n",
                "            json_dmi = res.json()\n",
                "            prop = [item['properties'] for item in json_dmi['features']]\n",
                "            temp_df = pd.DataFrame(prop)\n",
                "\n",
                "            # Drops any accidental duplicates before converting to CET localization, which is done in the transformation function,\n",
                "            # as otherwise the below if condition would not be satisfied due to way the observations are returned in the response object.\n",
                "            temp_df['to'].drop_duplicates(inplace = True)\n",
                "            temp_df.index = pd.to_datetime(temp_df['to'])\n",
                "\n",
                "            # Checks if the above stated condition is fulfilled\n",
                "            if temp_df.index[-1].date() != datetime.date.fromisoformat(start_date):\n",
                "                            \n",
                "                if local_counter == 1:\n",
                "                    df_climate_func = temp_df.copy()\n",
                "                    local_counter += 1\n",
                "                elif local_counter > 1:\n",
                "                    df_climate_func = pd.concat([df_climate_func,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in IF\")\n",
                "\n",
                "                end_date = temp_df.index[-1].date().strftime(\"%Y-%m-%d\")\n",
                "                continue\n",
                "            else:\n",
                "                if local_counter == 1:\n",
                "                    df_climate_func = temp_df\n",
                "                elif local_counter > 1:\n",
                "                    df_climate_func = pd.concat([df_climate_func,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in ELSE\")\n",
                "\n",
                "                break\n",
                "\n",
                "        # Checks if all observations were retrieved in the first response, and if so, return temp_df, otherwise df_met_obs_func\n",
                "        # would be defined, so return this DF then.\n",
                "        except KeyError as e:\n",
                "\n",
                "            try:\n",
                "                df_climate_func\n",
                "            except NameError:\n",
                "                return temp_df\n",
                "            else:\n",
                "                return df_climate_func\n",
                "        \n",
                "    return df_climate_func\n",
                "\n",
                "### Defining a function that will fill in the NaN with data from other DMI weather stations for the Climate data ###        \n",
                "def ClimateNaFiller(stat, start_date, end_date):\n",
                "    \"\"\"This function takes a list of parameters, a start date and a end date and then \n",
                "    fills in the NaN from Climate data with data from other DMI weather stations\"\"\"\n",
                "    stationList = ['06181', '06186', '06187', '06188']\n",
                "\n",
                "    # Create the DF for the first and default station\n",
                "    df = getClimateData(stat, start_date, end_date)\n",
                "\n",
                "    # Fill in the NAs with observations from all stations listed above\n",
                "    for station in stationList:\n",
                "        df = df.combine_first(getClimateData(stat, start_date, end_date, stationId = station))\n",
                "\n",
                "    return df\n",
                "\n",
                "### Transforming the metObs data so that it goes from long format to wide format and has date and hour columns ###\n",
                "def transformMetObsData(df):\n",
                "    \"\"\"This function transforms the metObs data by adding date and hour columns, dropping unnecessary columns,\n",
                "    and makes the format from long format to wide format\"\"\"\n",
                "    df = df\n",
                "    df['hour'] = df.index.hour\n",
                "    df['date'] = df.index.date\n",
                "    df = df.drop(['created', 'stationId'], axis = 1)\\\n",
                "            .sort_values(by = ['date', 'hour'])\\\n",
                "            .copy()\n",
                "            #.drop_duplicates(['parameterId', 'date','hour'])\n",
                "\n",
                "    df_new = df.groupby(['parameterId', 'date', 'hour'])['value'].mean()\\\n",
                "            .unstack(level = 0)\\\n",
                "            .reset_index().rename(columns={df.index.name:None})\n",
                "\n",
                "    return df_new\n",
                "    \n",
                "### Transforming the Climate data so that it goes from long format to wide format and has date and hour columns ###\n",
                "def transformClimateData(df):\n",
                "    \"\"\"This function transforms the Climate data by adding date and hour columns, dropping unnecessary columns,\n",
                "    and makes the format from long format to wide format\"\"\"\n",
                "    df = df\n",
                "    df = df.tz_convert('CET')\n",
                "    df['hour'] = df.index.hour\n",
                "    df['date'] = df.index.date\n",
                "    df = df.drop(['calculatedAt', 'created', 'from', 'qcStatus', 'timeResolution', 'validity'], axis = 1)\\\n",
                "            .sort_values(by = ['date', 'hour'])\\\n",
                "            .copy()\n",
                "    \n",
                "    df_new = df.groupby(['parameterId', 'date', 'hour'])['value'].mean()\\\n",
                "            .unstack(level = 0)\\\n",
                "            .reset_index().rename(columns={df.index.name:None})\n",
                "\n",
                "    return df_new\n",
                "\n",
                "### Function that handles the total procesing of metObs data ###\n",
                "def metObsPipeline(stats, start_date, end_date):\n",
                "    \"\"\"This function handles all of the metObs data gathering and transformation given a list of parameters,\n",
                "    a start date and a end date. Also provides a progress bar to the data gathering process\"\"\"\n",
                "    dict_df = dict()\n",
                "    for stat in tqdm(stats):\n",
                "        dict_df[stat] = metObsNaFiller(stat, start_date, end_date)\n",
                "\n",
                "    new_df_met = pd.concat(dict_df.values())\n",
                "    transform_df_met = transformMetObsData(new_df_met)\n",
                "    return transform_df_met\n",
                "\n",
                "### Function that handles the total procesing of Climate data ###\n",
                "def climatePipeline(stats, start_date, end_date):\n",
                "    \"\"\"This function handles all of the Climate data gathering and transformation given a list of parameters,\n",
                "    a start date and a end date. Also provides a progress bar to the data gathering process\"\"\"\n",
                "    dict_df = dict()\n",
                "    for stat in tqdm(stats):\n",
                "        dict_df[stat] = ClimateNaFiller(stat, start_date, end_date)\n",
                "\n",
                "    new_df_climate = pd.concat(dict_df.values())\n",
                "    transform_df_climate = transformClimateData(new_df_climate)\n",
                "    return transform_df_climate\n",
                "\n",
                "### Function that merges the metObs and Climate data together into a single DataFrame ###\n",
                "def merger(df_met, df_climate, start_date, end_date):\n",
                "    \"\"\"This function merges together all of the DMI weather data from the Climate and metObs APIs,\n",
                "    and makes sure that there's an index for every hour from start till end date.\"\"\"\n",
                "    index1 = pd.date_range(start=start_date, end=end_date, freq = 'H').to_pydatetime().tolist()\n",
                "    dfindex = pd.DataFrame(index = index1)\n",
                "    dfindex['date'] = dfindex.index.date\n",
                "    dfindex['hour'] = dfindex.index.hour\n",
                "    merge_df = dfindex.reset_index()\\\n",
                "            .merge(df_met, how=\"left\", on = ['date', 'hour'])\\\n",
                "            .merge(df_climate, how = 'left', on = ['date', 'hour'])\\\n",
                "            .set_index('index')\\\n",
                "            .sort_index()\n",
                "\n",
                "    return merge_df\n",
                "\n",
                "### This function handles all of the DMI data handling by calling the above functions ###\n",
                "def total_DMI_pipeline(met_stats, climate_stats, start_date, end_date):\n",
                "    \"\"\"This function merely handles the total data gathering and wrangling process for the DMI data and adds info messages along the way to track progress\"\"\"\n",
                "    print('INFO: Running metObs pipeline')\n",
                "    df_met = metObsPipeline(met_stats, start_date, end_date)\n",
                "    print('INFO: Running Climate data pipeline')\n",
                "    df_climate = climatePipeline(climate_stats, start_date, end_date)\n",
                "    print('INFO: Merging the climate and metObs data together')\n",
                "    df_total = merger(df_met, df_climate, start_date, end_date)\n",
                "    print('INFO: Done')\n",
                "    return df_total"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# DEPRECIATED DMI FUNCTIONS\n",
                "The following functions are merely kept to show the missing data that was encountered during the old DMI data gathering approach described in the paper."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def oldClimateData(stat, start_date, end_date, stationId = '06184',apiKey = climateDataAPIKey):\n",
                "    \"\"\"\n",
                "    Function that takes a parameter ID and searches for all observations from DMI\n",
                "    for the given dates, where the date format is given as 'YYYY-MM-DD'\n",
                "    \"\"\"\n",
                "\n",
                "    # Define constants to be used in the function\n",
                "    url= lambda API_KEY, id, startDate, endDate, stationId: f'https://dmigw.govcloud.dk/v2/climateData/collections/stationValue/items?timeResolution=hour&limit=300000&stationId={stationId}&parameterId={id}&datetime={startDate}T00:00:00Z/{endDate}T00:00:00Z&api-key={API_KEY}'\n",
                "    local_counter = 1\n",
                "    ErrorCounter = 0\n",
                "    stationCounter = 0\n",
                "    stationList = ['06181', '06186', '06187', '06188']\n",
                "\n",
                "    while True:\n",
                "        try:\n",
                "            temp_url = url(apiKey, stat, start_date, end_date, stationId)\n",
                "            ErrorCounter += 1\n",
                "\n",
                "            res, _ = conn.get(temp_url, 'DMI Data')\n",
                "            json_dmi = res.json()\n",
                "            prop = [item['properties'] for item in json_dmi['features']]\n",
                "            temp_df = pd.DataFrame(prop)\n",
                "            \n",
                "            temp_df['to'].drop_duplicates(inplace = True)\n",
                "            temp_df.index = pd.to_datetime(temp_df['to'])\n",
                "            #temp_df = temp_df.tz_convert('CET')\n",
                "\n",
                "            if temp_df.index[-1].date() != datetime.date.fromisoformat(start_date):\n",
                "                            \n",
                "                if local_counter == 1:\n",
                "                    df = temp_df.copy()\n",
                "                    local_counter += 1\n",
                "                elif local_counter > 1:\n",
                "                    df = pd.concat([df,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in IF\")\n",
                "\n",
                "                end_date = temp_df.index[-1].date().strftime(\"%Y-%m-%d\")\n",
                "                #warnings.warn('Loop loop date wrong')\n",
                "                continue\n",
                "            else:\n",
                "                if local_counter == 1:\n",
                "                    df = temp_df\n",
                "                elif local_counter > 1:\n",
                "                    df = pd.concat([df,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in ELSE\")\n",
                "\n",
                "                break\n",
                "            \n",
                "\n",
                "\n",
                "        except KeyError as e:\n",
                "\n",
                "            try:\n",
                "                stationId = stationList[stationCounter]\n",
                "                stationCounter += 1\n",
                "                #warnings.warn('Loop loop station wrong')\n",
                "                continue\n",
                "            except IndexError as i:\n",
                "                if ErrorCounter == 1:\n",
                "                    warnings.warn(f\"The requested stat {stat} isn't available for any default stations\")\n",
                "                else:\n",
                "                    warnings.warn(f\"The requested stat {stat} isn't available for any default stations for the daterange: {start_date} - {end_date}\")\n",
                "                return None\n",
                "        \n",
                "    return df\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def oldMetObsData(stat, start_date, end_date, stationId = '06183', apiKey = metObsAPIKey):\n",
                "    \"\"\"\n",
                "    Function that takes a parameter ID and searches for all observations from DMI\n",
                "    for the given dates, where the date format is given as 'YYYY-MM-DD'\n",
                "    \"\"\"\n",
                "\n",
                "    # Define constants to be used in the function\n",
                "    url= lambda API_KEY, id, startDate, endDate, stationId: f'https://dmigw.govcloud.dk/v2/metObs/collections/observation/items?limit=300000&stationId={stationId}&parameterId={id}&bbox=7,54,16,58&datetime={startDate}T00:00:00Z/{endDate}T00:00:00Z&api-key={API_KEY}'\n",
                "    local_counter = 1\n",
                "    ErrorCounter = 0\n",
                "    stationCounter = 0\n",
                "    stationList = ['06184', '06186', '06187', '06188']\n",
                "\n",
                "    while True:\n",
                "        try:\n",
                "            temp_url = url(apiKey, stat, start_date, end_date, stationId)\n",
                "            ErrorCounter += 1\n",
                "\n",
                "            res, _ = conn.get(temp_url, 'DMI Data')\n",
                "            json_dmi = res.json()\n",
                "            prop = [item['properties'] for item in json_dmi['features']]\n",
                "            temp_df = pd.DataFrame(prop)\n",
                "            \n",
                "            temp_df['observed'].drop_duplicates(inplace = True)\n",
                "            temp_df.index = pd.to_datetime(temp_df['observed'])\n",
                "            temp_df = temp_df.tz_convert('CET')\n",
                "\n",
                "            if temp_df.index[-1].date() != datetime.date.fromisoformat(start_date):\n",
                "                            \n",
                "                if local_counter == 1:\n",
                "                    df = temp_df.copy()\n",
                "                    local_counter += 1\n",
                "                elif local_counter > 1:\n",
                "                    df = pd.concat([df,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in IF\")\n",
                "\n",
                "                end_date = temp_df.index[-1].date().strftime(\"%Y-%m-%d\")\n",
                "                continue\n",
                "            else:\n",
                "                if local_counter == 1:\n",
                "                    df = temp_df\n",
                "                elif local_counter > 1:\n",
                "                    df = pd.concat([df,temp_df])\n",
                "                else:\n",
                "                    warnings.warn(\"Something went wrong with the local counter in ELSE\")\n",
                "\n",
                "                break\n",
                "            \n",
                "\n",
                "\n",
                "        except KeyError as e:\n",
                "\n",
                "            try:\n",
                "                stationId = stationList[stationCounter]\n",
                "                stationCounter += 1\n",
                "                continue\n",
                "            except IndexError as i:\n",
                "                if ErrorCounter == 1:\n",
                "                    warnings.warn(f\"The requested stat {stat} isn't available for any default stations\")\n",
                "                else:\n",
                "                    warnings.warn(f\"The requested stat {stat} isn't available for any default stations for the daterange: {start_date} - {end_date}\")\n",
                "                return None\n",
                "        \n",
                "    return df\n",
                "\n",
                "        \n",
                "    "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "### Function that handles the total procesing of metObs data ###\n",
                "def oldMetObsPipeline(stats, start_date, end_date):\n",
                "    dict_df = dict()\n",
                "    for stat in tqdm(stats):\n",
                "        dict_df[stat] = oldMetObsData(stat, start_date, end_date)\n",
                "\n",
                "    new_df_met = pd.concat(dict_df.values())\n",
                "    transform_df_met = transformMetObsData(new_df_met)\n",
                "    return transform_df_met\n",
                "\n",
                "### Function that handles the total procesing of Climate data ###\n",
                "def oldClimatePipeline(stats, start_date, end_date):\n",
                "    dict_df = dict()\n",
                "    for stat in tqdm(stats):\n",
                "        dict_df[stat] = oldClimateData(stat, start_date, end_date)\n",
                "\n",
                "    new_df_climate = pd.concat(dict_df.values())\n",
                "    transform_df_climate = transformClimateData(new_df_climate)\n",
                "    return transform_df_climate\n",
                "\n",
                "def old_total_DMI_pipeline(met_stats, climate_stats, start_date, end_date):\n",
                "    print('INFO: Running metObs pipeline')\n",
                "    df_met = oldMetObsPipeline(met_stats, start_date, end_date)\n",
                "    print('INFO: Running Climate data pipeline')\n",
                "    df_climate = oldClimatePipeline(climate_stats, start_date, end_date)\n",
                "    print('INFO: Merging the climate and metObs data together')\n",
                "    df_total = merger(df_met, df_climate, start_date, end_date)\n",
                "    print('INFO: Done')\n",
                "    return df_total"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Getting commodity data from https://www.investing.com/\n",
                "In the below code, you will need to change the ***path*** variable to fit the location of your ChromeDriver, otherwise the code will fail."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "### List of all the commodities we will exctract data on from the website ###\n",
                "list_of_commodities = ['gold', 'silver', 'crude-oil', 'rotterdam-coal-futures', 'natural-gas', 'carbon-emissions']\n",
                "\n",
                "### Function that scrapes data for a given commodity and date range from the website using Selenium ###\n",
                "def getCommodityData(commodity, startDate, endDate):\n",
                "    \"\"\"Function that scrapes data for a given commodity and date range from investing.com using Selenium\"\"\"\n",
                "\n",
                "    # The commodity has to be specified in the form of '-'.join(commodity_name) e.g. rotterdam-coal-futures or crude-oil.\n",
                "    url = lambda id: f'https://www.investing.com/commodities/{id}-historical-data'\n",
                "    temp_url = url(commodity)\n",
                "    startDate = datetime.datetime.strptime(startDate, \"%Y-%m-%d\").strftime('%m/%d/%Y')\n",
                "    endDate = datetime.datetime.strptime(endDate, \"%Y-%m-%d\").strftime('%m/%d/%Y')\n",
                "\n",
                "    # Insert the path to your Chrome Driver.\n",
                "    path = '/Users/simonjuulhansen/Desktop/Polit/ISDS/chromedriver'\n",
                "    driver = webdriver.Chrome(executable_path=path)\n",
                "\n",
                "    # Create a request interceptor that will contain the request header for Selenium requests.\n",
                "    # To confirm that the header is present, in the driver window go to https://httpbin.org/headers\n",
                "    def interceptor(request):\n",
                "        request.headers['Contact_details'] = 'qxd466@alumni.ku.dk'\n",
                "        request.headers['Purpose'] = \"Dear recipient, we've scraped your data with the purpose of creating an academic report on electricity prices and if it's possible to predict these. Please contact us at via the contact details, if there's any issue with this. Best regards.\"\n",
                "\n",
                "    # Set the interceptor on the driver\n",
                "    driver.request_interceptor = interceptor\n",
                "    \n",
                "    driver.get(temp_url)\n",
                "    \n",
                "    time.sleep(3)\n",
                "\n",
                "    # Accept cookies\n",
                "    cookies = driver.find_element_by_css_selector('#onetrust-accept-btn-handler')\n",
                "    ActionChains(driver).click(cookies).perform()\n",
                "\n",
                "    time.sleep(2)\n",
                "\n",
                "    # Opens the date range menu\n",
                "    menu = driver.find_element_by_css_selector('#widgetFieldDateRange')\n",
                "    ActionChains(driver).click(menu).perform()\n",
                "\n",
                "    # Enters the date values. The dates have to be in the form of DD/MM/YYYY e.g. 01/01/2000.\n",
                "    # This formatting is done in the above conversion of the start and end date from YYYY-MM-DD.\n",
                "    inputElement1 = driver.find_element_by_id(\"startDate\")\n",
                "    inputElement1.clear()\n",
                "    inputElement1.send_keys(startDate)\n",
                "\n",
                "    inputElement2 = driver.find_element_by_id(\"endDate\")\n",
                "    inputElement2.clear()\n",
                "    inputElement2.send_keys(endDate)\n",
                "\n",
                "    # Applies the new dates\n",
                "    driver.find_element_by_id(\"applyBtn\").click();\n",
                "\n",
                "    time.sleep(3)\n",
                "\n",
                "    # Gets the results from the webpage\n",
                "    results = driver.find_elements_by_xpath('//*[(@id = \"curr_table\")]//td')\n",
                "\n",
                "    # Store the results in a DataFrame\n",
                "    d = defaultdict(list)\n",
                "    date_counter = 0\n",
                "    value_counter = 1\n",
                "    while True:\n",
                "        try:\n",
                "            d['date'].append(results[date_counter].text)\n",
                "            d[commodity].append(results[value_counter].text)\n",
                "            date_counter += 7\n",
                "            value_counter += 7\n",
                "        except IndexError:\n",
                "            break\n",
                "    df = pd.DataFrame(d)\n",
                "    df['date'] = pd.to_datetime(df['date'], format = '%b %d, %Y')\n",
                "\n",
                "    driver.quit()\n",
                "    \n",
                "    return df\n",
                "\n",
                "### Function that gathers all of the commodity data and puts them into one single DataFrame, where NaN has been forward filled (daily data) ###\n",
                "def getAllCommodities(commodities, startDate, endDate):\n",
                "    \"\"\"Function that gathers all of the commodity data and puts them into one single DataFrame, where NaN has been forward filled (daily data)\"\"\"\n",
                "\n",
                "    dict_df = dict()\n",
                "\n",
                "    for i in tqdm(commodities):\n",
                "        print(f'INFO: Gathering commodity data for {i}')\n",
                "        dict_df[i] = getCommodityData(i, startDate, endDate)\n",
                "\n",
                "    date_range = pd.date_range(start=startDate, end=endDate, freq = 'D').to_pydatetime().tolist()\n",
                "    df = pd.DataFrame(date_range, columns = ['date'])\n",
                "\n",
                "    for i in dict_df.values():\n",
                "        df = df.merge(i, on = 'date', how = 'left')\n",
                "        df[i.columns[-1]] = df[i.columns[-1]].str.replace(',','').astype('float')\n",
                "\n",
                "    df = df.ffill()\n",
                "    return df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Total data gathering and handling"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Running the below cell will gather and wrangle all of the data from DMI, Nordpool and investing.com.\n",
                "This might take a while, e.g. 20 min.\n",
                "Remember that the commodity price functions will fail unless you have amended the path in the above cell to match the path to your ChromeDriver.\n",
                "\n",
                "Furthermore, sometimes the scraping-class Connector fails while collection DMI data for cloud_cover due to a timeout (HTTPSConnectionPool(host='dmigw.govcloud.dk', port=443): Read timed out. (read timeout=30)). This mainly happens when DMI has cloud server problems. Refer to https://confluence.govcloud.dk/display/FDAPI/Operational+Status+of+API to see the server status, as this is most likely the reason if the DMI functions are failing\n",
                "\n",
                "***Selenium***: do not move your cursor between the Selenium driver start-up and the selection of dates, as this will prompt a log-in pop-up that Selenium will not close and thus cause the program to fail. In general, just leave your PC be while Selenium runs for optimal perfomance and no accidental failure."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "startDate = '2016-04-01'\n",
                "endDate = '2021-08-01'\n",
                "\n",
                "# Gathering Nordpool data \n",
                "#df_Nordpool = mergeNordpool([getElspotPrices(), ConsumptionPrognosis(), ProductionPrognosis(), WindPrognosis(), ExchangeConnections()])\n",
                "\n",
                "# Gathering DMI weather data\n",
                "metObs_listV2 = [ 'wind_max_per10min_past1h', 'temp_soil_max_past1h', 'cloud_cover']\n",
                "climate_listV1 = ['bright_sunshine', 'mean_radiation', 'mean_pressure', 'acc_precip', 'temp_grass', 'mean_relative_hum', 'mean_temp', 'mean_wind_speed' ,'mean_wind_dir']\n",
                "#df_dmi = total_DMI_pipeline(metObs_listV2, climate_listV1, startDate, endDate)\n",
                "\n",
                "# Gathering commodity data\n",
                "list_of_commodities = ['gold', 'silver', 'crude-oil', 'rotterdam-coal-futures', 'natural-gas', 'carbon-emissions']\n",
                "commodities_df = getAllCommodities(list_of_commodities, startDate, endDate)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Snippet of Data Analysis"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## DMI Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The below gathers the ***old*** DMI data, i.e., where we were gathering data and returning only a DataFrame for a single station and not a combination of data from all given stations. This, as can be seen below, does give us plenty of NA's as the DMI data is far from *pretty*."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df_old_dmi = old_total_DMI_pipeline(metObs_listV2, climate_listV1, startDate, endDate)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "list_of_ylabels_dmi = ['%', '°C', 'm/s', 'mm', 'minutes/hours', 'hPa', 'W/m^2', '%', '°C', 'degrees', 'm/s', '°C']\n",
                "axs = df_old_dmi[df_old_dmi.columns[2:]].plot(subplots=True, layout=(4,3), figsize = (20,10), sharex=True, xlabel = '');\n",
                "\n",
                "\n",
                "for ax in range(len(axs.flat)):\n",
                "    axs.flat[ax].set( ylabel=list_of_ylabels_dmi[ax])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Below is the new approach to gathering DMI data, i.e., filling in all NA's with observations from other stations to combine one DataFrame"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "list_of_ylabels_dmi = ['%', '°C', 'm/s', 'mm', 'minutes/hours', 'hPa', 'W/m^2', '%', '°C', 'degrees', 'm/s', '°C']\n",
                "\n",
                "axs = df_dmi[df_dmi.columns[2:]].plot(subplots=True, layout = (4,3), figsize = (20,10), sharex=True, xlabel = '');\n",
                "for ax in range(len(axs.flat)):\n",
                "    axs.flat[ax].set( ylabel=list_of_ylabels_dmi[ax])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for i in df_dmi.columns:\n",
                "    num_na = len(df_dmi[i]) - len(df_dmi[i].dropna())\n",
                "    print(f\"{i}: {num_na}\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The above shows no signs of outliers or unordinary data that causes any concerns. All of the data conforms to the data descriptions on DMI's [API website](\"https://confluence.govcloud.dk/display/FDAPI/Danish+Meteorological+Institute+-+Open+Data\"). Both the cloud_cover spikes and mean_relative_hum downward spikes appear to merely be extreme observations that are not the result of any measurement errors.\n",
                "\n",
                "If one looks at the missing data, one will see that the NA's do not appear more consecutively than 2 hours, therefore we've decided to forward fill all of the NA's in the below merging function."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Nordpool data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Makes the index to datetime to give an easy datetime x-axis ticks for the below graphs\n",
                "df_Nordpool.index = pd.to_datetime(df_Nordpool[\"date\"].apply(str)+' '+pd.to_datetime(df_Nordpool[\"hour\"], format = '%H').dt.time.apply(str))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Restoring default plt settings\n",
                "plt.rcParams.update(plt.rcParamsDefault)\n",
                "plt.rc('legend',fontsize=11) # using a size in points\n",
                "\n",
                "# Only plot for the dates that we've collected commodity prices and weather data for\n",
                "df_Nordpool[df_Nordpool.index.isin(pd.date_range(start = startDate, end = endDate, freq = 'H'))]\\\n",
                "            .drop(['date', 'hour', 'DK2'], axis = 1)\\\n",
                "            .plot(subplots=True, layout = (9,2), figsize = (10,20), sharex=True );"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "font = {'family' : 'normal',\n",
                "        'weight' : 'normal',\n",
                "        'size'   : 14}\n",
                "\n",
                "plt.rc('font', **font)\n",
                "\n",
                "df_Nordpool[df_Nordpool.index.isin(pd.date_range(start = startDate, end = endDate, freq = 'H'))]['DK2'].plot(figsize = (20,10), ylabel = 'Price in DKK per MWh')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for i in df_Nordpool.columns:\n",
                "    num_na = len(df_Nordpool[df_Nordpool.index.isin(pd.date_range(start = startDate, end = endDate, freq = 'H'))][i]) - len(df_Nordpool[df_Nordpool.index.isin(pd.date_range(start = startDate, end = endDate, freq = 'H'))][i].dropna())\n",
                "    print(f\"{i}: {num_na}\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Based on the above figure and no. of NA's, we've decided to remove DK1-NL from our dataset. This is done in the below function that merges all 3 datasources together. \n",
                "\n",
                "If one looks at the missing data, one will see that the NA's do not appear more consecutively than 2 hours, therefore we've decided to forward fill all of the NA's in the below merging function.\n",
                "\n",
                "Other than that we think that the transformation of the dataset is good as is, so we'll leave it until merging.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Commodity price data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "commodities_df.set_index('date').plot(subplots=True, layout = (2,3), figsize = (20,10), sharex=True, xlabel = '', ylabel='Closing price in US Dollars');"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for i in commodities_df.columns:\n",
                "    num_na = len(commodities_df[i]) - len(commodities_df[i].dropna())\n",
                "    print(f\"{i}: {num_na}\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The above graph shows no signs of mysterious nor unusual data. The commodity data is still only daily observations until the merging happens below. The crude oil drop in start Q2 2020 is of course not a fault of wrong data, however, it was the actual crude oil future price at the time. (https://www.cnbc.com/2020/06/16/how-negative-oil-prices-revealed-the-dangers-of-futures-trading.html)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## The below transforms all of the above dataframes into a single master DataFrame"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def total_dataframe(df_nordpool, df_dmi, df_commodities):\n",
                "\n",
                "    list_of_commodities = ['gold', 'silver', 'crude-oil', 'rotterdam-coal-futures', 'natural-gas', 'carbon-emissions']\n",
                "\n",
                "    # Below we're merging all of the data together into one DataFrame from all 3 input DF's\n",
                "    df_nordpool_new = df_nordpool.copy()\n",
                "    df_nordpool_new['date'] = df_nordpool_new['date'].astype('string')\n",
                "\n",
                "    df_dmi_new = df_dmi.copy()\n",
                "    df_dmi_new['date'] = df_dmi_new['date'].astype('string')\n",
                "\n",
                "    df_commodities_new = df_commodities.copy()\n",
                "    df_commodities_new['date'] = df_commodities_new['date'].astype('string')\n",
                "\n",
                "    first_merge = df_dmi_new.merge(df_nordpool_new, on = ['date', 'hour'], how = 'left')\n",
                "    second_merge = first_merge.merge(df_commodities_new, on = 'date', how = 'left')\n",
                "\n",
                "    ## Deleting unneeded columns, NaN's, duplicates and dealing with daylight savings.\n",
                "    # The duplicates were only for changes to wintertime, where there were one duplicate for each year at 3 AM, but with different values\n",
                "    # for each of the 3 AM observations due to winter time.\n",
                "    # The mean of those two were taken and are replacing the duplicated hours\n",
                "    second_merge.drop(['DK1 - NL', 'DK1 - DE', 'DK1 - SE3', 'DK2 - SE4', 'DK2 - DE'], axis = 1, inplace = True)\n",
                "    new_mean = second_merge[second_merge.duplicated(['date', 'hour'])].groupby(['date', 'hour']).mean().reset_index()\n",
                "    second_merge.drop_duplicates(['date', 'hour'],  keep = False, inplace = True)\n",
                "    df_concat = pd.concat([second_merge, new_mean]).sort_values(['date', 'hour']).reset_index(drop = True)\n",
                "\n",
                "    # We've also replaced the summer time NA's with the mean of the 2nd and 4th hour to create a value for the 3rd hour of the day\n",
                "    # for all variables except for the commodities, as these does not suffer from this issue, as they're daily observations.\n",
                "    # They are merely forward filled below.\n",
                "    list1 = df_concat.columns.tolist()\n",
                "    list2 = [item for item in list1 if item not in list_of_commodities+['date','hour']]\n",
                "    for i in list2:\n",
                "        df_concat[i] = (df_concat[i].ffill() + df_concat[i].bfill())/2\n",
                "\n",
                "    # Forward fill the commodity data to make the price of all hours of the day equal to the daily price\n",
                "    df_concat = df_concat.ffill()\n",
                "\n",
                "    # Below we're generating hour, weekday and month dummies and appending them to the DataFrame \n",
                "    # while also converting the index to a Datetime object in the main DataFrame\n",
                "    df_concat.index = pd.to_datetime(df_concat['date'].apply(str)+' '+pd.to_datetime(df_concat['hour'], format = '%H').dt.time.apply(str))\n",
                "    df_concat['month'] = df_concat.index.month\n",
                "    df_concat['weekday'] = df_concat.index.weekday\n",
                "\n",
                "    d = defaultdict(list)\n",
                "\n",
                "    d[0] = pd.get_dummies(df_concat['hour'], prefix = 'hour')\n",
                "    d[1] = pd.get_dummies(df_concat['weekday'], prefix = 'weekday')\n",
                "    d[2] = pd.get_dummies(df_concat['month'], prefix = 'month')\n",
                "    df_join = df_concat.copy()\n",
                "    for i in d.values():\n",
                "        df_join = df_join.join(i)\n",
                "\n",
                "\n",
                "    # Below we're lagging the variables to comply with the time series element of our regression\n",
                "    # as to not create future leakage\n",
                "\n",
                "    # Lagging the DK2 prices 48 times and only keeping the 24-48 lags\n",
                "    lagged_DK2 = lagmat(df_join['DK2'], 48, use_pandas = True, trim = 'both')\n",
                "    lagged_DK2 = lagged_DK2[lagged_DK2.columns[23:]]\n",
                "\n",
                "    # Lagging the commodity data once by 24 hours\n",
                "    d_lag_commodity = {}\n",
                "    for i in list_of_commodities:\n",
                "        d_lag_commodity[i] = lagmat(df_join[i], 24, use_pandas = True, trim = 'both')\n",
                "        d_lag_commodity[i] = d_lag_commodity[i][d_lag_commodity[i].columns[23]]\n",
                "    df_lag_commodity = pd.concat(d_lag_commodity.values(), axis = 1)\n",
                "\n",
                "    # Lagging the exchange data once\n",
                "    exchange_list = ['DK net exchange', 'DK1 net exchange', 'DK2 net exchange', 'DK - DE',\n",
                "       'DK - NO', 'DK - SE', 'DK1 - DK2']\n",
                "    d_lag_exchange = {}\n",
                "    for i in exchange_list:\n",
                "        d_lag_exchange[i] = lagmat(df_join[i], 24, use_pandas = True, trim = 'both')\n",
                "        d_lag_exchange[i] = d_lag_exchange[i][d_lag_exchange[i].columns[23]]\n",
                "    df_lag_exchange = pd.concat(d_lag_exchange.values(), axis = 1)\n",
                "\n",
                "    # Appending all lags to the DataFrame\n",
                "    list_of_df_lags = [df_lag_commodity, df_lag_exchange, lagged_DK2]\n",
                "    for i in list_of_df_lags:\n",
                "        df_join = df_join.join(i)\n",
                "    \n",
                "    # Dropping original non-lagged columns except the target, DK2\n",
                "    df_join = df_join.dropna().drop(exchange_list+list_of_commodities+['month', 'weekday', 'date', 'hour'], axis = 1)\n",
                "    df_join.drop(['weekday_0', 'hour_0', 'month_1'], axis = 1, inplace = True)\n",
                "\n",
                "    # Adding a trend column to detrend\n",
                "    df_join['trend'] = [i+1 for i in range(len(df_join))]\n",
                "\n",
                "    return df_join"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df_all_data = total_dataframe(df_Nordpool, df_dmi, commodities_df)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df_all_data"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Logging \n",
                "The below is a short overview of the logging activity from the above scraping and API request exercises"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df_log = pd.read_csv('log.csv', sep = ';')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df_log.index = pd.to_datetime(df_log.t, unit='s')\\\n",
                "               .dt.strftime('%H-%M-%S')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "plt.rcParams.update(plt.rcParamsDefault)\n",
                "f,ax = plt.subplots(1,2, figsize=(13,4))\n",
                "\n",
                "ax[0].plot('response_code', data = df_log[df_log['project'] == 'DMI Data'])\n",
                "ax[1].plot( 'response_code', data = df_log[df_log['project'] == 'Nordpool Data']) \n",
                "\n",
                "ax[0].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
                "\n",
                "ax[0].set_xlabel('time') # Choose title x-axis\n",
                "ax[1].set_xlabel('time') # Choose title x-axis\n",
                "\n",
                "ax[0].set_ylabel('Response code') # Choose title y-axis\n",
                "ax[1].set_ylabel('Response code') # Choose title y-axis\n",
                "\n",
                "ax[0].set_title('DMI Data')\n",
                "ax[1].set_title('Nordpool Data')\n",
                "    "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "plt.rcParams.update(plt.rcParamsDefault)\n",
                "f,ax = plt.subplots(1,2, figsize=(13,4))\n",
                "\n",
                "ax[0].plot('delta_t', data = df_log[df_log['project'] == 'DMI Data'])\n",
                "ax[1].plot( 'delta_t', data = df_log[df_log['project'] == 'Nordpool Data']) \n",
                "\n",
                "ax[0].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
                "\n",
                "ax[0].set_xlabel('time') # Choose title x-axis\n",
                "ax[1].set_xlabel('time') # Choose title x-axis\n",
                "\n",
                "ax[0].set_ylabel('Response time') # Choose title y-axis\n",
                "ax[1].set_ylabel('Response time') # Choose title y-axis\n",
                "\n",
                "ax[0].set_title('DMI Data')\n",
                "ax[1].set_title('Nordpool Data')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "plt.rcParams.update(plt.rcParamsDefault)\n",
                "f,ax = plt.subplots(1,2, figsize=(13,4))\n",
                "\n",
                "ax[0].plot('response_size', data = df_log[df_log['project'] == 'DMI Data'])\n",
                "ax[1].plot( 'response_size', data = df_log[df_log['project'] == 'Nordpool Data']) \n",
                "\n",
                "ax[0].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
                "ax[1].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
                "\n",
                "ax[0].set_xlabel('time') # Choose title x-axis\n",
                "ax[1].set_xlabel('time') # Choose title x-axis\n",
                "\n",
                "ax[0].set_ylabel('Response size') # Choose title y-axis\n",
                "ax[1].set_ylabel('Response size') # Choose title y-axis\n",
                "\n",
                "ax[0].set_title('DMI Data')\n",
                "ax[1].set_title('Nordpool Data')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Machine Learning"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## LASSO regression using grid search CV \n",
                "The following code is highly inspired by \"https://github.com/fnauman/timeseries/blob/master/candydata/xgboost_pipeline_candy.ipynb\" but modified to fit our data. The times series handling and use of TimeSeriesSplit is confirmed by https://github.com/carl24k/fight-churn\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Defining different measures to evaluate performance \n",
                "# MAPE\n",
                "def mean_absolute_percentage_error(y_true, y_pred, eps = 1e-8): \n",
                "    return np.mean(np.abs((y_true - y_pred) / (y_true))) * 100 \n",
                "\n",
                "mape = make_scorer(mean_absolute_percentage_error, \n",
                "                   greater_is_better=False)\n",
                "\n",
                "# SMAPE\n",
                "def symmetric_mape(y_true, y_pred, eps = 1e-8):\n",
                "    summ = ((np.abs(y_true) + np.abs(y_pred)) + eps)\n",
                "    return np.mean(np.abs(y_pred - y_true) / summ) * 100\n",
                "\n",
                "smape = make_scorer(symmetric_mape, \n",
                "                    greater_is_better=False)\n",
                "\n",
                "def print_scores(y_test, y_pred):\n",
                "    print(f\"R2 score: {r2_score(y_test, y_pred)}\")\n",
                "    print(f\"MSE score: {MSE(y_test, y_pred)}\")\n",
                "    print(f\"MAE score: {MAE(y_test, y_pred)}\")\n",
                "    print(f\"Median AE score: {MAE(y_test, y_pred)}\")\n",
                "    print(f\"MAPE score: {mean_absolute_percentage_error(y_test, y_pred)}\")\n",
                "    print(f\"SMAPE score: {symmetric_mape(y_test, y_pred)}\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Split the data into a train and test set with a 80/20 split\n",
                "df_train, df_test = df_all_data.iloc[:-9341], df_all_data.iloc[-9341:]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Create plot visualizing the train and test data \n",
                "\n",
                "# Create an axis\n",
                "fig, ax = plt.subplots(figsize = (20,5))\n",
                "\n",
                "# Plot the train and test setsa dn show\n",
                "df_train[\"DK2\"].plot(ax=ax)\n",
                "df_test[\"DK2\"].plot(ax=ax)\n",
                "#plt.axvline(x=4000, color='k', linestyle='--')\n",
                "ax.legend([\"Train\", \"Test\"]);\n",
                "plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Create arrays for X and y for both training and testing data set. \n",
                "y_train = df_train[[\"DK2\"]].to_numpy()\n",
                "y_test  = df_test[[\"DK2\"]].to_numpy()\n",
                "\n",
                "X_train = df_train.copy()\n",
                "X_test  = df_test.copy()\n",
                "\n",
                "X_train.drop('DK2', inplace=True, axis=1)\n",
                "X_test.drop('DK2', inplace=True, axis=1)\n",
                "\n",
                "X_train = X_train.to_numpy()\n",
                "X_test  = X_test.to_numpy()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Define the pipeline for the Lasso regression\n",
                "pipe_lasso = Pipeline([\n",
                "                       #('polynomial', PolynomialFeatures(degree = 2, include_bias=True, interaction_only=True)),\n",
                "                       ('scale', StandardScaler()),\n",
                "                       ('lasso', Lasso())\n",
                "                       ])\n",
                "\n",
                "# Set the different values of lambdas to be searched through in the CV\n",
                "params_lasso = {'lasso__alpha': np.logspace(-4,4,20)}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Define the type of cross validation used in the grid search. The TimesSeriesSplit \n",
                "# is the k-fold alternative for times series keeping the natural order in the data \n",
                "# i.e. one avoid using future observations to predict the past. \n",
                "ts_cv = TimeSeriesSplit(n_splits=20).split(X_train)\n",
                "\n",
                "# Define the pipeline for the grid search using the LASSO pipeline and defined type of CV.\n",
                "# Further, we apply the relevant scoring and parallelizes the CV\n",
                "gs_lasso = GridSearchCV(pipe_lasso,\n",
                "                        param_grid=params_lasso,\n",
                "                        scoring='neg_mean_absolute_error',\n",
                "                        cv=ts_cv,\n",
                "                        n_jobs=4)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Using the defined pipeline to fit our training set\n",
                "gs_lasso.fit(X_train,y_train)\n",
                "# And predicting using the LASSO regression model and the test data \n",
                "y_pred_lasso = gs_lasso.predict(X_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Plot the actual values (train and test) and predicted valuesw\n",
                "fig, ax = plt.subplots(figsize = (20,5))\n",
                "\n",
                "df_train[\"DK2\"].plot(ax=ax)\n",
                "df_test[\"DK2\"].plot(ax=ax)\n",
                "\n",
                "df_test[\"pred\"] = y_pred_lasso\n",
                "df_test[\"pred\"].plot(ax=ax)\n",
                "\n",
                "ax.legend([\"Train\",\"Test\",\"Prediction\"]);\n",
                "plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Optimal lambda value from the grid search\n",
                "print(f\"Optimal lambda value in Lasso using grid search: {gs_lasso.best_params_}\")\n",
                "\n",
                "# Different scoring values\n",
                "print_scores(y_test, y_pred_lasso)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Defining the final pipeline using the optimal lambda detected from the grid search \n",
                "finalpipe_lasso = Pipeline([\n",
                "                       #('polynomial', PolynomialFeatures(degree = 2, include_bias=True, interaction_only=True)),\n",
                "                       ('scale', StandardScaler()),\n",
                "                       ('lasso', Lasso(alpha = gs_lasso.best_params_[\"lasso__alpha\"]))\n",
                "                       ])\n",
                "\n",
                "# Fitting the training data and predicting on the test data\n",
                "finalpipe_lasso.fit(X_train,y_train)\n",
                "finalpipe_lasso.predict(X_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Save coefficients to a dictionary\n",
                "df_new = df_all_data.drop(\"DK2\", axis = 1).copy()\n",
                "d_lasso = {}\n",
                "for i in range(len(df_new.columns)):\n",
                "    d_lasso[df_new.columns[i]] = list(finalpipe_lasso.named_steps['lasso'].coef_)[i]\n",
                "d_lasso"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Ridge regression using grid search CV \n",
                "The following code is highly inspired by \"https://github.com/fnauman/timeseries/blob/master/candydata/xgboost_pipeline_candy.ipynb\" but modified to fit our data. The times series handling and use of TimeSeriesSplit is confirmed by https://github.com/carl24k/fight-churn.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Define the pipeline for the Lasso regression\n",
                "pipe_ridge = Pipeline([\n",
                "                       #('polynomial', PolynomialFeatures(degree = 2, include_bias=True, interaction_only=True)),\n",
                "                       ('scale', StandardScaler()),\n",
                "                       ('ridge', Ridge())\n",
                "                       ])\n",
                "\n",
                "# Set the different values of lambdas to be searched through in the CV\n",
                "params_ridge = {'ridge__alpha': np.logspace(-4,4,20)}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Define the type of cross validation used in the grid search. The TimesSeriesSplit \n",
                "# is the k-fold alternative for times series keeping the natural order in the data \n",
                "# i.e. one avoid using future observations to predict the past. \n",
                "ts_cv = TimeSeriesSplit(n_splits=20).split(X_train)\n",
                "\n",
                "# Define the pipeline for the grid search using the LASSO pipeline and defined type of CV.\n",
                "# Further, we apply the relevant scoring and parallelizes the CV\n",
                "gs_ridge = GridSearchCV(pipe_ridge,\n",
                "                        param_grid=params_ridge,\n",
                "                        scoring='neg_mean_absolute_error',\n",
                "                        cv=ts_cv,\n",
                "                        n_jobs=4)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Using the defined pipeline to fit our training set\n",
                "gs_ridge.fit(X_train,y_train)\n",
                "# And predicting using the LASSO regression model and the test data \n",
                "y_pred_ridge = gs_ridge.predict(X_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Plot the actual values (train and test) and predicted valuesw\n",
                "fig, ax = plt.subplots(figsize = (20,5))\n",
                "\n",
                "df_train[\"DK2\"].plot(ax=ax)\n",
                "df_test[\"DK2\"].plot(alpha = 0.5, color = \"orange\", ax=ax)\n",
                "\n",
                "df_test[\"pred\"] = y_pred_ridge\n",
                "df_test[\"pred\"].plot(alpha = 0.5, color = \"green\", ax=ax)\n",
                "ax.set_xlabel(\"\")\n",
                "ax.set_ylabel(\"Price pr MWH in DKK\")\n",
                "ax.legend([\"Train\",\"Test\",\"Prediction\"]);\n",
                "plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Optimal lambda value from the grid search\n",
                "print(f\"Optimal lambda value in ridge using grid search: {gs_ridge.best_params_}\")\n",
                "\n",
                "# Different scoring values\n",
                "print_scores(y_test, y_pred_ridge)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Defining the final pipeline using the optimal lambda detected from the grid search \n",
                "finalpipe_ridge = Pipeline([\n",
                "                       #('polynomial', PolynomialFeatures(degree = 2, include_bias=True, interaction_only=True)),\n",
                "                       ('scale', StandardScaler()),\n",
                "                       ('ridge', Ridge(alpha = gs_ridge.best_params_[\"ridge__alpha\"]))\n",
                "                       ])\n",
                "\n",
                "# Fitting the training data and predicting on the test data\n",
                "finalpipe_ridge.fit(X_train,y_train)\n",
                "finalpipe_ridge.predict(X_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Save coefficients to a dictionary\n",
                "df_new = df_all_data.drop(\"DK2\", axis = 1).copy()\n",
                "d_ridge = {}\n",
                "for i in range(len(df_new.columns)):\n",
                "    d_ridge[df_new.columns[i]] = list(finalpipe_ridge.named_steps['ridge'].coef_.T)[i]\n",
                "d_ridge"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Elastic net regression using grid search CV \n",
                "The following code is highly inspired by \"https://github.com/fnauman/timeseries/blob/master/candydata/xgboost_pipeline_candy.ipynb\" but modified to fit our data. The times series handling and use of TimeSeriesSplit is confirmed by https://github.com/carl24k/fight-churn\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Define the pipeline for the Elastic Net regression\n",
                "pipe_elasticnet = Pipeline([\n",
                "                       #('polynomial', PolynomialFeatures(degree = 2, include_bias=True, interaction_only=True)),\n",
                "                       ('scale', StandardScaler()),\n",
                "                       ('elasticnet', ElasticNet())\n",
                "                       ])\n",
                "\n",
                "# Set the different values of lambdas to be searched through in the CV\n",
                "params_elasticnet = {'elasticnet__alpha':np.logspace(-4,4,20), 'elasticnet__l1_ratio':np.linspace(0,1,20)},"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Define the type of cross validation used in the grid search. The TimesSeriesSplit \n",
                "# is the k-fold alternative for times series keeping the natural order in the data \n",
                "# i.e. one avoid using future observations to predict the past. \n",
                "ts_cv = TimeSeriesSplit(n_splits=20).split(X_train)\n",
                "\n",
                "# Define the pipeline for the grid search using the LASSO pipeline and defined type of CV.\n",
                "# Further, we apply the relevant scoring and parallelizes the CV\n",
                "gs_elasticnet = GridSearchCV(pipe_elasticnet,\n",
                "                        param_grid=params_elasticnet,\n",
                "                        scoring='neg_mean_absolute_error',\n",
                "                        cv=ts_cv,\n",
                "                        verbose = 2,\n",
                "                        n_jobs=4)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Plot the actual values (train and test) and predicted valuesw\n",
                "fig, ax = plt.subplots(figsize = (20,5))\n",
                "\n",
                "df_train[\"DK2\"].plot(ax=ax)\n",
                "df_test[\"DK2\"].plot(ax=ax)\n",
                "\n",
                "df_test[\"pred\"] = y_pred_elasticnet\n",
                "df_test[\"pred\"].plot(ax=ax)\n",
                "ax.set_xlabel(\"\")\n",
                "ax.set_ylabel(\"DKK/MWh\", fontsize = 14)\n",
                "ax.set_title(\"Elastic net\", fontsize = 20)\n",
                "ax.legend([\"Train\",\"Test\",\"Prediction\"],loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
                "          fancybox=True, shadow=False, ncol=3, fontsize = 14);\n",
                "\n",
                "plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Optimal lambda value from the grid search\n",
                "print(f\"Optimal lambda value in elastic net using grid search: {gs_elasticnet.best_params_}\")\n",
                "\n",
                "# Different scoring values\n",
                "print_scores(y_test, y_pred_elasticnet)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Defining the final pipeline using the optimal lambda detected from the grid search \n",
                "finalpipe_elasticnet = Pipeline([\n",
                "                       #('polynomial', PolynomialFeatures(degree = 2, include_bias=True, interaction_only=True)),\n",
                "                       ('scale', StandardScaler()),\n",
                "                       ('elasticnet', ElasticNet(alpha = gs_elasticnet.best_params_[\"elasticnet__alpha\"], l1_ratio = gs_elasticnet.best_params_[\"elasticnet__l1_ratio\"]))\n",
                "                       ])\n",
                "\n",
                "# Fitting the training data and predicting on the test data\n",
                "finalpipe_elasticnet.fit(X_train,y_train)\n",
                "finalpipe_elasticnet.predict(X_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Save coefficients to a dictionary\n",
                "df_new = df_all_data.drop(\"DK2\", axis = 1).copy()\n",
                "d_elastic = {}\n",
                "for i in range(len(df_new.columns)):\n",
                "    d_elastic[df_new.columns[i]] = list(finalpipe_elasticnet.named_steps['lasso'].coef_)[i]\n",
                "d_elastic"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Naive forecasting\n",
                "### Note: Univariate forecast assuming that $x_{d,h} = x_{d-1,h}$\n",
                "*Where d=day and h=hour*"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Taking a de-tour to satisfy the young Sktime module's hate towards datetime\n",
                "df_all_data.to_csv('naive.csv')\n",
                "\n",
                "data_naive_start = pd.read_csv('naive.csv')\n",
                "data_naive_start[\"Unnamed: 0\"] = pd.to_datetime(data_naive_start[\"Unnamed: 0\"])\n",
                "data_naive_start = data_naive_start.set_index('Unnamed: 0')\n",
                "\n",
                "\n",
                "data_naive = pd.read_csv('naive.csv')['DK2']\n",
                "data_naive_test = data_naive[-9341:]    # Test data consists of 20% og the entire data set\n",
                "data_naive_train = data_naive[:-9341]   \n",
                "\n",
                "\n",
                "forecaster = NaiveForecaster(strategy='last', sp=24)    # Seasonality trajectory at 24 since prices are folowing daily seasons\n",
                "forecaster.fit(data_naive_train)\n",
                "pred = forecaster.predict(list(range(1,9342)))   #list(range(1,10001))\n",
                "\n",
                "g = pd.DataFrame([data_naive_test,pred]).T.set_index(data_naive_start[-9341:].index)\n",
                "\n",
                "print('MAE from Naive Forecast: ', MAE(g['DK2'],g['Unnamed 0']))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "g['Naive forecast'] = g['Unnamed 0']\n",
                "g.drop('Unnamed 0', axis=1,inplace=True) \n",
                "\n",
                "g.reindex_like(data_naive_start[-6000:])\n",
                "\n",
                "# Calculate error measures:\n",
                "g['MSE'] = MSE(g['DK2'],g['Naive forecast'])\n",
                "g['MAE'] = MAE(g['DK2'],g['Naive forecast'])\n",
                "\n",
                "g"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Plot the actual values (train and test) and predicted valuesw\n",
                "fig, ax = plt.subplots(figsize = (20,5))\n",
                "\n",
                "data_naive_start.loc['2016-04-03 00:00:00':'2020-11-24 01:00:00'][\"DK2\"].plot(ax=ax)   #Train data platted\n",
                "g.loc[:'2021-08-01 00:00:00'][\"DK2\"].plot(ax=ax)      # Test data plotted. Insert .loc[:'2020-06-01 23:00:00'] to make graph better\n",
                "\n",
                "g.loc[:'2021-08-01 00:00:00'][\"Naive forecast\"].plot(ax=ax,xlabel='')  #Forecasts plotted. Inster .loc[:'2020-06-01 23:00:00'] to make graph better\n",
                "\n",
                "ax.legend([\"Train\",\"Test\",\"Prediction\"]);\n",
                "\n",
                "plt.show();"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Validation curve for Ridge"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "ts_cv = TimeSeriesSplit(n_splits=20).split(X_train)\n",
                "\n",
                "# FIT AND EVALUATE FOR DIFFERENT LAMBDAS\n",
                "train_scores, test_scores = \\\n",
                "    validation_curve(estimator=pipe_ridge,\n",
                "                     X=X_train,\n",
                "                     y=y_train,\n",
                "                     param_name='ridge__alpha',\n",
                "                     param_range=np.logspace(-4,10,20),\n",
                "                     scoring='neg_mean_absolute_error',                 \n",
                "                     cv=ts_cv,\n",
                "                     n_jobs = 4)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# OBTAIN MSE FOR DIFFERENT LAMBDAS AND PRINT BEST\n",
                "mse_score = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
                "                          'Validation':-test_scores.mean(axis=1),\n",
                "                          'lambda':np.logspace(-4,10,20)})\\\n",
                "              .set_index('lambda')   \n",
                "print(mse_score.Validation.nsmallest(1))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "mse_score.plot(logx=True, logy=True, figsize=(10,6))\n",
                "plt.axvline(mse_score.Validation.nsmallest(1).index, color='r', linestyle='dashed');"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Heatmap for Feature Selection"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "data_hm = df_all_data\n",
                "\n",
                "l = []\n",
                "for column in data_hm.columns:      # Loop to remove all the variables that have been lagged several times and all dummies using REGEX\n",
                "    if len(re.findall(r'\\_\\d|^DK2.L', column)) > 0:\n",
                "        pass\n",
                "    else:\n",
                "        l.append(column)\n",
                "\n",
                "data_hm = data_hm[l]              \n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10,10))\n",
                "hm_all = sns.heatmap(pd.DataFrame(data_hm.corr()), cmap = 'vlag', annot = False)    # Heatmap with ALL correlations\n",
                "plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Machine learning results after Feature Selection"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# List of features to be excluded in the following feature selection process and machine learning models.\n",
                "not_list = ['acc_precip',\n",
                "'temp_grass',\n",
                "'DK1 - DK2.L.24',\n",
                "'DK - DE.L.24',\n",
                "'cloud_cover',\n",
                "'temp_soil_max_past1h',\n",
                "'mean_relative_hum',\n",
                "'DK1_CP',\n",
                "'DK1_PP',\n",
                "'DK1_WP',\n",
                "'DK1 net exchange.L.24',\n",
                "'DK2 net exchange.L.24',\n",
                "'mean_wind_dir']"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Create arrays for X and y for both training and testing data set. \n",
                "y_train = df_train[[\"DK2\"]].to_numpy()\n",
                "y_test  = df_test[[\"DK2\"]].to_numpy()\n",
                "\n",
                "X_train_new = df_train[df_train.columns[~df_train.columns.isin(not_list)]].copy()\n",
                "X_test_new  = df_test[df_test.columns[~df_test.columns.isin(not_list)]].drop('pred',axis=1).copy()\n",
                "\n",
                "X_train_new.drop('DK2', inplace=True, axis=1)\n",
                "X_test_new.drop('DK2', inplace=True, axis=1)\n",
                "\n",
                "X_train_new = X_train_new.to_numpy()\n",
                "X_test_new  = X_test_new.to_numpy()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Elastic net with feature selection"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Define the type of cross validation used in the grid search. The TimesSeriesSplit \n",
                "# is the k-fold alternative for times series keeping the natural order in the data \n",
                "# i.e. one avoid using future observations to predict the past. \n",
                "ts_cv = TimeSeriesSplit(n_splits=20).split(X_train_new)\n",
                "\n",
                "# Define the pipeline for the grid search using the LASSO pipeline and defined type of CV.\n",
                "# Further, we apply the relevant scoring and parallelizes the CV\n",
                "gs_elasticnet = GridSearchCV(pipe_elasticnet,\n",
                "                        param_grid=params_elasticnet,\n",
                "                        scoring='neg_mean_absolute_error',\n",
                "                        cv=ts_cv,\n",
                "                        verbose = 2,\n",
                "                        n_jobs=4)\n",
                "\n",
                "# Using the defined pipeline to fit our training set\n",
                "gs_elasticnet.fit(X_train_new,y_train)\n",
                "\n",
                "# And predicting using the elastic net regression model and the test data \n",
                "y_pred_elasticnet = gs_elasticnet.predict(X_test_new)\n",
                "\n",
                "# Optimal lambda value from the grid search\n",
                "print(f\"Optimal lambda value in elastic net using grid search: {gs_elasticnet.best_params_}\")\n",
                "\n",
                "# Different scoring values\n",
                "print_scores(y_test, y_pred_elasticnet)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Ridge with feature selection"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Define the type of cross validation used in the grid search. The TimesSeriesSplit \n",
                "# is the k-fold alternative for times series keeping the natural order in the data \n",
                "# i.e. one avoid using future observations to predict the past. \n",
                "ts_cv = TimeSeriesSplit(n_splits=20).split(X_train_new)\n",
                "\n",
                "# Define the pipeline for the grid search using the LASSO pipeline and defined type of CV.\n",
                "# Further, we apply the relevant scoring and parallelizes the CV\n",
                "gs_ridge = GridSearchCV(pipe_ridge,\n",
                "                        param_grid=params_ridge,\n",
                "                        scoring='neg_mean_absolute_error',\n",
                "                        cv=ts_cv,\n",
                "                        n_jobs=4)\n",
                "\n",
                "\n",
                "# Using the defined pipeline to fit our training set\n",
                "gs_ridge.fit(X_train_new,y_train)\n",
                "# And predicting using the LASSO regression model and the test data \n",
                "y_pred_ridge = gs_ridge.predict(X_test_new)\n",
                "\n",
                "# Optimal lambda value from the grid search\n",
                "print(f\"Optimal lambda value in ridge using grid search: {gs_ridge.best_params_}\")\n",
                "\n",
                "# Different scoring values\n",
                "print_scores(y_test, y_pred_ridge)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### LASSO with feature selection"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Define the type of cross validation used in the grid search. The TimesSeriesSplit \n",
                "# is the k-fold alternative for times series keeping the natural order in the data \n",
                "# i.e. one avoid using future observations to predict the past. \n",
                "ts_cv = TimeSeriesSplit(n_splits=20).split(X_train_new)\n",
                "\n",
                "# Define the pipeline for the grid search using the LASSO pipeline and defined type of CV.\n",
                "# Further, we apply the relevant scoring and parallelizes the CV\n",
                "gs_lasso = GridSearchCV(pipe_lasso,\n",
                "                        param_grid=params_lasso,\n",
                "                        scoring='neg_mean_absolute_error',\n",
                "                        cv=ts_cv,\n",
                "                        n_jobs=4)\n",
                "\n",
                "# Using the defined pipeline to fit our training set\n",
                "gs_lasso.fit(X_train_new,y_train)\n",
                "# And predicting using the LASSO regression model and the test data \n",
                "y_pred_lasso = gs_lasso.predict(X_test_new)\n",
                "\n",
                "# Optimal lambda value from the grid search\n",
                "print(f\"Optimal lambda value in Lasso using grid search: {gs_lasso.best_params_}\")\n",
                "\n",
                "# Different scoring values\n",
                "print_scores(y_test, y_pred_lasso)"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit"
        },
        "interpreter": {
            "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}